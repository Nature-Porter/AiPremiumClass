{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install evaluate seqeval","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:16.763571Z","iopub.execute_input":"2025-06-13T22:57:16.763777Z","iopub.status.idle":"2025-06-13T22:57:24.506656Z","shell.execute_reply.started":"2025-06-13T22:57:16.763758Z","shell.execute_reply":"2025-06-13T22:57:24.505713Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:24.508681Z","iopub.execute_input":"2025-06-13T22:57:24.509235Z","iopub.status.idle":"2025-06-13T22:57:52.440114Z","shell.execute_reply.started":"2025-06-13T22:57:24.509209Z","shell.execute_reply":"2025-06-13T22:57:52.439508Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-06-13 22:57:37.362404: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749855457.528430      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749855457.576200      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:52.440682Z","iopub.execute_input":"2025-06-13T22:57:52.441111Z","iopub.status.idle":"2025-06-13T22:57:54.966247Z","shell.execute_reply.started":"2025-06-13T22:57:52.441093Z","shell.execute_reply":"2025-06-13T22:57:54.965658Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d12a34854b44149fc994af1371527f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88e88dfa77a644d0976d6beefb2c991b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:54.966977Z","iopub.execute_input":"2025-06-13T22:57:54.967263Z","iopub.status.idle":"2025-06-13T22:57:54.973594Z","shell.execute_reply.started":"2025-06-13T22:57:54.967243Z","shell.execute_reply":"2025-06-13T22:57:54.972807Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:54.975719Z","iopub.execute_input":"2025-06-13T22:57:54.975936Z","iopub.status.idle":"2025-06-13T22:57:55.514662Z","shell.execute_reply.started":"2025-06-13T22:57:54.975919Z","shell.execute_reply":"2025-06-13T22:57:55.514120Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f8efeff9c44283b3788123a39e8e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a211481672243bf92907f3b0e4f5372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499f86b9cfc94e778056ea0fe584eb4b"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# 模型测试\n# message= \"命名实体识别\"\n# label = torch.tensor([0,1,0,2,5,4])\n\n# model_input = tokenizer([message], return_tensors='pt')\n# result = model(**model_input)\n\n# print(result.loss)\n# print(result.logits)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:55.515218Z","iopub.execute_input":"2025-06-13T22:57:55.515403Z","iopub.status.idle":"2025-06-13T22:57:55.518919Z","shell.execute_reply.started":"2025-06-13T22:57:55.515388Z","shell.execute_reply":"2025-06-13T22:57:55.518080Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 加载hf中dataset\nds = load_dataset('nlhappy/CLUE-NER')\nds","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:55.519696Z","iopub.execute_input":"2025-06-13T22:57:55.519955Z","iopub.status.idle":"2025-06-13T22:57:57.326134Z","shell.execute_reply.started":"2025-06-13T22:57:55.519930Z","shell.execute_reply":"2025-06-13T22:57:57.325504Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a0b44a18f76471682daa1a22aa07a0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/970 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af8f274f5b34efd8891ce3e3ab98620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-a33d0e4276aef9b4.parquet:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960797bf62f1403c985d9acb578f5e0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-07f476b71c5edde6.parquet:   0%|          | 0.00/178k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63bbd5164804fafba4cd843ee56a8f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def168595c5443008d911295da44244c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1343 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93746571e5944a8b8bca8b203c09401a"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'ents'],\n        num_rows: 10748\n    })\n    validation: Dataset({\n        features: ['text', 'ents'],\n        num_rows: 1343\n    })\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## 实体映射数据集词典准备","metadata":{}},{"cell_type":"code","source":"# entity_index\nentites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n           'company', 'scene', 'book', 'organization', 'government'})\ntags = ['O']\nfor entity in entites[1:]:\n    tags.append('B-' + entity.upper())\n    tags.append('I-' + entity.upper())\n\nentity_index = {entity:i for i, entity in enumerate(entites)}\n","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:57.326816Z","iopub.execute_input":"2025-06-13T22:57:57.327052Z","iopub.status.idle":"2025-06-13T22:57:57.331929Z","shell.execute_reply.started":"2025-06-13T22:57:57.327021Z","shell.execute_reply":"2025-06-13T22:57:57.331260Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"entity_index","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:57.332793Z","iopub.execute_input":"2025-06-13T22:57:57.333105Z","iopub.status.idle":"2025-06-13T22:57:57.559809Z","shell.execute_reply.started":"2025-06-13T22:57:57.333079Z","shell.execute_reply":"2025-06-13T22:57:57.559062Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'O': 0,\n 'government': 1,\n 'scene': 2,\n 'game': 3,\n 'company': 4,\n 'position': 5,\n 'name': 6,\n 'movie': 7,\n 'organization': 8,\n 'book': 9,\n 'address': 10}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"tags","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:57.560617Z","iopub.execute_input":"2025-06-13T22:57:57.560885Z","iopub.status.idle":"2025-06-13T22:57:57.571868Z","shell.execute_reply.started":"2025-06-13T22:57:57.560868Z","shell.execute_reply":"2025-06-13T22:57:57.571261Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['O',\n 'B-GOVERNMENT',\n 'I-GOVERNMENT',\n 'B-SCENE',\n 'I-SCENE',\n 'B-GAME',\n 'I-GAME',\n 'B-COMPANY',\n 'I-COMPANY',\n 'B-POSITION',\n 'I-POSITION',\n 'B-NAME',\n 'I-NAME',\n 'B-MOVIE',\n 'I-MOVIE',\n 'B-ORGANIZATION',\n 'I-ORGANIZATION',\n 'B-BOOK',\n 'I-BOOK',\n 'B-ADDRESS',\n 'I-ADDRESS']"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# entity_index\nentites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n           'company', 'scene', 'book', 'organization', 'government'})\ntags = ['O']\nfor entity in entites[1:]:\n    tags.append('B-' + entity.upper())\n    tags.append('I-' + entity.upper())\n\nentity_index = {entity:i for i, entity in enumerate(entites)}\n\n\ndef entity_tags_proc(item):\n    # item即是dataset中记录\n    text_len = len(item['text'])  # 根据文本长度生成tags列表\n    tags = [0] * text_len    # 初始值为‘O’\n    # 遍历实体列表，所有实体类别标记填入tags\n    entites = item['ents']\n    for ent in entites:\n        indices = ent['indices']  # 实体索引\n        label = ent['label']   # 实体名\n        tags[indices[0]] = entity_index[label] * 2 - 1\n        for idx in indices[1:]:\n            tags[idx] = entity_index[label] * 2\n    return {'ent_tag': tags}\n\n# 使用自定义回调函数处理数据集记录\nds1 = ds.map(entity_tags_proc)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:57.572553Z","iopub.execute_input":"2025-06-13T22:57:57.572766Z","iopub.status.idle":"2025-06-13T22:57:58.965846Z","shell.execute_reply.started":"2025-06-13T22:57:57.572741Z","shell.execute_reply":"2025-06-13T22:57:58.965057Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"644af53f2658458f8ff9db09e544efe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1343 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349a3c23c0304edd9daecf39ee9e39d6"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# 训练集\nfor row in ds1['train']:\n    print(row['text'])\n    print(row['ent_tag'])\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:58.966748Z","iopub.execute_input":"2025-06-13T22:57:58.967055Z","iopub.status.idle":"2025-06-13T22:57:58.971765Z","shell.execute_reply.started":"2025-06-13T22:57:58.967028Z","shell.execute_reply":"2025-06-13T22:57:58.971055Z"},"trusted":true},"outputs":[{"name":"stdout","text":"浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，\n[7, 8, 8, 8, 0, 0, 0, 0, 0, 11, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"ds1","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:58.973090Z","iopub.execute_input":"2025-06-13T22:57:58.973354Z","iopub.status.idle":"2025-06-13T22:57:58.993915Z","shell.execute_reply.started":"2025-06-13T22:57:58.973330Z","shell.execute_reply":"2025-06-13T22:57:58.993293Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'ents', 'ent_tag'],\n        num_rows: 10748\n    })\n    validation: Dataset({\n        features: ['text', 'ents', 'ent_tag'],\n        num_rows: 1343\n    })\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"中文bert分词在日期时间和英文转换token过程中，出现合并。影响ner标注准确性。","metadata":{}},{"cell_type":"code","source":"token_index = tokenizer.encode('2000年2月add', add_special_tokens=False)\nprint(token_index)\ntokens = tokenizer.decode(token_index)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:58.996824Z","iopub.execute_input":"2025-06-13T22:57:58.997332Z","iopub.status.idle":"2025-06-13T22:57:59.007448Z","shell.execute_reply.started":"2025-06-13T22:57:58.997307Z","shell.execute_reply":"2025-06-13T22:57:59.006610Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[8202, 2399, 123, 3299, 10253]\n2000 年 2 月 add\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"input_data = tokenizer([list('2000年2月add')], add_special_tokens=False, truncation=True, \n                       is_split_into_words=True)\nprint(input_data)\n\ntokens = tokenizer.decode(token_index)\nprint(tokens) # 返回token对应逐个字符","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:59.008231Z","iopub.execute_input":"2025-06-13T22:57:59.008437Z","iopub.status.idle":"2025-06-13T22:57:59.013322Z","shell.execute_reply.started":"2025-06-13T22:57:59.008423Z","shell.execute_reply":"2025-06-13T22:57:59.012744Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'input_ids': [[123, 121, 121, 121, 2399, 123, 3299, 143, 146, 146]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n2000 年 2 月 add\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# entity_index\nentites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n           'company', 'scene', 'book', 'organization', 'government'})\ntags = ['O']\nfor entity in entites[1:]:\n    tags.append('B-' + entity.upper())\n    tags.append('I-' + entity.upper())\n\nentity_index = {entity:i for i, entity in enumerate(entites)}\n\n\ndef entity_tags_proc(item):\n    # item即是dataset中记录\n    text_len = len(item['text'])  # 根据文本长度生成tags列表\n    tags = [0] * text_len    # 初始值为‘O’\n    # 遍历实体列表，所有实体类别标记填入tags\n    entites = item['ents']\n    for ent in entites:\n        indices = ent['indices']  # 实体索引\n        label = ent['label']   # 实体名\n        tags[indices[0]] = entity_index[label] * 2 - 1\n        for idx in indices[1:]:\n            tags[idx] = entity_index[label] * 2\n    return {'ent_tag': tags}\n\n# 使用自定义回调函数处理数据集记录\nds1 = ds.map(entity_tags_proc)\n\ndef data_input_proc(item):\n    # 输入文本先拆分为字符，再转换为模型输入的token索引\n    batch_texts = [list(text) for text in item['text']]\n    # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n    input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n                           is_split_into_words=True, padding='max_length')\n    input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n    return input_data\n    \n\nds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:57:59.013948Z","iopub.execute_input":"2025-06-13T22:57:59.014295Z","iopub.status.idle":"2025-06-13T22:58:05.128862Z","shell.execute_reply.started":"2025-06-13T22:57:59.014258Z","shell.execute_reply":"2025-06-13T22:58:05.128053Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feffb1dcfbff4ee098b2f2416654fe6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1343 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fe759aa14254ed89897ae31c5e0951c"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"for item in ds2['train']:\n    print(len(item['input_ids']), len(item['token_type_ids']), len(item['attention_mask']), len(item['labels']))\n    break","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:05.129890Z","iopub.execute_input":"2025-06-13T22:58:05.130306Z","iopub.status.idle":"2025-06-13T22:58:05.135647Z","shell.execute_reply.started":"2025-06-13T22:58:05.130264Z","shell.execute_reply":"2025-06-13T22:58:05.135133Z"},"trusted":true},"outputs":[{"name":"stdout","text":"512 512 512 512\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# 记录转换为pytorch\nds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n# ds_new = ds2.with_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:05.136195Z","iopub.execute_input":"2025-06-13T22:58:05.136435Z","iopub.status.idle":"2025-06-13T22:58:07.754402Z","shell.execute_reply.started":"2025-06-13T22:58:05.136419Z","shell.execute_reply":"2025-06-13T22:58:07.753634Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## 模型手动训练\n\n","metadata":{}},{"cell_type":"code","source":"[name for name, params in model.named_parameters()]","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:07.755397Z","iopub.execute_input":"2025-06-13T22:58:07.755709Z","iopub.status.idle":"2025-06-13T22:58:07.768128Z","shell.execute_reply.started":"2025-06-13T22:58:07.755685Z","shell.execute_reply":"2025-06-13T22:58:07.767447Z"},"trusted":true},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['bert.embeddings.word_embeddings.weight',\n 'bert.embeddings.position_embeddings.weight',\n 'bert.embeddings.token_type_embeddings.weight',\n 'bert.embeddings.LayerNorm.weight',\n 'bert.embeddings.LayerNorm.bias',\n 'bert.encoder.layer.0.attention.self.query.weight',\n 'bert.encoder.layer.0.attention.self.query.bias',\n 'bert.encoder.layer.0.attention.self.key.weight',\n 'bert.encoder.layer.0.attention.self.key.bias',\n 'bert.encoder.layer.0.attention.self.value.weight',\n 'bert.encoder.layer.0.attention.self.value.bias',\n 'bert.encoder.layer.0.attention.output.dense.weight',\n 'bert.encoder.layer.0.attention.output.dense.bias',\n 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.0.intermediate.dense.weight',\n 'bert.encoder.layer.0.intermediate.dense.bias',\n 'bert.encoder.layer.0.output.dense.weight',\n 'bert.encoder.layer.0.output.dense.bias',\n 'bert.encoder.layer.0.output.LayerNorm.weight',\n 'bert.encoder.layer.0.output.LayerNorm.bias',\n 'bert.encoder.layer.1.attention.self.query.weight',\n 'bert.encoder.layer.1.attention.self.query.bias',\n 'bert.encoder.layer.1.attention.self.key.weight',\n 'bert.encoder.layer.1.attention.self.key.bias',\n 'bert.encoder.layer.1.attention.self.value.weight',\n 'bert.encoder.layer.1.attention.self.value.bias',\n 'bert.encoder.layer.1.attention.output.dense.weight',\n 'bert.encoder.layer.1.attention.output.dense.bias',\n 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.1.intermediate.dense.weight',\n 'bert.encoder.layer.1.intermediate.dense.bias',\n 'bert.encoder.layer.1.output.dense.weight',\n 'bert.encoder.layer.1.output.dense.bias',\n 'bert.encoder.layer.1.output.LayerNorm.weight',\n 'bert.encoder.layer.1.output.LayerNorm.bias',\n 'bert.encoder.layer.2.attention.self.query.weight',\n 'bert.encoder.layer.2.attention.self.query.bias',\n 'bert.encoder.layer.2.attention.self.key.weight',\n 'bert.encoder.layer.2.attention.self.key.bias',\n 'bert.encoder.layer.2.attention.self.value.weight',\n 'bert.encoder.layer.2.attention.self.value.bias',\n 'bert.encoder.layer.2.attention.output.dense.weight',\n 'bert.encoder.layer.2.attention.output.dense.bias',\n 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.2.intermediate.dense.weight',\n 'bert.encoder.layer.2.intermediate.dense.bias',\n 'bert.encoder.layer.2.output.dense.weight',\n 'bert.encoder.layer.2.output.dense.bias',\n 'bert.encoder.layer.2.output.LayerNorm.weight',\n 'bert.encoder.layer.2.output.LayerNorm.bias',\n 'bert.encoder.layer.3.attention.self.query.weight',\n 'bert.encoder.layer.3.attention.self.query.bias',\n 'bert.encoder.layer.3.attention.self.key.weight',\n 'bert.encoder.layer.3.attention.self.key.bias',\n 'bert.encoder.layer.3.attention.self.value.weight',\n 'bert.encoder.layer.3.attention.self.value.bias',\n 'bert.encoder.layer.3.attention.output.dense.weight',\n 'bert.encoder.layer.3.attention.output.dense.bias',\n 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.3.intermediate.dense.weight',\n 'bert.encoder.layer.3.intermediate.dense.bias',\n 'bert.encoder.layer.3.output.dense.weight',\n 'bert.encoder.layer.3.output.dense.bias',\n 'bert.encoder.layer.3.output.LayerNorm.weight',\n 'bert.encoder.layer.3.output.LayerNorm.bias',\n 'bert.encoder.layer.4.attention.self.query.weight',\n 'bert.encoder.layer.4.attention.self.query.bias',\n 'bert.encoder.layer.4.attention.self.key.weight',\n 'bert.encoder.layer.4.attention.self.key.bias',\n 'bert.encoder.layer.4.attention.self.value.weight',\n 'bert.encoder.layer.4.attention.self.value.bias',\n 'bert.encoder.layer.4.attention.output.dense.weight',\n 'bert.encoder.layer.4.attention.output.dense.bias',\n 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.4.intermediate.dense.weight',\n 'bert.encoder.layer.4.intermediate.dense.bias',\n 'bert.encoder.layer.4.output.dense.weight',\n 'bert.encoder.layer.4.output.dense.bias',\n 'bert.encoder.layer.4.output.LayerNorm.weight',\n 'bert.encoder.layer.4.output.LayerNorm.bias',\n 'bert.encoder.layer.5.attention.self.query.weight',\n 'bert.encoder.layer.5.attention.self.query.bias',\n 'bert.encoder.layer.5.attention.self.key.weight',\n 'bert.encoder.layer.5.attention.self.key.bias',\n 'bert.encoder.layer.5.attention.self.value.weight',\n 'bert.encoder.layer.5.attention.self.value.bias',\n 'bert.encoder.layer.5.attention.output.dense.weight',\n 'bert.encoder.layer.5.attention.output.dense.bias',\n 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.5.intermediate.dense.weight',\n 'bert.encoder.layer.5.intermediate.dense.bias',\n 'bert.encoder.layer.5.output.dense.weight',\n 'bert.encoder.layer.5.output.dense.bias',\n 'bert.encoder.layer.5.output.LayerNorm.weight',\n 'bert.encoder.layer.5.output.LayerNorm.bias',\n 'bert.encoder.layer.6.attention.self.query.weight',\n 'bert.encoder.layer.6.attention.self.query.bias',\n 'bert.encoder.layer.6.attention.self.key.weight',\n 'bert.encoder.layer.6.attention.self.key.bias',\n 'bert.encoder.layer.6.attention.self.value.weight',\n 'bert.encoder.layer.6.attention.self.value.bias',\n 'bert.encoder.layer.6.attention.output.dense.weight',\n 'bert.encoder.layer.6.attention.output.dense.bias',\n 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.6.intermediate.dense.weight',\n 'bert.encoder.layer.6.intermediate.dense.bias',\n 'bert.encoder.layer.6.output.dense.weight',\n 'bert.encoder.layer.6.output.dense.bias',\n 'bert.encoder.layer.6.output.LayerNorm.weight',\n 'bert.encoder.layer.6.output.LayerNorm.bias',\n 'bert.encoder.layer.7.attention.self.query.weight',\n 'bert.encoder.layer.7.attention.self.query.bias',\n 'bert.encoder.layer.7.attention.self.key.weight',\n 'bert.encoder.layer.7.attention.self.key.bias',\n 'bert.encoder.layer.7.attention.self.value.weight',\n 'bert.encoder.layer.7.attention.self.value.bias',\n 'bert.encoder.layer.7.attention.output.dense.weight',\n 'bert.encoder.layer.7.attention.output.dense.bias',\n 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.7.intermediate.dense.weight',\n 'bert.encoder.layer.7.intermediate.dense.bias',\n 'bert.encoder.layer.7.output.dense.weight',\n 'bert.encoder.layer.7.output.dense.bias',\n 'bert.encoder.layer.7.output.LayerNorm.weight',\n 'bert.encoder.layer.7.output.LayerNorm.bias',\n 'bert.encoder.layer.8.attention.self.query.weight',\n 'bert.encoder.layer.8.attention.self.query.bias',\n 'bert.encoder.layer.8.attention.self.key.weight',\n 'bert.encoder.layer.8.attention.self.key.bias',\n 'bert.encoder.layer.8.attention.self.value.weight',\n 'bert.encoder.layer.8.attention.self.value.bias',\n 'bert.encoder.layer.8.attention.output.dense.weight',\n 'bert.encoder.layer.8.attention.output.dense.bias',\n 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.8.intermediate.dense.weight',\n 'bert.encoder.layer.8.intermediate.dense.bias',\n 'bert.encoder.layer.8.output.dense.weight',\n 'bert.encoder.layer.8.output.dense.bias',\n 'bert.encoder.layer.8.output.LayerNorm.weight',\n 'bert.encoder.layer.8.output.LayerNorm.bias',\n 'bert.encoder.layer.9.attention.self.query.weight',\n 'bert.encoder.layer.9.attention.self.query.bias',\n 'bert.encoder.layer.9.attention.self.key.weight',\n 'bert.encoder.layer.9.attention.self.key.bias',\n 'bert.encoder.layer.9.attention.self.value.weight',\n 'bert.encoder.layer.9.attention.self.value.bias',\n 'bert.encoder.layer.9.attention.output.dense.weight',\n 'bert.encoder.layer.9.attention.output.dense.bias',\n 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.9.intermediate.dense.weight',\n 'bert.encoder.layer.9.intermediate.dense.bias',\n 'bert.encoder.layer.9.output.dense.weight',\n 'bert.encoder.layer.9.output.dense.bias',\n 'bert.encoder.layer.9.output.LayerNorm.weight',\n 'bert.encoder.layer.9.output.LayerNorm.bias',\n 'bert.encoder.layer.10.attention.self.query.weight',\n 'bert.encoder.layer.10.attention.self.query.bias',\n 'bert.encoder.layer.10.attention.self.key.weight',\n 'bert.encoder.layer.10.attention.self.key.bias',\n 'bert.encoder.layer.10.attention.self.value.weight',\n 'bert.encoder.layer.10.attention.self.value.bias',\n 'bert.encoder.layer.10.attention.output.dense.weight',\n 'bert.encoder.layer.10.attention.output.dense.bias',\n 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.10.intermediate.dense.weight',\n 'bert.encoder.layer.10.intermediate.dense.bias',\n 'bert.encoder.layer.10.output.dense.weight',\n 'bert.encoder.layer.10.output.dense.bias',\n 'bert.encoder.layer.10.output.LayerNorm.weight',\n 'bert.encoder.layer.10.output.LayerNorm.bias',\n 'bert.encoder.layer.11.attention.self.query.weight',\n 'bert.encoder.layer.11.attention.self.query.bias',\n 'bert.encoder.layer.11.attention.self.key.weight',\n 'bert.encoder.layer.11.attention.self.key.bias',\n 'bert.encoder.layer.11.attention.self.value.weight',\n 'bert.encoder.layer.11.attention.self.value.bias',\n 'bert.encoder.layer.11.attention.output.dense.weight',\n 'bert.encoder.layer.11.attention.output.dense.bias',\n 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.11.intermediate.dense.weight',\n 'bert.encoder.layer.11.intermediate.dense.bias',\n 'bert.encoder.layer.11.output.dense.weight',\n 'bert.encoder.layer.11.output.dense.bias',\n 'bert.encoder.layer.11.output.LayerNorm.weight',\n 'bert.encoder.layer.11.output.LayerNorm.bias',\n 'classifier.weight',\n 'classifier.bias']"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# dataLoader\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\n\ntrain_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n\n\n# 模型创建\nid2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                        num_labels=21,\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel.to('cuda')\n\n# 模型参数分组\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=100, \n                                            num_training_steps=train_steps)\n","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:07.768932Z","iopub.execute_input":"2025-06-13T22:58:07.769209Z","iopub.status.idle":"2025-06-13T22:58:08.202893Z","shell.execute_reply.started":"2025-06-13T22:58:07.769186Z","shell.execute_reply":"2025-06-13T22:58:08.202291Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# for item in train_dl:\n#     print(item['input_ids'].shape, \n#           item['token_type_ids'].shape, \n#           item['attention_mask'].shape, \n#           item['labels'].shape)\n#     break","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:08.203543Z","iopub.execute_input":"2025-06-13T22:58:08.203737Z","iopub.status.idle":"2025-06-13T22:58:08.207069Z","shell.execute_reply.started":"2025-06-13T22:58:08.203722Z","shell.execute_reply":"2025-06-13T22:58:08.206311Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from tqdm import tqdm\n\nDEVICE='cuda'\n\n# for epoch in range(5):\n#     model.train()\n#     tpbar = tqdm(train_dl)\n#     for items in tpbar:\n#         items = {k:v.to(DEVICE) for k,v in items.items()}\n#         optimizer.zero_grad()\n#         outputs = model(**items)\n#         loss = outputs.loss\n#         loss.backward()\n#         optimizer.step()\n#         scheduler.step()\n    \n#         tpbar.set_description(f'Epoch:{epoch+1} ' + \n#                           f'bert_lr:{scheduler.get_lr()[0]} ' + \n#                           f'classifier_lr:{scheduler.get_lr()[1]} '+\n#                           f'Loss:{loss.item():.4f}')","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:08.207671Z","iopub.execute_input":"2025-06-13T22:58:08.207826Z","iopub.status.idle":"2025-06-13T22:58:08.217588Z","shell.execute_reply.started":"2025-06-13T22:58:08.207813Z","shell.execute_reply":"2025-06-13T22:58:08.216783Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## 支持混合精度训练","metadata":{}},{"cell_type":"code","source":"# dataLoader\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\n\ntrain_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n\n\n# 模型创建\nid2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                        num_labels=21,\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel.to('cuda')\n\n# 模型参数分组\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=100, \n                                            num_training_steps=train_steps)","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:08.218348Z","iopub.execute_input":"2025-06-13T22:58:08.218529Z","iopub.status.idle":"2025-06-13T22:58:08.705387Z","shell.execute_reply.started":"2025-06-13T22:58:08.218514Z","shell.execute_reply":"2025-06-13T22:58:08.704839Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# from tqdm import tqdm\n# import torch\n\n# DEVICE='cuda'\n\n# # 梯度计算缩放器\n# scaler = torch.GradScaler()\n\n# for epoch in range(5):\n#     model.train()\n#     tpbar = tqdm(train_dl)\n#     for items in tpbar:\n#         items = {k:v.to(DEVICE) for k,v in items.items()}\n#         optimizer.zero_grad()\n\n#         with torch.autocast(device_type='cuda'):\n#             outputs = model(**items)\n#         loss = outputs.loss\n\n#         # 缩放loss后，调用backward\n#         scaler.scale(loss).backward()\n#         scaler.step(optimizer)\n#         scaler.update()\n#         scheduler.step()\n    \n#         tpbar.set_description(f'Epoch:{epoch+1} ' + \n#                           f'bert_lr:{scheduler.get_lr()[0]} ' + \n#                           f'classifier_lr:{scheduler.get_lr()[1]} '+\n#                           f'Loss:{loss.item():.4f}')","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:08.706164Z","iopub.execute_input":"2025-06-13T22:58:08.706398Z","iopub.status.idle":"2025-06-13T22:58:08.710166Z","shell.execute_reply.started":"2025-06-13T22:58:08.706382Z","shell.execute_reply":"2025-06-13T22:58:08.709562Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## 支持分布式训练","metadata":{}},{"cell_type":"code","source":"%%writefile ddp_simple.py\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\n\n# 设置分布式环境\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# 清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n\n# 定义训练循环\ndef train(rank, world_size):\n    setup(rank, world_size)\n    \n    # 定义模型并将其移动到对应的 GPU 设备端\n    model = models.resnet50().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n    \n    # 损失函数及优化器\n    criterion = nn.CrossEntropyLoss().to(rank)\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)\n    \n    # 定义数据集Dataset的转换和图像增强\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    # 分布式训练采样器\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n    \n    # 在训练开始时创建一次\n    scaler = torch.cuda.amp.GradScaler()\n    \n    for epoch in range(10):\n        ddp_model.train()\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(rank), labels.to(rank)\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():\n                outputs = ddp_model(inputs)\n                loss = criterion(outputs, labels)\n            \n            scaler.scale(loss).backward()\n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(ddp_model.parameters(), 1)\n            \n            scaler.step(optimizer)\n            scaler.update()\n            \n#             loss.backward()\n#             optimizer.step()\n            print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n\n    cleanup()\n    \ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T22:58:08.710907Z","iopub.execute_input":"2025-06-13T22:58:08.711159Z","iopub.status.idle":"2025-06-13T22:58:08.725833Z","shell.execute_reply.started":"2025-06-13T22:58:08.711140Z","shell.execute_reply":"2025-06-13T22:58:08.725274Z"}},"outputs":[{"name":"stdout","text":"Writing ddp_simple.py\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# !python ddp_simple.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T22:58:08.726590Z","iopub.execute_input":"2025-06-13T22:58:08.726773Z","iopub.status.idle":"2025-06-13T22:58:08.739419Z","shell.execute_reply.started":"2025-06-13T22:58:08.726751Z","shell.execute_reply":"2025-06-13T22:58:08.738798Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Trainer模型训练\n### TrainingArguments\n","metadata":{}},{"cell_type":"code","source":"%%writefile ner_ddp.py\n\nimport os\nimport numpy as np\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n# 设置分布式环境\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# 清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n    \n\ndef train(rank, world_size):\n    setup(rank, world_size)\n    # 数据集\n    ds = load_dataset('nlhappy/CLUE-NER')\n    # entity_index\n    entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n               'company', 'scene', 'book', 'organization', 'government'})\n    tags = ['O']\n    for entity in entites[1:]:\n        tags.append('B-' + entity.upper())\n        tags.append('I-' + entity.upper())\n    \n    entity_index = {entity:i for i, entity in enumerate(entites)}\n\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    \n    def entity_tags_proc(item):\n        # item即是dataset中记录\n        text_len = len(item['text'])  # 根据文本长度生成tags列表\n        tags = [0] * text_len    # 初始值为‘O’\n        # 遍历实体列表，所有实体类别标记填入tags\n        entites = item['ents']\n        for ent in entites:\n            indices = ent['indices']  # 实体索引\n            label = ent['label']   # 实体名\n            tags[indices[0]] = entity_index[label] * 2 - 1\n            for idx in indices[1:]:\n                tags[idx] = entity_index[label] * 2\n        return {'ent_tag': tags}\n    \n    # 使用自定义回调函数处理数据集记录\n    ds1 = ds.map(entity_tags_proc)\n    \n    def data_input_proc(item):\n        # 输入文本先拆分为字符，再转换为模型输入的token索引\n        batch_texts = [list(text) for text in item['text']]\n        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n        input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n                               is_split_into_words=True, padding='max_length')\n        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n        return input_data\n        \n    \n    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n    \n    \n    local_rank = rank\n    \n    id2lbl = {i:tag for i, tag in enumerate(tags)}\n    lbl2id = {tag:i for i, tag in enumerate(tags)}\n    \n    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                            num_labels=21,\n                                                            id2label=id2lbl,\n                                                            label2id=lbl2id)\n    model.to(local_rank)\n    \n    args = TrainingArguments(\n        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n        num_train_epochs = 3,    # 训练 epoch\n        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n        per_device_train_batch_size=8,  # 训练批次\n        per_device_eval_batch_size=8,\n        report_to='tensorboard',  # 训练输出记录\n        eval_strategy=\"epoch\",\n        local_rank=local_rank,   # 当前进程 RANK\n        fp16=True,               # 使用混合精度\n        lr_scheduler_type='linear',  # 动态学习率\n        warmup_steps=100,        # 预热步数\n        ddp_find_unused_parameters=False  # 优化DDP性能\n    )\n    \n    def compute_metric(result):\n        # result 是一个tuple (predicts, labels)\n        \n        # 获取评估对象\n        seqeval = evaluate.load('seqeval')\n        predicts,labels = result\n        predicts = np.argmax(predicts, axis=2)\n        \n        # 准备评估数据\n        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        results = seqeval.compute(predictions=predicts, references=labels)\n    \n        return results\n    \n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n    \n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=ds2['train'],\n        eval_dataset=ds2['validation'],\n        data_collator=data_collator,\n        compute_metrics=compute_metric\n    )\n    \n    trainer.train()\n\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:08.740217Z","iopub.execute_input":"2025-06-13T22:58:08.740490Z","iopub.status.idle":"2025-06-13T22:58:08.751639Z","shell.execute_reply.started":"2025-06-13T22:58:08.740466Z","shell.execute_reply":"2025-06-13T22:58:08.750885Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Writing ner_ddp.py\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"!python ner_ddp.py","metadata":{"execution":{"iopub.status.busy":"2025-06-13T22:58:08.752332Z","iopub.execute_input":"2025-06-13T22:58:08.752512Z","iopub.status.idle":"2025-06-14T00:05:56.139840Z","shell.execute_reply.started":"2025-06-13T22:58:08.752497Z","shell.execute_reply":"2025-06-14T00:05:56.138872Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"2025-06-13 22:58:13.799702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749855493.821288     128 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749855493.828168     128 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-13 22:58:23.590822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749855503.615115     142 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749855503.622217     142 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-13 22:58:23.691779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749855503.715775     143 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749855503.722959     143 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nMap: 100%|███████████████████████| 10748/10748 [00:01<00:00, 9145.65 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 9536.17 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:01<00:00, 9106.83 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 9272.41 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:05<00:00, 1880.94 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:05<00:00, 1876.72 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1943.27 examples/s]\nMap:  74%|██████████████████▌      | 1000/1343 [00:00<00:00, 2082.41 examples/s]Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1940.04 examples/s]\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n  0%|                                                  | 0/2016 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.1146, 'grad_norm': 13905.53125, 'learning_rate': 3.9587682672233825e-05, 'epoch': 0.74}\n{'loss': 0.137, 'grad_norm': 8368.365234375, 'learning_rate': 3.9587682672233825e-05, 'epoch': 0.74}\n 25%|█████████▉                              | 500/2016 [15:58<48:16,  1.91s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n 33%|█████████████▎                          | 672/2016 [21:32<41:25,  1.85s/it]\n  0%|                                                    | 0/84 [00:00<?, ?it/s]\u001b[A\n  0%|                                                    | 0/84 [00:00<?, ?it/s]\u001b[A\n  2%|█                                           | 2/84 [00:00<00:21,  3.90it/s]\u001b[A\n  2%|█                                           | 2/84 [00:00<00:26,  3.13it/s]\u001b[A\n  4%|█▌                                          | 3/84 [00:01<00:31,  2.54it/s]\u001b[A\n  4%|█▌                                          | 3/84 [00:01<00:33,  2.39it/s]\u001b[A\n  5%|██                                          | 4/84 [00:01<00:36,  2.20it/s]\u001b[A\n  5%|██                                          | 4/84 [00:01<00:37,  2.12it/s]\u001b[A\n  6%|██▌                                         | 5/84 [00:02<00:38,  2.06it/s]\u001b[A\n  6%|██▌                                         | 5/84 [00:02<00:40,  1.94it/s]\u001b[A\n  7%|███▏                                        | 6/84 [00:02<00:39,  1.96it/s]\u001b[A\n  7%|███▏                                        | 6/84 [00:02<00:41,  1.89it/s]\u001b[A\n  8%|███▋                                        | 7/84 [00:03<00:40,  1.89it/s]\u001b[A\n  8%|███▋                                        | 7/84 [00:03<00:41,  1.85it/s]\u001b[A\n 10%|████▏                                       | 8/84 [00:03<00:40,  1.87it/s]\u001b[A\n 10%|████▏                                       | 8/84 [00:04<00:42,  1.80it/s]\u001b[A\n 11%|████▋                                       | 9/84 [00:04<00:40,  1.85it/s]\u001b[A\n 11%|████▋                                       | 9/84 [00:04<00:41,  1.79it/s]\u001b[A\n 12%|█████                                      | 10/84 [00:04<00:40,  1.83it/s]\u001b[A\n 12%|█████                                      | 10/84 [00:05<00:41,  1.80it/s]\u001b[A\n 13%|█████▋                                     | 11/84 [00:05<00:40,  1.80it/s]\u001b[A\n 13%|█████▋                                     | 11/84 [00:05<00:40,  1.80it/s]\u001b[A\n 14%|██████▏                                    | 12/84 [00:06<00:39,  1.81it/s]\u001b[A\n 14%|██████▏                                    | 12/84 [00:06<00:40,  1.79it/s]\u001b[A\n 15%|██████▋                                    | 13/84 [00:06<00:39,  1.80it/s]\u001b[A\n 15%|██████▋                                    | 13/84 [00:06<00:39,  1.80it/s]\u001b[A\n 17%|███████▏                                   | 14/84 [00:07<00:39,  1.76it/s]\u001b[A\n 17%|███████▏                                   | 14/84 [00:07<00:39,  1.78it/s]\u001b[A\n 18%|███████▋                                   | 15/84 [00:07<00:38,  1.78it/s]\u001b[A\n 18%|███████▋                                   | 15/84 [00:08<00:39,  1.75it/s]\u001b[A\n 19%|████████▏                                  | 16/84 [00:08<00:38,  1.79it/s]\u001b[A\n 19%|████████▏                                  | 16/84 [00:08<00:38,  1.76it/s]\u001b[A\n 20%|████████▋                                  | 17/84 [00:08<00:37,  1.79it/s]\u001b[A\n 20%|████████▋                                  | 17/84 [00:09<00:37,  1.78it/s]\u001b[A\n 21%|█████████▏                                 | 18/84 [00:09<00:37,  1.75it/s]\u001b[A\n 21%|█████████▏                                 | 18/84 [00:09<00:37,  1.78it/s]\u001b[A\n 23%|█████████▋                                 | 19/84 [00:10<00:36,  1.76it/s]\u001b[A\n 23%|█████████▋                                 | 19/84 [00:10<00:36,  1.78it/s]\u001b[A\n 24%|██████████▏                                | 20/84 [00:10<00:35,  1.78it/s]\u001b[A\n 24%|██████████▏                                | 20/84 [00:10<00:36,  1.77it/s]\u001b[A\n 25%|██████████▊                                | 21/84 [00:11<00:35,  1.78it/s]\u001b[A\n 25%|██████████▊                                | 21/84 [00:11<00:35,  1.77it/s]\u001b[A\n 26%|███████████▎                               | 22/84 [00:11<00:34,  1.79it/s]\u001b[A\n 26%|███████████▎                               | 22/84 [00:11<00:34,  1.77it/s]\u001b[A\n 27%|███████████▊                               | 23/84 [00:12<00:34,  1.77it/s]\u001b[A\n 27%|███████████▊                               | 23/84 [00:12<00:34,  1.78it/s]\u001b[A\n 29%|████████████▎                              | 24/84 [00:12<00:33,  1.78it/s]\u001b[A\n 29%|████████████▎                              | 24/84 [00:13<00:33,  1.78it/s]\u001b[A\n 30%|████████████▊                              | 25/84 [00:13<00:33,  1.78it/s]\u001b[A\n 30%|████████████▊                              | 25/84 [00:13<00:32,  1.79it/s]\u001b[A\n 31%|█████████████▎                             | 26/84 [00:14<00:32,  1.78it/s]\u001b[A\n 31%|█████████████▎                             | 26/84 [00:14<00:32,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 27/84 [00:14<00:32,  1.77it/s]\u001b[A\n 32%|█████████████▊                             | 27/84 [00:14<00:31,  1.79it/s]\u001b[A\n 33%|██████████████▎                            | 28/84 [00:15<00:31,  1.78it/s]\u001b[A\n 33%|██████████████▎                            | 28/84 [00:15<00:31,  1.80it/s]\u001b[A\n 35%|██████████████▊                            | 29/84 [00:15<00:30,  1.77it/s]\u001b[A\n 35%|██████████████▊                            | 29/84 [00:15<00:30,  1.80it/s]\u001b[A\n 36%|███████████████▎                           | 30/84 [00:16<00:30,  1.78it/s]\u001b[A\n 36%|███████████████▎                           | 30/84 [00:16<00:30,  1.80it/s]\u001b[A\n 37%|███████████████▊                           | 31/84 [00:16<00:29,  1.78it/s]\u001b[A\n 37%|███████████████▊                           | 31/84 [00:16<00:29,  1.79it/s]\u001b[A\n 38%|████████████████▍                          | 32/84 [00:17<00:29,  1.79it/s]\u001b[A\n 38%|████████████████▍                          | 32/84 [00:17<00:29,  1.79it/s]\u001b[A\n 39%|████████████████▉                          | 33/84 [00:17<00:28,  1.79it/s]\u001b[A\n 39%|████████████████▉                          | 33/84 [00:18<00:28,  1.78it/s]\u001b[A\n 40%|█████████████████▍                         | 34/84 [00:18<00:27,  1.80it/s]\u001b[A\n 40%|█████████████████▍                         | 34/84 [00:18<00:28,  1.78it/s]\u001b[A\n 42%|█████████████████▉                         | 35/84 [00:19<00:27,  1.79it/s]\u001b[A\n 42%|█████████████████▉                         | 35/84 [00:19<00:27,  1.78it/s]\u001b[A\n 43%|██████████████████▍                        | 36/84 [00:19<00:26,  1.79it/s]\u001b[A\n 43%|██████████████████▍                        | 36/84 [00:19<00:27,  1.78it/s]\u001b[A\n 44%|██████████████████▉                        | 37/84 [00:20<00:26,  1.81it/s]\u001b[A\n 44%|██████████████████▉                        | 37/84 [00:20<00:26,  1.78it/s]\u001b[A\n 45%|███████████████████▍                       | 38/84 [00:20<00:25,  1.78it/s]\u001b[A\n 45%|███████████████████▍                       | 38/84 [00:20<00:25,  1.78it/s]\u001b[A\n 46%|███████████████████▉                       | 39/84 [00:21<00:25,  1.79it/s]\u001b[A\n 46%|███████████████████▉                       | 39/84 [00:21<00:25,  1.78it/s]\u001b[A\n 48%|████████████████████▍                      | 40/84 [00:21<00:24,  1.79it/s]\u001b[A\n 48%|████████████████████▍                      | 40/84 [00:22<00:24,  1.79it/s]\u001b[A\n 49%|████████████████████▉                      | 41/84 [00:22<00:24,  1.78it/s]\u001b[A\n 49%|████████████████████▉                      | 41/84 [00:22<00:24,  1.79it/s]\u001b[A\n 50%|█████████████████████▌                     | 42/84 [00:22<00:23,  1.79it/s]\u001b[A\n 50%|█████████████████████▌                     | 42/84 [00:23<00:23,  1.78it/s]\u001b[A\n 51%|██████████████████████                     | 43/84 [00:23<00:22,  1.79it/s]\u001b[A\n 51%|██████████████████████                     | 43/84 [00:23<00:22,  1.79it/s]\u001b[A\n 52%|██████████████████████▌                    | 44/84 [00:24<00:22,  1.79it/s]\u001b[A\n 52%|██████████████████████▌                    | 44/84 [00:24<00:22,  1.79it/s]\u001b[A\n 54%|███████████████████████                    | 45/84 [00:24<00:22,  1.76it/s]\u001b[A\n 54%|███████████████████████                    | 45/84 [00:24<00:21,  1.79it/s]\u001b[A\n 55%|███████████████████████▌                   | 46/84 [00:25<00:21,  1.78it/s]\u001b[A\n 55%|███████████████████████▌                   | 46/84 [00:25<00:21,  1.77it/s]\u001b[A\n 56%|████████████████████████                   | 47/84 [00:25<00:20,  1.78it/s]\u001b[A\n 56%|████████████████████████                   | 47/84 [00:25<00:20,  1.77it/s]\u001b[A\n 57%|████████████████████████▌                  | 48/84 [00:26<00:20,  1.79it/s]\u001b[A\n 57%|████████████████████████▌                  | 48/84 [00:26<00:20,  1.77it/s]\u001b[A\n 58%|█████████████████████████                  | 49/84 [00:26<00:19,  1.78it/s]\u001b[A\n 58%|█████████████████████████                  | 49/84 [00:27<00:19,  1.77it/s]\u001b[A\n 60%|█████████████████████████▌                 | 50/84 [00:27<00:19,  1.78it/s]\u001b[A\n 60%|█████████████████████████▌                 | 50/84 [00:27<00:19,  1.78it/s]\u001b[A\n 61%|██████████████████████████                 | 51/84 [00:28<00:18,  1.78it/s]\u001b[A\n 61%|██████████████████████████                 | 51/84 [00:28<00:18,  1.78it/s]\u001b[A\n 62%|██████████████████████████▌                | 52/84 [00:28<00:17,  1.78it/s]\u001b[A\n 62%|██████████████████████████▌                | 52/84 [00:28<00:17,  1.79it/s]\u001b[A\n 63%|███████████████████████████▏               | 53/84 [00:29<00:17,  1.80it/s]\u001b[A\n 63%|███████████████████████████▏               | 53/84 [00:29<00:17,  1.80it/s]\u001b[A\n 64%|███████████████████████████▋               | 54/84 [00:29<00:16,  1.77it/s]\u001b[A\n 64%|███████████████████████████▋               | 54/84 [00:29<00:16,  1.79it/s]\u001b[A\n 65%|████████████████████████████▏              | 55/84 [00:30<00:15,  1.82it/s]\u001b[A\n 65%|████████████████████████████▏              | 55/84 [00:30<00:16,  1.79it/s]\u001b[A\n 67%|████████████████████████████▋              | 56/84 [00:30<00:15,  1.80it/s]\u001b[A\n 67%|████████████████████████████▋              | 56/84 [00:31<00:15,  1.76it/s]\u001b[A\n 68%|█████████████████████████████▏             | 57/84 [00:31<00:14,  1.81it/s]\u001b[A\n 68%|█████████████████████████████▏             | 57/84 [00:31<00:15,  1.77it/s]\u001b[A\n 69%|█████████████████████████████▋             | 58/84 [00:31<00:14,  1.80it/s]\u001b[A\n 69%|█████████████████████████████▋             | 58/84 [00:32<00:14,  1.77it/s]\u001b[A\n 70%|██████████████████████████████▏            | 59/84 [00:32<00:13,  1.80it/s]\u001b[A\n 70%|██████████████████████████████▏            | 59/84 [00:32<00:13,  1.79it/s]\u001b[A\n 71%|██████████████████████████████▋            | 60/84 [00:33<00:13,  1.76it/s]\u001b[A\n 71%|██████████████████████████████▋            | 60/84 [00:33<00:13,  1.78it/s]\u001b[A\n 73%|███████████████████████████████▏           | 61/84 [00:33<00:13,  1.77it/s]\u001b[A\n 73%|███████████████████████████████▏           | 61/84 [00:33<00:12,  1.78it/s]\u001b[A\n 74%|███████████████████████████████▋           | 62/84 [00:34<00:12,  1.77it/s]\u001b[A\n 74%|███████████████████████████████▋           | 62/84 [00:34<00:12,  1.79it/s]\u001b[A\n 75%|████████████████████████████████▎          | 63/84 [00:34<00:11,  1.77it/s]\u001b[A\n 75%|████████████████████████████████▎          | 63/84 [00:34<00:11,  1.79it/s]\u001b[A\n 76%|████████████████████████████████▊          | 64/84 [00:35<00:11,  1.78it/s]\u001b[A\n 76%|████████████████████████████████▊          | 64/84 [00:35<00:10,  1.89it/s]\u001b[A\n 77%|█████████████████████████████████▎         | 65/84 [00:35<00:09,  2.01it/s]\u001b[A\n 77%|█████████████████████████████████▎         | 65/84 [00:36<00:12,  1.56it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 66/84 [00:36<00:09,  1.94it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 66/84 [00:36<00:11,  1.63it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 67/84 [00:36<00:08,  1.89it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 67/84 [00:37<00:10,  1.67it/s]\u001b[A\n 81%|██████████████████████████████████▊        | 68/84 [00:37<00:08,  1.86it/s]\u001b[A\n 81%|██████████████████████████████████▊        | 68/84 [00:37<00:09,  1.71it/s]\u001b[A\n 82%|███████████████████████████████████▎       | 69/84 [00:38<00:08,  1.83it/s]\u001b[A\n 82%|███████████████████████████████████▎       | 69/84 [00:38<00:08,  1.74it/s]\u001b[A\n 83%|███████████████████████████████████▊       | 70/84 [00:38<00:07,  1.84it/s]\u001b[A\n 83%|███████████████████████████████████▊       | 70/84 [00:38<00:08,  1.70it/s]\u001b[A\n 85%|████████████████████████████████████▎      | 71/84 [00:39<00:07,  1.82it/s]\u001b[A\n 85%|████████████████████████████████████▎      | 71/84 [00:39<00:07,  1.73it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 72/84 [00:39<00:06,  1.81it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 72/84 [00:40<00:06,  1.74it/s]\u001b[A\n 87%|█████████████████████████████████████▎     | 73/84 [00:40<00:06,  1.81it/s]\u001b[A\n 87%|█████████████████████████████████████▎     | 73/84 [00:40<00:06,  1.76it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 74/84 [00:40<00:05,  1.79it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 74/84 [00:41<00:05,  1.77it/s]\u001b[A\n 89%|██████████████████████████████████████▍    | 75/84 [00:41<00:05,  1.80it/s]\u001b[A\n 89%|██████████████████████████████████████▍    | 75/84 [00:41<00:05,  1.78it/s]\u001b[A\n 90%|██████████████████████████████████████▉    | 76/84 [00:41<00:04,  1.79it/s]\u001b[A\n 90%|██████████████████████████████████████▉    | 76/84 [00:42<00:04,  1.78it/s]\u001b[A\n 92%|███████████████████████████████████████▍   | 77/84 [00:42<00:03,  1.80it/s]\u001b[A\n 92%|███████████████████████████████████████▍   | 77/84 [00:42<00:03,  1.78it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 78/84 [00:43<00:03,  1.79it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 78/84 [00:43<00:03,  1.79it/s]\u001b[A\n 94%|████████████████████████████████████████▍  | 79/84 [00:43<00:02,  1.80it/s]\u001b[A\n 94%|████████████████████████████████████████▍  | 79/84 [00:44<00:02,  1.76it/s]\u001b[A\n 95%|████████████████████████████████████████▉  | 80/84 [00:44<00:02,  1.78it/s]\u001b[A\n 95%|████████████████████████████████████████▉  | 80/84 [00:44<00:02,  1.77it/s]\u001b[A\n 96%|█████████████████████████████████████████▍ | 81/84 [00:44<00:01,  1.78it/s]\u001b[A\n 96%|█████████████████████████████████████████▍ | 81/84 [00:45<00:01,  1.77it/s]\u001b[A\n 98%|█████████████████████████████████████████▉ | 82/84 [00:45<00:01,  1.79it/s]\u001b[A\n 98%|█████████████████████████████████████████▉ | 82/84 [00:45<00:01,  1.77it/s]\u001b[A\n 99%|██████████████████████████████████████████▍| 83/84 [00:45<00:00,  1.78it/s]\u001b[A\n 99%|██████████████████████████████████████████▍| 83/84 [00:46<00:00,  1.75it/s]\u001b[A\n100%|███████████████████████████████████████████| 84/84 [00:46<00:00,  1.81it/s]\u001b[A\n100%|███████████████████████████████████████████| 84/84 [00:46<00:00,  1.96it/s]\u001b[A\n\nDownloading builder script: 100%|██████████| 6.34k/6.34k [00:00<00:00, 25.8MB/s]\u001b[A\u001b[A\nTrainer is attempting to log a value of \"{'precision': 0.5123152709359606, 'recall': 0.5576407506702413, 'f1': 0.5340179717586649, 'number': 373}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.5353982300884956, 'recall': 0.7857142857142857, 'f1': 0.636842105263158, 'number': 154}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7111650485436893, 'recall': 0.7751322751322751, 'f1': 0.7417721518987341, 'number': 378}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7530864197530864, 'recall': 0.8271186440677966, 'f1': 0.7883683360258481, 'number': 295}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6503496503496503, 'recall': 0.7530364372469636, 'f1': 0.6979362101313321, 'number': 247}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7169811320754716, 'recall': 0.5033112582781457, 'f1': 0.5914396887159533, 'number': 151}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7896749521988528, 'recall': 0.8881720430107527, 'f1': 0.8360323886639676, 'number': 465}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6046511627906976, 'recall': 0.779291553133515, 'f1': 0.680952380952381, 'number': 367}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6888454011741683, 'recall': 0.812933025404157, 'f1': 0.7457627118644067, 'number': 433}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.5241157556270096, 'recall': 0.7799043062200957, 'f1': 0.6269230769230769, 'number': 209}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.018578948453068733, 'eval_ADDRESS': {'precision': 0.5123152709359606, 'recall': 0.5576407506702413, 'f1': 0.5340179717586649, 'number': 373}, 'eval_BOOK': {'precision': 0.5353982300884956, 'recall': 0.7857142857142857, 'f1': 0.636842105263158, 'number': 154}, 'eval_COMPANY': {'precision': 0.7111650485436893, 'recall': 0.7751322751322751, 'f1': 0.7417721518987341, 'number': 378}, 'eval_GAME': {'precision': 0.7530864197530864, 'recall': 0.8271186440677966, 'f1': 0.7883683360258481, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6503496503496503, 'recall': 0.7530364372469636, 'f1': 0.6979362101313321, 'number': 247}, 'eval_MOVIE': {'precision': 0.7169811320754716, 'recall': 0.5033112582781457, 'f1': 0.5914396887159533, 'number': 151}, 'eval_NAME': {'precision': 0.7896749521988528, 'recall': 0.8881720430107527, 'f1': 0.8360323886639676, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.6046511627906976, 'recall': 0.779291553133515, 'f1': 0.680952380952381, 'number': 367}, 'eval_POSITION': {'precision': 0.6888454011741683, 'recall': 0.812933025404157, 'f1': 0.7457627118644067, 'number': 433}, 'eval_SCENE': {'precision': 0.5241157556270096, 'recall': 0.7799043062200957, 'f1': 0.6269230769230769, 'number': 209}, 'eval_overall_precision': 0.6545556176634991, 'eval_overall_recall': 0.7623697916666666, 'eval_overall_f1': 0.7043609022556391, 'eval_overall_accuracy': 0.9944780226172748, 'eval_runtime': 54.491, 'eval_samples_per_second': 24.646, 'eval_steps_per_second': 1.542, 'epoch': 1.0}\n 33%|█████████████▎                          | 672/2016 [22:27<41:25,  1.85s/it]\n100%|███████████████████████████████████████████| 84/84 [00:53<00:00,  1.81it/s]\u001b[A\n                                                                                \u001b[ATrainer is attempting to log a value of \"{'precision': 0.5163551401869159, 'recall': 0.5924932975871313, 'f1': 0.5518102372034955, 'number': 373}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6507936507936508, 'recall': 0.7987012987012987, 'f1': 0.7172011661807581, 'number': 154}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6867749419953596, 'recall': 0.783068783068783, 'f1': 0.7317676143386898, 'number': 378}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7024793388429752, 'recall': 0.864406779661017, 'f1': 0.7750759878419452, 'number': 295}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6762589928057554, 'recall': 0.7611336032388664, 'f1': 0.7161904761904763, 'number': 247}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7130434782608696, 'recall': 0.543046357615894, 'f1': 0.6165413533834586, 'number': 151}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7915869980879541, 'recall': 0.8903225806451613, 'f1': 0.8380566801619435, 'number': 465}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6155507559395248, 'recall': 0.776566757493188, 'f1': 0.6867469879518072, 'number': 367}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6893787575150301, 'recall': 0.7944572748267898, 'f1': 0.7381974248927038, 'number': 433}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.5927272727272728, 'recall': 0.7799043062200957, 'f1': 0.6735537190082644, 'number': 209}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.017117273062467575, 'eval_ADDRESS': {'precision': 0.5163551401869159, 'recall': 0.5924932975871313, 'f1': 0.5518102372034955, 'number': 373}, 'eval_BOOK': {'precision': 0.6507936507936508, 'recall': 0.7987012987012987, 'f1': 0.7172011661807581, 'number': 154}, 'eval_COMPANY': {'precision': 0.6867749419953596, 'recall': 0.783068783068783, 'f1': 0.7317676143386898, 'number': 378}, 'eval_GAME': {'precision': 0.7024793388429752, 'recall': 0.864406779661017, 'f1': 0.7750759878419452, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6762589928057554, 'recall': 0.7611336032388664, 'f1': 0.7161904761904763, 'number': 247}, 'eval_MOVIE': {'precision': 0.7130434782608696, 'recall': 0.543046357615894, 'f1': 0.6165413533834586, 'number': 151}, 'eval_NAME': {'precision': 0.7915869980879541, 'recall': 0.8903225806451613, 'f1': 0.8380566801619435, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.6155507559395248, 'recall': 0.776566757493188, 'f1': 0.6867469879518072, 'number': 367}, 'eval_POSITION': {'precision': 0.6893787575150301, 'recall': 0.7944572748267898, 'f1': 0.7381974248927038, 'number': 433}, 'eval_SCENE': {'precision': 0.5927272727272728, 'recall': 0.7799043062200957, 'f1': 0.6735537190082644, 'number': 209}, 'eval_overall_precision': 0.6652637485970819, 'eval_overall_recall': 0.7718098958333334, 'eval_overall_f1': 0.71458710066305, 'eval_overall_accuracy': 0.9948561406366344, 'eval_runtime': 55.3036, 'eval_samples_per_second': 24.284, 'eval_steps_per_second': 1.519, 'epoch': 1.0}\n 33%|█████████████▎                          | 672/2016 [22:28<41:29,  1.85s/it]\n100%|███████████████████████████████████████████| 84/84 [00:54<00:00,  1.96it/s]\u001b[A\n{'loss': 0.0154, 'grad_norm': 12405.392578125, 'learning_rate': 2.6539665970772443e-05, 'epoch': 1.49}\n{'loss': 0.0153, 'grad_norm': 13791.3515625, 'learning_rate': 2.6539665970772443e-05, 'epoch': 1.49}\n 50%|███████████████████▎                   | 1000/2016 [32:54<28:00,  1.65s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n 50%|███████████████████▎                   | 1001/2016 [32:57<39:38,  2.34s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n 67%|█████████████████████████▉             | 1343/2016 [43:55<21:16,  1.90s/it]\n  0%|                                                    | 0/84 [00:00<?, ?it/s]\u001b[A\n  2%|█                                           | 2/84 [00:00<00:24,  3.37it/s]\u001b[A\n 67%|██████████████████████████             | 1344/2016 [43:57<20:11,  1.80s/it]\u001b[A\n  5%|██                                          | 4/84 [00:01<00:37,  2.11it/s]\u001b[A\n  0%|                                                    | 0/84 [00:00<?, ?it/s]\u001b[A\n  6%|██▌                                         | 5/84 [00:02<00:39,  2.00it/s]\u001b[A\n  2%|█                                           | 2/84 [00:00<00:23,  3.51it/s]\u001b[A\n  7%|███▏                                        | 6/84 [00:02<00:40,  1.94it/s]\u001b[A\n  4%|█▌                                          | 3/84 [00:01<00:32,  2.47it/s]\u001b[A\n  8%|███▋                                        | 7/84 [00:03<00:40,  1.89it/s]\u001b[A\n  5%|██                                          | 4/84 [00:01<00:37,  2.15it/s]\u001b[A\n 10%|████▏                                       | 8/84 [00:03<00:40,  1.87it/s]\u001b[A\n  6%|██▌                                         | 5/84 [00:02<00:39,  1.99it/s]\u001b[A\n 11%|████▋                                       | 9/84 [00:04<00:40,  1.84it/s]\u001b[A\n  7%|███▏                                        | 6/84 [00:02<00:40,  1.90it/s]\u001b[A\n 12%|█████                                      | 10/84 [00:05<00:41,  1.80it/s]\u001b[A\n  8%|███▋                                        | 7/84 [00:03<00:41,  1.87it/s]\u001b[A\n 13%|█████▋                                     | 11/84 [00:05<00:40,  1.80it/s]\u001b[A\n 10%|████▏                                       | 8/84 [00:03<00:41,  1.85it/s]\u001b[A\n 14%|██████▏                                    | 12/84 [00:06<00:40,  1.79it/s]\u001b[A\n 11%|████▋                                       | 9/84 [00:04<00:41,  1.82it/s]\u001b[A\n 15%|██████▋                                    | 13/84 [00:06<00:36,  1.95it/s]\u001b[A\n 17%|███████▏                                   | 14/84 [00:07<00:36,  1.94it/s]\u001b[A\n 12%|█████                                      | 10/84 [00:05<00:47,  1.57it/s]\u001b[A\n 18%|███████▋                                   | 15/84 [00:07<00:35,  1.93it/s]\u001b[A\n 13%|█████▋                                     | 11/84 [00:05<00:45,  1.61it/s]\u001b[A\n 19%|████████▏                                  | 16/84 [00:08<00:35,  1.90it/s]\u001b[A\n 14%|██████▏                                    | 12/84 [00:06<00:43,  1.66it/s]\u001b[A\n 20%|████████▋                                  | 17/84 [00:08<00:35,  1.86it/s]\u001b[A\n 15%|██████▋                                    | 13/84 [00:07<00:42,  1.69it/s]\u001b[A\n 21%|█████████▏                                 | 18/84 [00:09<00:35,  1.85it/s]\u001b[A\n 17%|███████▏                                   | 14/84 [00:07<00:40,  1.72it/s]\u001b[A\n 23%|█████████▋                                 | 19/84 [00:09<00:35,  1.82it/s]\u001b[A\n 18%|███████▋                                   | 15/84 [00:08<00:40,  1.72it/s]\u001b[A\n 24%|██████████▏                                | 20/84 [00:10<00:35,  1.82it/s]\u001b[A\n 19%|████████▏                                  | 16/84 [00:08<00:39,  1.74it/s]\u001b[A\n 25%|██████████▊                                | 21/84 [00:11<00:34,  1.80it/s]\u001b[A\n 20%|████████▋                                  | 17/84 [00:09<00:38,  1.76it/s]\u001b[A\n 26%|███████████▎                               | 22/84 [00:11<00:34,  1.80it/s]\u001b[A\n 21%|█████████▏                                 | 18/84 [00:09<00:37,  1.77it/s]\u001b[A\n 27%|███████████▊                               | 23/84 [00:12<00:33,  1.79it/s]\u001b[A\n 23%|█████████▋                                 | 19/84 [00:10<00:36,  1.78it/s]\u001b[A\n 29%|████████████▎                              | 24/84 [00:12<00:33,  1.78it/s]\u001b[A\n 24%|██████████▏                                | 20/84 [00:11<00:35,  1.78it/s]\u001b[A\n 30%|████████████▊                              | 25/84 [00:13<00:32,  1.79it/s]\u001b[A\n 25%|██████████▊                                | 21/84 [00:11<00:35,  1.77it/s]\u001b[A\n 31%|█████████████▎                             | 26/84 [00:13<00:32,  1.79it/s]\u001b[A\n 26%|███████████▎                               | 22/84 [00:12<00:35,  1.74it/s]\u001b[A\n 32%|█████████████▊                             | 27/84 [00:14<00:31,  1.80it/s]\u001b[A\n 27%|███████████▊                               | 23/84 [00:12<00:35,  1.73it/s]\u001b[A\n 33%|██████████████▎                            | 28/84 [00:14<00:31,  1.78it/s]\u001b[A\n 29%|████████████▎                              | 24/84 [00:13<00:34,  1.74it/s]\u001b[A\n 35%|██████████████▊                            | 29/84 [00:15<00:30,  1.78it/s]\u001b[A\n 30%|████████████▊                              | 25/84 [00:13<00:33,  1.76it/s]\u001b[A\n 36%|███████████████▎                           | 30/84 [00:16<00:30,  1.78it/s]\u001b[A\n 31%|█████████████▎                             | 26/84 [00:14<00:33,  1.75it/s]\u001b[A\n 37%|███████████████▊                           | 31/84 [00:16<00:29,  1.80it/s]\u001b[A\n 32%|█████████████▊                             | 27/84 [00:15<00:32,  1.74it/s]\u001b[A\n 38%|████████████████▍                          | 32/84 [00:17<00:28,  1.81it/s]\u001b[A\n 33%|██████████████▎                            | 28/84 [00:15<00:32,  1.73it/s]\u001b[A\n 39%|████████████████▉                          | 33/84 [00:17<00:29,  1.75it/s]\u001b[A\n 35%|██████████████▊                            | 29/84 [00:16<00:31,  1.75it/s]\u001b[A\n 40%|█████████████████▍                         | 34/84 [00:18<00:28,  1.76it/s]\u001b[A\n 36%|███████████████▎                           | 30/84 [00:16<00:30,  1.76it/s]\u001b[A\n 42%|█████████████████▉                         | 35/84 [00:18<00:27,  1.78it/s]\u001b[A\n 37%|███████████████▊                           | 31/84 [00:17<00:30,  1.75it/s]\u001b[A\n 43%|██████████████████▍                        | 36/84 [00:19<00:26,  1.78it/s]\u001b[A\n 38%|████████████████▍                          | 32/84 [00:17<00:29,  1.75it/s]\u001b[A\n 44%|██████████████████▉                        | 37/84 [00:20<00:26,  1.78it/s]\u001b[A\n 39%|████████████████▉                          | 33/84 [00:18<00:28,  1.76it/s]\u001b[A\n 45%|███████████████████▍                       | 38/84 [00:20<00:25,  1.78it/s]\u001b[A\n 40%|█████████████████▍                         | 34/84 [00:19<00:28,  1.77it/s]\u001b[A\n 46%|███████████████████▉                       | 39/84 [00:21<00:25,  1.79it/s]\u001b[A\n 42%|█████████████████▉                         | 35/84 [00:19<00:27,  1.78it/s]\u001b[A\n 48%|████████████████████▍                      | 40/84 [00:21<00:25,  1.76it/s]\u001b[A\n 43%|██████████████████▍                        | 36/84 [00:20<00:26,  1.79it/s]\u001b[A\n 49%|████████████████████▉                      | 41/84 [00:22<00:23,  1.80it/s]\u001b[A\n 44%|██████████████████▉                        | 37/84 [00:20<00:27,  1.73it/s]\u001b[A\n 50%|█████████████████████▌                     | 42/84 [00:22<00:23,  1.78it/s]\u001b[A\n 45%|███████████████████▍                       | 38/84 [00:21<00:24,  1.86it/s]\u001b[A\n 46%|███████████████████▉                       | 39/84 [00:21<00:22,  1.97it/s]\u001b[A\n 51%|██████████████████████                     | 43/84 [00:23<00:26,  1.56it/s]\u001b[A\n 48%|████████████████████▍                      | 40/84 [00:22<00:22,  1.93it/s]\u001b[A\n 52%|██████████████████████▌                    | 44/84 [00:24<00:25,  1.60it/s]\u001b[A\n 49%|████████████████████▉                      | 41/84 [00:22<00:23,  1.87it/s]\u001b[A\n 54%|███████████████████████                    | 45/84 [00:24<00:23,  1.65it/s]\u001b[A\n 50%|█████████████████████▌                     | 42/84 [00:23<00:22,  1.84it/s]\u001b[A\n 55%|███████████████████████▌                   | 46/84 [00:25<00:22,  1.68it/s]\u001b[A\n 51%|██████████████████████                     | 43/84 [00:23<00:22,  1.83it/s]\u001b[A\n 56%|████████████████████████                   | 47/84 [00:25<00:21,  1.70it/s]\u001b[A\n 52%|██████████████████████▌                    | 44/84 [00:24<00:22,  1.79it/s]\u001b[A\n 57%|████████████████████████▌                  | 48/84 [00:26<00:20,  1.72it/s]\u001b[A\n 54%|███████████████████████                    | 45/84 [00:24<00:21,  1.79it/s]\u001b[A\n 58%|█████████████████████████                  | 49/84 [00:27<00:20,  1.74it/s]\u001b[A\n 55%|███████████████████████▌                   | 46/84 [00:25<00:21,  1.80it/s]\u001b[A\n 60%|█████████████████████████▌                 | 50/84 [00:27<00:19,  1.74it/s]\u001b[A\n 56%|████████████████████████                   | 47/84 [00:26<00:20,  1.80it/s]\u001b[A\n 61%|██████████████████████████                 | 51/84 [00:28<00:18,  1.78it/s]\u001b[A\n 57%|████████████████████████▌                  | 48/84 [00:26<00:19,  1.82it/s]\u001b[A\n 62%|██████████████████████████▌                | 52/84 [00:28<00:18,  1.70it/s]\u001b[A\n 58%|█████████████████████████                  | 49/84 [00:27<00:19,  1.84it/s]\u001b[A\n 63%|███████████████████████████▏               | 53/84 [00:29<00:17,  1.73it/s]\u001b[A\n 60%|█████████████████████████▌                 | 50/84 [00:27<00:18,  1.81it/s]\u001b[A\n 64%|███████████████████████████▋               | 54/84 [00:29<00:16,  1.77it/s]\u001b[A\n 61%|██████████████████████████                 | 51/84 [00:28<00:18,  1.78it/s]\u001b[A\n 65%|████████████████████████████▏              | 55/84 [00:30<00:16,  1.76it/s]\u001b[A\n 62%|██████████████████████████▌                | 52/84 [00:28<00:17,  1.78it/s]\u001b[A\n 67%|████████████████████████████▋              | 56/84 [00:31<00:15,  1.77it/s]\u001b[A\n 63%|███████████████████████████▏               | 53/84 [00:29<00:17,  1.76it/s]\u001b[A\n 68%|█████████████████████████████▏             | 57/84 [00:31<00:15,  1.78it/s]\u001b[A\n 64%|███████████████████████████▋               | 54/84 [00:30<00:16,  1.76it/s]\u001b[A\n 69%|█████████████████████████████▋             | 58/84 [00:32<00:14,  1.79it/s]\u001b[A\n 65%|████████████████████████████▏              | 55/84 [00:30<00:16,  1.78it/s]\u001b[A\n 70%|██████████████████████████████▏            | 59/84 [00:32<00:14,  1.79it/s]\u001b[A\n 67%|████████████████████████████▋              | 56/84 [00:31<00:15,  1.77it/s]\u001b[A\n 71%|██████████████████████████████▋            | 60/84 [00:33<00:13,  1.78it/s]\u001b[A\n 68%|█████████████████████████████▏             | 57/84 [00:31<00:15,  1.77it/s]\u001b[A\n 73%|███████████████████████████████▏           | 61/84 [00:33<00:12,  1.78it/s]\u001b[A\n 69%|█████████████████████████████▋             | 58/84 [00:32<00:14,  1.77it/s]\u001b[A\n 74%|███████████████████████████████▋           | 62/84 [00:34<00:12,  1.78it/s]\u001b[A\n 70%|██████████████████████████████▏            | 59/84 [00:32<00:14,  1.78it/s]\u001b[A\n 75%|████████████████████████████████▎          | 63/84 [00:34<00:11,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▋            | 60/84 [00:33<00:13,  1.77it/s]\u001b[A\n 76%|████████████████████████████████▊          | 64/84 [00:35<00:11,  1.78it/s]\u001b[A\n 73%|███████████████████████████████▏           | 61/84 [00:33<00:12,  1.78it/s]\u001b[A\n 77%|█████████████████████████████████▎         | 65/84 [00:36<00:10,  1.77it/s]\u001b[A\n 74%|███████████████████████████████▋           | 62/84 [00:34<00:12,  1.79it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 66/84 [00:36<00:10,  1.77it/s]\u001b[A\n 75%|████████████████████████████████▎          | 63/84 [00:35<00:11,  1.79it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 67/84 [00:37<00:09,  1.78it/s]\u001b[A\n 76%|████████████████████████████████▊          | 64/84 [00:35<00:11,  1.79it/s]\u001b[A\n 81%|██████████████████████████████████▊        | 68/84 [00:37<00:08,  1.79it/s]\u001b[A\n 77%|█████████████████████████████████▎         | 65/84 [00:36<00:10,  1.77it/s]\u001b[A\n 82%|███████████████████████████████████▎       | 69/84 [00:38<00:08,  1.78it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 66/84 [00:36<00:10,  1.74it/s]\u001b[A\n 83%|███████████████████████████████████▊       | 70/84 [00:38<00:07,  1.78it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 67/84 [00:37<00:09,  1.76it/s]\u001b[A\n 85%|████████████████████████████████████▎      | 71/84 [00:39<00:07,  1.77it/s]\u001b[A\n 81%|██████████████████████████████████▊        | 68/84 [00:37<00:09,  1.77it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 72/84 [00:40<00:06,  1.77it/s]\u001b[A\n 82%|███████████████████████████████████▎       | 69/84 [00:38<00:08,  1.78it/s]\u001b[A\n 87%|█████████████████████████████████████▎     | 73/84 [00:40<00:06,  1.78it/s]\u001b[A\n 83%|███████████████████████████████████▊       | 70/84 [00:39<00:07,  1.78it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 74/84 [00:41<00:05,  1.78it/s]\u001b[A\n 85%|████████████████████████████████████▎      | 71/84 [00:39<00:07,  1.78it/s]\u001b[A\n 89%|██████████████████████████████████████▍    | 75/84 [00:41<00:05,  1.78it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 72/84 [00:40<00:06,  1.78it/s]\u001b[A\n 90%|██████████████████████████████████████▉    | 76/84 [00:42<00:04,  1.77it/s]\u001b[A\n 87%|█████████████████████████████████████▎     | 73/84 [00:40<00:06,  1.78it/s]\u001b[A\n 92%|███████████████████████████████████████▍   | 77/84 [00:42<00:03,  1.78it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 74/84 [00:41<00:05,  1.79it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 78/84 [00:43<00:03,  1.78it/s]\u001b[A\n 89%|██████████████████████████████████████▍    | 75/84 [00:41<00:05,  1.78it/s]\u001b[A\n 94%|████████████████████████████████████████▍  | 79/84 [00:43<00:02,  1.76it/s]\u001b[A\n 90%|██████████████████████████████████████▉    | 76/84 [00:42<00:04,  1.79it/s]\u001b[A\n 95%|████████████████████████████████████████▉  | 80/84 [00:44<00:02,  1.78it/s]\u001b[A\n 92%|███████████████████████████████████████▍   | 77/84 [00:42<00:04,  1.75it/s]\u001b[A\n 96%|█████████████████████████████████████████▍ | 81/84 [00:45<00:01,  1.78it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 78/84 [00:43<00:03,  1.76it/s]\u001b[A\n 98%|█████████████████████████████████████████▉ | 82/84 [00:45<00:01,  1.78it/s]\u001b[A\n 94%|████████████████████████████████████████▍  | 79/84 [00:44<00:02,  1.76it/s]\u001b[A\n 99%|██████████████████████████████████████████▍| 83/84 [00:46<00:00,  1.79it/s]\u001b[A\n 95%|████████████████████████████████████████▉  | 80/84 [00:44<00:02,  1.75it/s]\u001b[A\n100%|███████████████████████████████████████████| 84/84 [00:46<00:00,  1.84it/s]\u001b[A\n 96%|█████████████████████████████████████████▍ | 81/84 [00:45<00:01,  1.96it/s]\u001b[A\n 98%|█████████████████████████████████████████▉ | 82/84 [00:45<00:00,  2.26it/s]\u001b[A\n 99%|██████████████████████████████████████████▍| 83/84 [00:45<00:00,  2.54it/s]\u001b[A\n100%|███████████████████████████████████████████| 84/84 [00:45<00:00,  2.84it/s]\u001b[ATrainer is attempting to log a value of \"{'precision': 0.5849514563106796, 'recall': 0.646112600536193, 'f1': 0.6140127388535033, 'number': 373}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7721518987341772, 'recall': 0.7922077922077922, 'f1': 0.7820512820512822, 'number': 154}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7086247086247086, 'recall': 0.8042328042328042, 'f1': 0.7534076827757126, 'number': 378}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7477477477477478, 'recall': 0.8440677966101695, 'f1': 0.7929936305732483, 'number': 295}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7064516129032258, 'recall': 0.8866396761133604, 'f1': 0.7863554757630162, 'number': 247}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8181818181818182, 'recall': 0.7748344370860927, 'f1': 0.7959183673469389, 'number': 151}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7949709864603481, 'recall': 0.8838709677419355, 'f1': 0.8370672097759675, 'number': 465}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7181571815718157, 'recall': 0.7220708446866485, 'f1': 0.720108695652174, 'number': 367}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7151767151767152, 'recall': 0.7944572748267898, 'f1': 0.7527352297592999, 'number': 433}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6171875, 'recall': 0.7559808612440191, 'f1': 0.6795698924731183, 'number': 209}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.015362013131380081, 'eval_ADDRESS': {'precision': 0.5849514563106796, 'recall': 0.646112600536193, 'f1': 0.6140127388535033, 'number': 373}, 'eval_BOOK': {'precision': 0.7721518987341772, 'recall': 0.7922077922077922, 'f1': 0.7820512820512822, 'number': 154}, 'eval_COMPANY': {'precision': 0.7086247086247086, 'recall': 0.8042328042328042, 'f1': 0.7534076827757126, 'number': 378}, 'eval_GAME': {'precision': 0.7477477477477478, 'recall': 0.8440677966101695, 'f1': 0.7929936305732483, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.7064516129032258, 'recall': 0.8866396761133604, 'f1': 0.7863554757630162, 'number': 247}, 'eval_MOVIE': {'precision': 0.8181818181818182, 'recall': 0.7748344370860927, 'f1': 0.7959183673469389, 'number': 151}, 'eval_NAME': {'precision': 0.7949709864603481, 'recall': 0.8838709677419355, 'f1': 0.8370672097759675, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.7181571815718157, 'recall': 0.7220708446866485, 'f1': 0.720108695652174, 'number': 367}, 'eval_POSITION': {'precision': 0.7151767151767152, 'recall': 0.7944572748267898, 'f1': 0.7527352297592999, 'number': 433}, 'eval_SCENE': {'precision': 0.6171875, 'recall': 0.7559808612440191, 'f1': 0.6795698924731183, 'number': 209}, 'eval_overall_precision': 0.7130281690140845, 'eval_overall_recall': 0.791015625, 'eval_overall_f1': 0.7499999999999999, 'eval_overall_accuracy': 0.9953855058637379, 'eval_runtime': 54.268, 'eval_samples_per_second': 24.748, 'eval_steps_per_second': 1.548, 'epoch': 2.0}\n 67%|██████████████████████████             | 1344/2016 [44:50<20:13,  1.81s/it]\n100%|███████████████████████████████████████████| 84/84 [00:53<00:00,  1.84it/s]\u001b[A\n 67%|████████████████████████▋            | 1346/2016 [44:51<2:22:23, 12.75s/it]\u001b[ATrainer is attempting to log a value of \"{'precision': 0.5586206896551724, 'recall': 0.6514745308310992, 'f1': 0.6014851485148515, 'number': 373}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.782608695652174, 'recall': 0.8181818181818182, 'f1': 0.8, 'number': 154}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7216981132075472, 'recall': 0.8095238095238095, 'f1': 0.7630922693266834, 'number': 378}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7723076923076924, 'recall': 0.8508474576271187, 'f1': 0.8096774193548387, 'number': 295}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6925566343042071, 'recall': 0.8663967611336032, 'f1': 0.7697841726618705, 'number': 247}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7785234899328859, 'recall': 0.7682119205298014, 'f1': 0.7733333333333333, 'number': 151}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8076923076923077, 'recall': 0.9032258064516129, 'f1': 0.852791878172589, 'number': 465}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.687960687960688, 'recall': 0.7629427792915532, 'f1': 0.7235142118863049, 'number': 367}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7322175732217573, 'recall': 0.8083140877598153, 'f1': 0.7683863885839736, 'number': 433}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6293103448275862, 'recall': 0.6985645933014354, 'f1': 0.6621315192743764, 'number': 209}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.015471361577510834, 'eval_ADDRESS': {'precision': 0.5586206896551724, 'recall': 0.6514745308310992, 'f1': 0.6014851485148515, 'number': 373}, 'eval_BOOK': {'precision': 0.782608695652174, 'recall': 0.8181818181818182, 'f1': 0.8, 'number': 154}, 'eval_COMPANY': {'precision': 0.7216981132075472, 'recall': 0.8095238095238095, 'f1': 0.7630922693266834, 'number': 378}, 'eval_GAME': {'precision': 0.7723076923076924, 'recall': 0.8508474576271187, 'f1': 0.8096774193548387, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6925566343042071, 'recall': 0.8663967611336032, 'f1': 0.7697841726618705, 'number': 247}, 'eval_MOVIE': {'precision': 0.7785234899328859, 'recall': 0.7682119205298014, 'f1': 0.7733333333333333, 'number': 151}, 'eval_NAME': {'precision': 0.8076923076923077, 'recall': 0.9032258064516129, 'f1': 0.852791878172589, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.687960687960688, 'recall': 0.7629427792915532, 'f1': 0.7235142118863049, 'number': 367}, 'eval_POSITION': {'precision': 0.7322175732217573, 'recall': 0.8083140877598153, 'f1': 0.7683863885839736, 'number': 433}, 'eval_SCENE': {'precision': 0.6293103448275862, 'recall': 0.6985645933014354, 'f1': 0.6621315192743764, 'number': 209}, 'eval_overall_precision': 0.7127906976744186, 'eval_overall_recall': 0.7981770833333334, 'eval_overall_f1': 0.7530712530712531, 'eval_overall_accuracy': 0.9954873068689502, 'eval_runtime': 54.4575, 'eval_samples_per_second': 24.661, 'eval_steps_per_second': 1.542, 'epoch': 2.0}\n 67%|██████████████████████████             | 1344/2016 [44:52<20:11,  1.80s/it]\n100%|███████████████████████████████████████████| 84/84 [00:53<00:00,  2.84it/s]\u001b[A\n{'loss': 0.0113, 'grad_norm': 10768.2255859375, 'learning_rate': 1.3491649269311067e-05, 'epoch': 2.23}\n{'loss': 0.011, 'grad_norm': 9904.142578125, 'learning_rate': 1.3491649269311067e-05, 'epoch': 2.23}\n 74%|█████████████████████████████          | 1500/2016 [49:49<11:47,  1.37s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n 75%|█████████████████████████████          | 1503/2016 [49:52<13:20,  1.56s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.0073, 'grad_norm': 10406.9091796875, 'learning_rate': 4.4363256784968686e-07, 'epoch': 2.98}\n 99%|████████████████████████████████████▋| 1998/2016 [1:05:47<00:24,  1.38s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.0073, 'grad_norm': 6737.96533203125, 'learning_rate': 4.4363256784968686e-07, 'epoch': 2.98}\n 99%|████████████████████████████████████▊| 2005/2016 [1:05:53<00:15,  1.39s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n100%|████████████████████████████████████▉| 2012/2016 [1:06:15<00:06,  1.52s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n\n100%|████████████████████████████████████▉| 2013/2016 [1:06:16<00:04,  1.43s/it]\u001b[A\n  2%|█                                           | 2/84 [00:00<00:21,  3.85it/s]\u001b[A\n  4%|█▌                                          | 3/84 [00:01<00:32,  2.49it/s]\u001b[A\n100%|████████████████████████████████████▉| 2014/2016 [1:06:18<00:03,  1.55s/it]\u001b[A\n  6%|██▌                                         | 5/84 [00:02<00:41,  1.91it/s]\u001b[A\n  7%|███▏                                        | 6/84 [00:02<00:43,  1.81it/s]\u001b[A\n100%|████████████████████████████████████▉| 2015/2016 [1:06:20<00:01,  1.66s/it]\u001b[A\n 10%|████▏                                       | 8/84 [00:04<00:43,  1.75it/s]\u001b[A\n 11%|████▋                                       | 9/84 [00:04<00:43,  1.74it/s]\u001b[A\n100%|█████████████████████████████████████| 2016/2016 [1:06:22<00:00,  1.63s/it]\u001b[A\n 13%|█████▋                                     | 11/84 [00:05<00:35,  2.05it/s]\u001b[A\n 14%|██████▏                                    | 12/84 [00:05<00:31,  2.31it/s]\u001b[A\n 15%|██████▋                                    | 13/84 [00:06<00:28,  2.52it/s]\u001b[A\n 17%|███████▏                                   | 14/84 [00:06<00:25,  2.75it/s]\u001b[A\n 18%|███████▋                                   | 15/84 [00:06<00:23,  2.95it/s]\u001b[A\n 19%|████████▏                                  | 16/84 [00:07<00:21,  3.10it/s]\u001b[A\n 20%|████████▋                                  | 17/84 [00:07<00:20,  3.21it/s]\u001b[A\n 21%|█████████▏                                 | 18/84 [00:07<00:20,  3.30it/s]\u001b[A\n 23%|█████████▋                                 | 19/84 [00:07<00:19,  3.37it/s]\u001b[A\n 24%|██████████▏                                | 20/84 [00:08<00:18,  3.42it/s]\u001b[A\n 25%|██████████▊                                | 21/84 [00:08<00:18,  3.42it/s]\u001b[A\n 26%|███████████▎                               | 22/84 [00:08<00:18,  3.41it/s]\u001b[A/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n\n 27%|███████████▊                               | 23/84 [00:09<00:20,  2.97it/s]\u001b[A\n  0%|                                                    | 0/84 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▎                              | 24/84 [00:09<00:24,  2.48it/s]\u001b[A\n  2%|█                                           | 2/84 [00:00<00:22,  3.58it/s]\u001b[A\n 30%|████████████▊                              | 25/84 [00:10<00:26,  2.22it/s]\u001b[A\n  4%|█▌                                          | 3/84 [00:01<00:32,  2.53it/s]\u001b[A\n 31%|█████████████▎                             | 26/84 [00:10<00:28,  2.07it/s]\u001b[A\n  5%|██                                          | 4/84 [00:01<00:36,  2.21it/s]\u001b[A\n 32%|█████████████▊                             | 27/84 [00:11<00:29,  1.95it/s]\u001b[A\n  6%|██▌                                         | 5/84 [00:02<00:38,  2.06it/s]\u001b[A\n 33%|██████████████▎                            | 28/84 [00:12<00:29,  1.90it/s]\u001b[A\n  7%|███▏                                        | 6/84 [00:02<00:39,  1.97it/s]\u001b[A\n 35%|██████████████▊                            | 29/84 [00:12<00:29,  1.86it/s]\u001b[A\n  8%|███▋                                        | 7/84 [00:03<00:40,  1.91it/s]\u001b[A\n 36%|███████████████▎                           | 30/84 [00:13<00:29,  1.84it/s]\u001b[A\n 10%|████▏                                       | 8/84 [00:03<00:40,  1.88it/s]\u001b[A\n 37%|███████████████▊                           | 31/84 [00:13<00:28,  1.84it/s]\u001b[A\n 11%|████▋                                       | 9/84 [00:04<00:41,  1.80it/s]\u001b[A\n 38%|████████████████▍                          | 32/84 [00:14<00:28,  1.82it/s]\u001b[A\n 12%|█████                                      | 10/84 [00:05<00:41,  1.80it/s]\u001b[A\n 39%|████████████████▉                          | 33/84 [00:14<00:28,  1.82it/s]\u001b[A\n 13%|█████▋                                     | 11/84 [00:05<00:40,  1.79it/s]\u001b[A\n 40%|█████████████████▍                         | 34/84 [00:15<00:27,  1.81it/s]\u001b[A\n 14%|██████▏                                    | 12/84 [00:06<00:40,  1.78it/s]\u001b[A\n 42%|█████████████████▉                         | 35/84 [00:15<00:27,  1.80it/s]\u001b[A\n 15%|██████▋                                    | 13/84 [00:06<00:39,  1.78it/s]\u001b[A\n 43%|██████████████████▍                        | 36/84 [00:16<00:26,  1.80it/s]\u001b[A\n 17%|███████▏                                   | 14/84 [00:07<00:39,  1.77it/s]\u001b[A\n 44%|██████████████████▉                        | 37/84 [00:17<00:26,  1.80it/s]\u001b[A\n 18%|███████▋                                   | 15/84 [00:07<00:38,  1.79it/s]\u001b[A\n 45%|███████████████████▍                       | 38/84 [00:17<00:25,  1.78it/s]\u001b[A\n 19%|████████▏                                  | 16/84 [00:08<00:37,  1.82it/s]\u001b[A\n 46%|███████████████████▉                       | 39/84 [00:18<00:25,  1.76it/s]\u001b[A\n 20%|████████▋                                  | 17/84 [00:08<00:36,  1.82it/s]\u001b[A\n 48%|████████████████████▍                      | 40/84 [00:18<00:24,  1.78it/s]\u001b[A\n 21%|█████████▏                                 | 18/84 [00:09<00:36,  1.79it/s]\u001b[A\n 49%|████████████████████▉                      | 41/84 [00:19<00:24,  1.76it/s]\u001b[A\n 23%|█████████▋                                 | 19/84 [00:10<00:37,  1.74it/s]\u001b[A\n 50%|█████████████████████▌                     | 42/84 [00:19<00:23,  1.78it/s]\u001b[A\n 24%|██████████▏                                | 20/84 [00:10<00:36,  1.74it/s]\u001b[A\n 51%|██████████████████████                     | 43/84 [00:20<00:23,  1.78it/s]\u001b[A\n 25%|██████████▊                                | 21/84 [00:11<00:36,  1.74it/s]\u001b[A\n 52%|██████████████████████▌                    | 44/84 [00:21<00:22,  1.78it/s]\u001b[A\n 26%|███████████▎                               | 22/84 [00:11<00:35,  1.75it/s]\u001b[A\n 54%|███████████████████████                    | 45/84 [00:21<00:21,  1.78it/s]\u001b[A\n 27%|███████████▊                               | 23/84 [00:12<00:34,  1.78it/s]\u001b[A\n 55%|███████████████████████▌                   | 46/84 [00:22<00:21,  1.76it/s]\u001b[A\n 29%|████████████▎                              | 24/84 [00:12<00:33,  1.79it/s]\u001b[A\n 56%|████████████████████████                   | 47/84 [00:22<00:20,  1.77it/s]\u001b[A\n 30%|████████████▊                              | 25/84 [00:13<00:32,  1.79it/s]\u001b[A\n 57%|████████████████████████▌                  | 48/84 [00:23<00:20,  1.77it/s]\u001b[A\n 31%|█████████████▎                             | 26/84 [00:14<00:32,  1.79it/s]\u001b[A\n 58%|█████████████████████████                  | 49/84 [00:23<00:19,  1.78it/s]\u001b[A\n 32%|█████████████▊                             | 27/84 [00:14<00:32,  1.77it/s]\u001b[A\n 60%|█████████████████████████▌                 | 50/84 [00:24<00:19,  1.78it/s]\u001b[A\n 33%|██████████████▎                            | 28/84 [00:15<00:31,  1.78it/s]\u001b[A\n 61%|██████████████████████████                 | 51/84 [00:24<00:18,  1.78it/s]\u001b[A\n 35%|██████████████▊                            | 29/84 [00:15<00:30,  1.78it/s]\u001b[A\n 62%|██████████████████████████▌                | 52/84 [00:25<00:17,  1.79it/s]\u001b[A\n 36%|███████████████▎                           | 30/84 [00:16<00:30,  1.74it/s]\u001b[A\n 63%|███████████████████████████▏               | 53/84 [00:26<00:17,  1.79it/s]\u001b[A\n 37%|███████████████▊                           | 31/84 [00:16<00:30,  1.76it/s]\u001b[A\n 64%|███████████████████████████▋               | 54/84 [00:26<00:16,  1.79it/s]\u001b[A\n 38%|████████████████▍                          | 32/84 [00:17<00:29,  1.77it/s]\u001b[A\n 65%|████████████████████████████▏              | 55/84 [00:27<00:16,  1.77it/s]\u001b[A\n 39%|████████████████▉                          | 33/84 [00:17<00:28,  1.79it/s]\u001b[A\n 67%|████████████████████████████▋              | 56/84 [00:27<00:15,  1.76it/s]\u001b[A\n 40%|█████████████████▍                         | 34/84 [00:18<00:28,  1.78it/s]\u001b[A\n 68%|█████████████████████████████▏             | 57/84 [00:28<00:15,  1.77it/s]\u001b[A\n 42%|█████████████████▉                         | 35/84 [00:19<00:27,  1.77it/s]\u001b[A\n 69%|█████████████████████████████▋             | 58/84 [00:28<00:14,  1.78it/s]\u001b[A\n 43%|██████████████████▍                        | 36/84 [00:19<00:27,  1.75it/s]\u001b[A\n 70%|██████████████████████████████▏            | 59/84 [00:29<00:14,  1.78it/s]\u001b[A\n 44%|██████████████████▉                        | 37/84 [00:20<00:26,  1.76it/s]\u001b[A\n 71%|██████████████████████████████▋            | 60/84 [00:30<00:13,  1.77it/s]\u001b[A\n 45%|███████████████████▍                       | 38/84 [00:20<00:25,  1.77it/s]\u001b[A\n 73%|███████████████████████████████▏           | 61/84 [00:30<00:12,  1.77it/s]\u001b[A\n 46%|███████████████████▉                       | 39/84 [00:21<00:25,  1.78it/s]\u001b[A\n 74%|███████████████████████████████▋           | 62/84 [00:31<00:12,  1.77it/s]\u001b[A\n 48%|████████████████████▍                      | 40/84 [00:21<00:24,  1.78it/s]\u001b[A\n 75%|████████████████████████████████▎          | 63/84 [00:31<00:11,  1.77it/s]\u001b[A\n 49%|████████████████████▉                      | 41/84 [00:22<00:24,  1.79it/s]\u001b[A\n 76%|████████████████████████████████▊          | 64/84 [00:32<00:11,  1.78it/s]\u001b[A\n 50%|█████████████████████▌                     | 42/84 [00:23<00:23,  1.78it/s]\u001b[A\n 77%|█████████████████████████████████▎         | 65/84 [00:32<00:10,  1.77it/s]\u001b[A\n 51%|██████████████████████                     | 43/84 [00:23<00:23,  1.72it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 66/84 [00:33<00:10,  1.77it/s]\u001b[A\n 52%|██████████████████████▌                    | 44/84 [00:24<00:22,  1.75it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 67/84 [00:34<00:09,  1.73it/s]\u001b[A\n 54%|███████████████████████                    | 45/84 [00:24<00:22,  1.76it/s]\u001b[A\n 81%|██████████████████████████████████▊        | 68/84 [00:34<00:09,  1.75it/s]\u001b[A\n 55%|███████████████████████▌                   | 46/84 [00:25<00:21,  1.76it/s]\u001b[A\n 82%|███████████████████████████████████▎       | 69/84 [00:35<00:08,  1.76it/s]\u001b[A\n 56%|████████████████████████                   | 47/84 [00:25<00:20,  1.77it/s]\u001b[A\n 83%|███████████████████████████████████▊       | 70/84 [00:35<00:07,  1.76it/s]\u001b[A\n 57%|████████████████████████▌                  | 48/84 [00:26<00:20,  1.77it/s]\u001b[A\n 85%|████████████████████████████████████▎      | 71/84 [00:36<00:07,  1.77it/s]\u001b[A\n 58%|█████████████████████████                  | 49/84 [00:27<00:19,  1.76it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 72/84 [00:36<00:06,  1.79it/s]\u001b[A\n 60%|█████████████████████████▌                 | 50/84 [00:27<00:19,  1.77it/s]\u001b[A\n 87%|█████████████████████████████████████▎     | 73/84 [00:37<00:06,  1.74it/s]\u001b[A\n 61%|██████████████████████████                 | 51/84 [00:28<00:18,  1.77it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 74/84 [00:37<00:05,  1.75it/s]\u001b[A\n 62%|██████████████████████████▌                | 52/84 [00:28<00:18,  1.77it/s]\u001b[A\n 89%|██████████████████████████████████████▍    | 75/84 [00:38<00:05,  1.76it/s]\u001b[A\n 63%|███████████████████████████▏               | 53/84 [00:29<00:17,  1.76it/s]\u001b[A\n 90%|██████████████████████████████████████▉    | 76/84 [00:39<00:04,  1.76it/s]\u001b[A\n 64%|███████████████████████████▋               | 54/84 [00:29<00:16,  1.77it/s]\u001b[A\n 92%|███████████████████████████████████████▍   | 77/84 [00:39<00:03,  1.76it/s]\u001b[A\n 65%|████████████████████████████▏              | 55/84 [00:30<00:16,  1.78it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 78/84 [00:40<00:03,  1.77it/s]\u001b[A\n 67%|████████████████████████████▋              | 56/84 [00:31<00:15,  1.78it/s]\u001b[A\n 94%|████████████████████████████████████████▍  | 79/84 [00:40<00:02,  1.78it/s]\u001b[A\n 68%|█████████████████████████████▏             | 57/84 [00:31<00:15,  1.78it/s]\u001b[A\n 95%|████████████████████████████████████████▉  | 80/84 [00:41<00:02,  1.77it/s]\u001b[A\n 69%|█████████████████████████████▋             | 58/84 [00:32<00:14,  1.79it/s]\u001b[A\n 96%|█████████████████████████████████████████▍ | 81/84 [00:41<00:01,  1.77it/s]\u001b[A\n 70%|██████████████████████████████▏            | 59/84 [00:32<00:14,  1.79it/s]\u001b[A\n 98%|█████████████████████████████████████████▉ | 82/84 [00:42<00:01,  1.78it/s]\u001b[A\n 71%|██████████████████████████████▋            | 60/84 [00:33<00:13,  1.79it/s]\u001b[A\n 99%|██████████████████████████████████████████▍| 83/84 [00:43<00:00,  1.77it/s]\u001b[A\n 73%|███████████████████████████████▏           | 61/84 [00:33<00:13,  1.76it/s]\u001b[A\n100%|███████████████████████████████████████████| 84/84 [00:43<00:00,  1.85it/s]\u001b[A\n 74%|███████████████████████████████▋           | 62/84 [00:34<00:11,  1.93it/s]\u001b[A\n 75%|████████████████████████████████▎          | 63/84 [00:34<00:09,  2.24it/s]\u001b[A\n 76%|████████████████████████████████▊          | 64/84 [00:34<00:07,  2.53it/s]\u001b[A\n 77%|█████████████████████████████████▎         | 65/84 [00:35<00:06,  2.77it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 66/84 [00:35<00:06,  2.97it/s]\u001b[A\n 80%|██████████████████████████████████▎        | 67/84 [00:35<00:05,  3.12it/s]\u001b[A\n 81%|██████████████████████████████████▊        | 68/84 [00:35<00:04,  3.23it/s]\u001b[A\n 82%|███████████████████████████████████▎       | 69/84 [00:36<00:04,  3.32it/s]\u001b[A\n 83%|███████████████████████████████████▊       | 70/84 [00:36<00:04,  3.37it/s]\u001b[A\n 85%|████████████████████████████████████▎      | 71/84 [00:36<00:03,  3.42it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 72/84 [00:37<00:03,  3.45it/s]\u001b[A\n 87%|█████████████████████████████████████▎     | 73/84 [00:37<00:03,  3.43it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 74/84 [00:37<00:02,  3.46it/s]\u001b[A\n 89%|██████████████████████████████████████▍    | 75/84 [00:37<00:02,  3.48it/s]\u001b[A\n 90%|██████████████████████████████████████▉    | 76/84 [00:38<00:02,  3.49it/s]\u001b[A\n 92%|███████████████████████████████████████▍   | 77/84 [00:38<00:01,  3.50it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 78/84 [00:38<00:01,  3.50it/s]\u001b[A\n 94%|████████████████████████████████████████▍  | 79/84 [00:39<00:01,  3.51it/s]\u001b[A\n 95%|████████████████████████████████████████▉  | 80/84 [00:39<00:01,  3.52it/s]\u001b[A\n 96%|█████████████████████████████████████████▍ | 81/84 [00:39<00:00,  3.52it/s]\u001b[A\n 98%|█████████████████████████████████████████▉ | 82/84 [00:39<00:00,  3.47it/s]\u001b[A\n 99%|██████████████████████████████████████████▍| 83/84 [00:40<00:00,  3.45it/s]\u001b[A\n100%|███████████████████████████████████████████| 84/84 [00:40<00:00,  3.56it/s]\u001b[ATrainer is attempting to log a value of \"{'precision': 0.572429906542056, 'recall': 0.6568364611260054, 'f1': 0.6117353308364545, 'number': 373}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7439024390243902, 'recall': 0.7922077922077922, 'f1': 0.7672955974842767, 'number': 154}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7152941176470589, 'recall': 0.8042328042328042, 'f1': 0.7571606475716065, 'number': 378}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7351190476190477, 'recall': 0.8372881355932204, 'f1': 0.7828843106180665, 'number': 295}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7294520547945206, 'recall': 0.8623481781376519, 'f1': 0.7903525046382189, 'number': 247}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8, 'recall': 0.7947019867549668, 'f1': 0.79734219269103, 'number': 151}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8316633266533067, 'recall': 0.8924731182795699, 'f1': 0.8609958506224067, 'number': 465}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7175572519083969, 'recall': 0.7683923705722071, 'f1': 0.7421052631578947, 'number': 367}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7579617834394905, 'recall': 0.8244803695150116, 'f1': 0.7898230088495576, 'number': 433}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6551724137931034, 'recall': 0.7272727272727273, 'f1': 0.6893424036281179, 'number': 209}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.01633627712726593, 'eval_ADDRESS': {'precision': 0.572429906542056, 'recall': 0.6568364611260054, 'f1': 0.6117353308364545, 'number': 373}, 'eval_BOOK': {'precision': 0.7439024390243902, 'recall': 0.7922077922077922, 'f1': 0.7672955974842767, 'number': 154}, 'eval_COMPANY': {'precision': 0.7152941176470589, 'recall': 0.8042328042328042, 'f1': 0.7571606475716065, 'number': 378}, 'eval_GAME': {'precision': 0.7351190476190477, 'recall': 0.8372881355932204, 'f1': 0.7828843106180665, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.7294520547945206, 'recall': 0.8623481781376519, 'f1': 0.7903525046382189, 'number': 247}, 'eval_MOVIE': {'precision': 0.8, 'recall': 0.7947019867549668, 'f1': 0.79734219269103, 'number': 151}, 'eval_NAME': {'precision': 0.8316633266533067, 'recall': 0.8924731182795699, 'f1': 0.8609958506224067, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.7175572519083969, 'recall': 0.7683923705722071, 'f1': 0.7421052631578947, 'number': 367}, 'eval_POSITION': {'precision': 0.7579617834394905, 'recall': 0.8244803695150116, 'f1': 0.7898230088495576, 'number': 433}, 'eval_SCENE': {'precision': 0.6551724137931034, 'recall': 0.7272727272727273, 'f1': 0.6893424036281179, 'number': 209}, 'eval_overall_precision': 0.7247787610619469, 'eval_overall_recall': 0.7998046875, 'eval_overall_f1': 0.7604456824512534, 'eval_overall_accuracy': 0.9955672933730454, 'eval_runtime': 52.0464, 'eval_samples_per_second': 25.804, 'eval_steps_per_second': 1.614, 'epoch': 3.0}\n100%|█████████████████████████████████████| 2016/2016 [1:07:08<00:00,  1.78s/it]\n100%|███████████████████████████████████████████| 84/84 [00:51<00:00,  1.85it/s]\u001b[A\n{'train_runtime': 4028.2509, 'train_samples_per_second': 8.004, 'train_steps_per_second': 0.5, 'train_loss': 0.042449698716934235, 'epoch': 3.0}\n100%|█████████████████████████████████████| 2016/2016 [1:07:08<00:00,  2.00s/it]\n[rank0]:[W614 00:05:47.733808941 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTrainer is attempting to log a value of \"{'precision': 0.5510688836104513, 'recall': 0.6219839142091153, 'f1': 0.5843828715365238, 'number': 373}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7948717948717948, 'recall': 0.8051948051948052, 'f1': 0.8, 'number': 154}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7279236276849642, 'recall': 0.8068783068783069, 'f1': 0.7653701380175659, 'number': 378}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7626112759643917, 'recall': 0.8711864406779661, 'f1': 0.8132911392405063, 'number': 295}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7653429602888087, 'recall': 0.8582995951417004, 'f1': 0.8091603053435115, 'number': 247}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8223684210526315, 'recall': 0.8278145695364238, 'f1': 0.8250825082508252, 'number': 151}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8095238095238095, 'recall': 0.8774193548387097, 'f1': 0.8421052631578948, 'number': 465}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7323232323232324, 'recall': 0.7901907356948229, 'f1': 0.7601572739187418, 'number': 367}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7399577167019028, 'recall': 0.8083140877598153, 'f1': 0.772626931567329, 'number': 433}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6304347826086957, 'recall': 0.69377990430622, 'f1': 0.6605922551252847, 'number': 209}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.01656382717192173, 'eval_ADDRESS': {'precision': 0.5510688836104513, 'recall': 0.6219839142091153, 'f1': 0.5843828715365238, 'number': 373}, 'eval_BOOK': {'precision': 0.7948717948717948, 'recall': 0.8051948051948052, 'f1': 0.8, 'number': 154}, 'eval_COMPANY': {'precision': 0.7279236276849642, 'recall': 0.8068783068783069, 'f1': 0.7653701380175659, 'number': 378}, 'eval_GAME': {'precision': 0.7626112759643917, 'recall': 0.8711864406779661, 'f1': 0.8132911392405063, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.7653429602888087, 'recall': 0.8582995951417004, 'f1': 0.8091603053435115, 'number': 247}, 'eval_MOVIE': {'precision': 0.8223684210526315, 'recall': 0.8278145695364238, 'f1': 0.8250825082508252, 'number': 151}, 'eval_NAME': {'precision': 0.8095238095238095, 'recall': 0.8774193548387097, 'f1': 0.8421052631578948, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.7323232323232324, 'recall': 0.7901907356948229, 'f1': 0.7601572739187418, 'number': 367}, 'eval_POSITION': {'precision': 0.7399577167019028, 'recall': 0.8083140877598153, 'f1': 0.772626931567329, 'number': 433}, 'eval_SCENE': {'precision': 0.6304347826086957, 'recall': 0.69377990430622, 'f1': 0.6605922551252847, 'number': 209}, 'eval_overall_precision': 0.7274888558692422, 'eval_overall_recall': 0.796875, 'eval_overall_f1': 0.7606027652633213, 'eval_overall_accuracy': 0.9955309358711839, 'eval_runtime': 48.7703, 'eval_samples_per_second': 27.537, 'eval_steps_per_second': 1.722, 'epoch': 3.0}\n100%|█████████████████████████████████████| 2016/2016 [1:07:14<00:00,  1.63s/it]\n100%|███████████████████████████████████████████| 84/84 [00:48<00:00,  3.56it/s]\u001b[A\n{'train_runtime': 4034.4592, 'train_samples_per_second': 7.992, 'train_steps_per_second': 0.5, 'train_loss': 0.036815898522498114, 'epoch': 3.0}\n100%|█████████████████████████████████████| 2016/2016 [1:07:14<00:00,  2.00s/it]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}