{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":318737,"sourceType":"datasetVersion","datasetId":134082}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://blog.csdn.net/qq_24951479/article/details/132495408\n\n第十一周作业：\n1. 参考课堂案例，使用指定的数据集，编写代码实现ner模型训练和推流。\nhttps://huggingface.co/datasets/doushabao4766/msra_ner_k_V3\n2. 完成预测结果的实体抽取。\n    输入：“双方确定了今后发展中美关系的指导方针。”\n    输出：[{\"entity\":\"ORG\",\"content\":\"中\"},{\"entity\":\"ORG\",\"content\":\"美\"}]\n3. 整理Dataset、Trainer、TrainingArgument、DataCollator、Evaluate 知识点，总结文档（无需提交）","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:49:53.669973Z","iopub.execute_input":"2025-05-31T11:49:53.670204Z","iopub.status.idle":"2025-05-31T11:49:56.400603Z","shell.execute_reply.started":"2025-05-31T11:49:53.670179Z","shell.execute_reply":"2025-05-31T11:49:56.400007Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 导入相关包","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.optim import Adam  # 优化器\nfrom transformers import AutoTokenizer , AutoModelForSequenceClassification  # 分词器\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter #Tensorboard 跟踪\nfrom sklearn.model_selection import train_test_split # 自动拆分测试集和验证集\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:49:56.401234Z","iopub.execute_input":"2025-05-31T11:49:56.401515Z","iopub.status.idle":"2025-05-31T11:50:35.800117Z","shell.execute_reply.started":"2025-05-31T11:49:56.401477Z","shell.execute_reply":"2025-05-31T11:50:35.799347Z"}},"outputs":[{"name":"stderr","text":"2025-05-31 11:50:19.471785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748692219.924268      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748692220.052600      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# 数据预处理","metadata":{}},{"cell_type":"code","source":"# 读取表格数据\ndata = pd.read_excel('/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx')\n\n# data.head()  # 查看读取出来的数据\n\n# data.columns  # 查看所有的列名\n\ndata_content = data['评价内容(content)'] != '此用户未填写评价内容'  # 获取评价内容，并且存入data_content中,去掉'此用户未填写评价内容'\ndata_two = data[data_content][['评价内容(content)' , '评分（总分5分）(score)']]  # 将评价内容和评分提取出来\n\n\ndata_ts = data_two.values  # 将数据转换为数组\ntrain , test = train_test_split(data_ts)\n\nprint(train.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:50:35.801622Z","iopub.execute_input":"2025-05-31T11:50:35.802109Z","iopub.status.idle":"2025-05-31T11:50:45.520043Z","shell.execute_reply.started":"2025-05-31T11:50:35.802091Z","shell.execute_reply":"2025-05-31T11:50:45.519238Z"}},"outputs":[{"name":"stdout","text":"(33316, 2)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 创建分词器以及将数据转换成DataLoader","metadata":{}},{"cell_type":"code","source":"# 分词器\ntokenzier = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n\n# 创建自定义DataLoader方法 collatefron = \ndef warp_data(btach_data):\n    comments , lables = [] , []\n    for b in btach_data:\n        comments.append(b[0])\n        lables.append(int(b[1])-1)  # 1-5分 所以标签取值0-4\n\n    # 转换成模型输入的数据\n    input_data = tokenzier(comments , return_tensors = 'pt' , # 返回ptorch类型的数据\n                          padding = True , # 自动补全\n                          truncation = True , # 超过最大值512 允许截断\n                          max_length = 512)\n    input_labels = torch.tensor(lables)\n    return input_data , input_labels\n\n# 装换成DataLoader\ntrain_dl = DataLoader(train , batch_size = 20 , shuffle = True , collate_fn = warp_data)\ntest_dl = DataLoader(test , batch_size = 20 , shuffle = False , collate_fn = warp_data)\n\nfor item in test_dl:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:50:45.520872Z","iopub.execute_input":"2025-05-31T11:50:45.521321Z","iopub.status.idle":"2025-05-31T11:50:47.265980Z","shell.execute_reply.started":"2025-05-31T11:50:45.521302Z","shell.execute_reply":"2025-05-31T11:50:47.265286Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/19.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3fc12f749d0451da5f92639d38759d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4672b82ef9534e0f9a00845fff4114fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b66672177d3d4e3c8c5a2cdd5921995c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d11ee912ef604af6917a17df1e54dff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68efea9e12534937a8579c5a406acf8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11d2fa009a3e41fe8051e1efc4c15904"}},"metadata":{}},{"name":"stdout","text":"({'input_ids': tensor([[ 101, 3418, 3315, 2218,  679,  833, 4178, 8024, 2697, 6230, 1008,  137,\n         4638, 8024, 2145, 3302, 2523, 4178, 2658,  102,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101,  691, 6205,  679, 7231, 1557, 8024, 3173, 2399, 2571,  727, 8024,\n          674,  752, 1963, 2692,  511,  809, 1400, 1914, 1914,  912, 2139,  102,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 6820, 1377,  809, 1416, 8024, 1057, 2797,  738,  679, 3221, 6929,\n         1408, 2141, 4500,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 2644, 3766, 3300, 1856, 1091, 1079, 2159, 8024, 7949, 6371, 1962,\n         6397,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101,  691, 6205, 2523, 1962, 2523, 6844, 1394, 2111, 2094, 2110,  739,\n         2972, 5773, 7471, 5001, 3302, 1218,  679, 7231, 1962,  782,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 1961, 2523, 1599, 3614,  102,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 1259, 6163,  679, 7231, 8024, 2923, 1962, 4638, 8024,  976, 2339,\n         7478, 2382, 4638, 1962, 8024,  772, 1501, 6574, 7030, 5314, 1213,  102,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101,  976, 2768, 1959, 3212, 8024, 1456, 6887,  679, 5543, 3291, 3472,\n          749, 8013,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 5018,  671, 3613, 6579,  743, 8024, 1355, 6573, 2523, 2571,  691,\n         6205, 3119, 1168,  722, 1400, 7716,  677, 3389, 4692,  749, 8024, 1469,\n         7564, 2682, 4638,  671, 3416,  102,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 2600,  860, 6820, 6121, 1416, 2218, 4761,  722, 1184, 3300,  697,\n         1898, 1631, 1631, 4638, 1898, 7509, 4680, 1184, 3766, 7309, 7579,  749,\n         1400, 5330,  886, 4500, 1086, 6397, 6389,  102,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 1962, 1962, 1962, 1962, 1962, 1962,  102,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 2644, 3766, 3300, 1856, 1091, 1079, 2159, 8024, 7949, 6371, 1962,\n         6397,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 3297, 6374, 1328,  743, 2130, 2218, 7360,  817,  102,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 1501, 4277,  772, 1501, 6574, 7030,  924, 6395,  966, 2533,  928,\n         6609, 8013,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 4500, 6629, 3341, 2523, 3175,  912, 2940, 3717, 3808, 4638, 1898,\n         7509, 4924, 2544, 3300,  763, 1920, 2600,  860, 3341, 6432, 6820, 3221,\n         1962, 6397,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 2376, 3301, 1351,  743, 4638,  800, 6432, 6820, 1377,  809, 1416,\n          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 2644, 3766, 3300, 1856, 1091, 1079, 2159, 8024, 7949, 6371, 1962,\n         6397,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 2644, 3766, 3300, 1856, 1091, 1079, 2159, 8024, 7949, 6371, 1962,\n         6397,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0],\n        [ 101, 2600, 4638, 3341, 6432, 6820,  679, 7231, 8024, 6821, 5632, 6121,\n         6756, 1962, 3300, 1798, 8024,  671, 1146, 7178,  671, 1146, 6573, 8024,\n         1372, 5543, 6821, 3416, 6432,  749, 8013,  966,  749, 8013,  102,    0,\n            0,    0,    0],\n        [ 101, 2600,  860, 6820, 1962, 1416, 8024, 2372,  719,  749, 5455, 3321,\n          833, 4578, 8024,  123,  702, 2207, 3198, 3036,  678, 3341,  828, 2622,\n          678, 5455, 3321, 3683, 6772, 1962, 8024, 5455, 2384, 3300, 4157, 1429,\n         4129, 2212,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]))\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 构建模型（模型微调 和 迁移学习bert）","metadata":{}},{"cell_type":"code","source":"# 选择设备\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# tensor 跟踪loss\nwriter = SummaryWriter()\n\n\n#构建模型\nmodel_1 = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-bert-wwm' , num_labels = 5)\nmodel_2 = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-bert-wwm' , num_labels = 5)\n\nmodel_1 = model_1.to(device)\nmodel_2 = model_2.to(device)\n# 冻结bert\nmodel_2.bert.trainable = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:50:47.266720Z","iopub.execute_input":"2025-05-31T11:50:47.267037Z","iopub.status.idle":"2025-05-31T11:50:54.323234Z","shell.execute_reply.started":"2025-05-31T11:50:47.267020Z","shell.execute_reply":"2025-05-31T11:50:54.322697Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c149e8702a7f4d76a525ab9024796e8b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 构建优化器和损失","metadata":{}},{"cell_type":"code","source":"# 损失\nloss_fn1 = nn.CrossEntropyLoss()\noptim1 = Adam(model_1.parameters() , lr = 1e-4)\n\nloss_fn2 = nn.CrossEntropyLoss()\noptim2 = Adam(model_2.parameters() , lr = 1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:50:54.324002Z","iopub.execute_input":"2025-05-31T11:50:54.324190Z","iopub.status.idle":"2025-05-31T11:50:54.329834Z","shell.execute_reply.started":"2025-05-31T11:50:54.324175Z","shell.execute_reply":"2025-05-31T11:50:54.329104Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 训练不冻结模型","metadata":{}},{"cell_type":"code","source":"# model1\nmodel1_train_loss_cnt = 0\nfor epoch in range(5):\n    model_1.train()\n    pbar = tqdm(train_dl)\n    for input_data , input_labels in pbar:\n        datas = { k:v.to(device) for k , v in input_data.items()}\n        labels = input_labels.to(device)\n\n        result = model_1(**datas)\n        loss = loss_fn1(result.logits , labels)\n\n        pbar.set_description(f'epoch:{epoch} train_loss:{loss.item():.4f}' )\n        writer.add_scalar(\"Fine Tuning Tain Loss\", loss , model1_train_loss_cnt)\n\n        model1_train_loss_cnt +=1\n\n        loss.backward()\n        optim1.step()\n        model_1.zero_grad()\n\n\ntorch.save(model_1.state_dict() , 'model_1.pt')\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T11:50:54.330691Z","iopub.execute_input":"2025-05-31T11:50:54.330878Z","iopub.status.idle":"2025-05-31T12:49:18.924403Z","shell.execute_reply.started":"2025-05-31T11:50:54.330863Z","shell.execute_reply":"2025-05-31T12:49:18.923836Z"}},"outputs":[{"name":"stderr","text":"epoch:0 train_loss:0.5681: 100%|██████████| 1666/1666 [11:34<00:00,  2.40it/s]\nepoch:1 train_loss:0.0738: 100%|██████████| 1666/1666 [11:38<00:00,  2.39it/s]\nepoch:2 train_loss:0.2452: 100%|██████████| 1666/1666 [11:50<00:00,  2.34it/s]\nepoch:3 train_loss:0.3213: 100%|██████████| 1666/1666 [11:39<00:00,  2.38it/s]\nepoch:4 train_loss:0.0547: 100%|██████████| 1666/1666 [11:41<00:00,  2.38it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 训练冻结bert模型","metadata":{}},{"cell_type":"code","source":"model2_train_loss_cnt = 0\nfor epoch in range(5):\n    model_2.train()\n    pbar = tqdm(train_dl)\n    for input_data , labels_data in pbar:\n        datas = { k:v.to(device) for k,v in input_data.items()}\n        labels = labels_data.to(device)\n\n        result = model_2(**datas)\n        loss = loss_fn2(result.logits , labels)\n\n        pbar.set_description(f'epoch:{epoch} train_loss:{loss.item():.4f}')\n        writer.add_scalar(\"Transfer Learning Train Loss\", loss, model2_train_loss_cnt)\n        model2_train_loss_cnt += 1\n        \n        loss.backward()\n        optim2.step()\n        \n        model_2.zero_grad()\n\ntorch.save(model_2.state_dict() , 'model_2.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T12:49:18.925228Z","iopub.execute_input":"2025-05-31T12:49:18.925484Z","iopub.status.idle":"2025-05-31T13:47:58.817128Z","shell.execute_reply.started":"2025-05-31T12:49:18.925462Z","shell.execute_reply":"2025-05-31T13:47:58.816554Z"}},"outputs":[{"name":"stderr","text":"epoch:0 train_loss:0.2812: 100%|██████████| 1666/1666 [11:42<00:00,  2.37it/s]\nepoch:1 train_loss:0.0631: 100%|██████████| 1666/1666 [11:47<00:00,  2.35it/s]\nepoch:2 train_loss:0.2997: 100%|██████████| 1666/1666 [11:45<00:00,  2.36it/s]\nepoch:3 train_loss:0.0948: 100%|██████████| 1666/1666 [11:41<00:00,  2.37it/s]\nepoch:4 train_loss:0.0514: 100%|██████████| 1666/1666 [11:42<00:00,  2.37it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 评估模型","metadata":{}},{"cell_type":"code","source":"model_1.eval()\nmodel_2.eval()\npbar = tqdm(test_dl)\ncorrect1 , correct2 = 0 , 0\n\nfor input_data, labels_data in pbar:\n    datas = { k:v.to(device) for k,v in input_data.items() }\n    labels = labels_data.to(device)\n\n    with torch.no_grad():\n        result1 = model_1(**datas)\n        result2 = model_2(**datas)\n\n    predict1 = torch.argmax(result1.logits, dim=-1)\n    predict2 = torch.argmax(result2.logits, dim=-1)\n\n    correct1 += (predict1 == labels).sum()\n    correct2 += (predict1 == labels).sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:47:58.818896Z","iopub.execute_input":"2025-05-31T13:47:58.819089Z","iopub.status.idle":"2025-05-31T13:50:28.936927Z","shell.execute_reply.started":"2025-05-31T13:47:58.819074Z","shell.execute_reply":"2025-05-31T13:50:28.936336Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 556/556 [02:30<00:00,  3.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"准确率 model1:0.0005719187902286649%.4f\n准确率 model2:0.0005719187902286649%.4f\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}