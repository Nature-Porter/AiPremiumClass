{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://blog.csdn.net/qq_24951479/article/details/132495408\n\n第十一周作业：\n1. 参考课堂案例，使用指定的数据集，编写代码实现ner模型训练和推流。\nhttps://huggingface.co/datasets/doushabao4766/msra_ner_k_V3\n2. 完成预测结果的实体抽取。\n    输入：“双方确定了今后发展中美关系的指导方针。”\n    输出：[{\"entity\":\"ORG\",\"content\":\"中\"},{\"entity\":\"ORG\",\"content\":\"美\"}]\n3. 整理Dataset、Trainer、TrainingArgument、DataCollator、Evaluate 知识点，总结文档（无需提交）","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification , AutoTokenizer\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval  # pip install seqeval\nimport torch\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:44:29.141979Z","iopub.execute_input":"2025-06-04T23:44:29.142261Z","iopub.status.idle":"2025-06-04T23:45:11.741297Z","shell.execute_reply.started":"2025-06-04T23:44:29.142232Z","shell.execute_reply":"2025-06-04T23:45:11.740522Z"}},"outputs":[{"name":"stderr","text":"2025-06-04 23:44:51.780137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749080692.263168      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749080692.375708      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 构建分词器","metadata":{}},{"cell_type":"code","source":"# 构建分词器\ntokenizer = AutoTokenizer.from_pretrained(\n    'google-bert/bert-base-chinese' , \n    max_length = 91 , \n    padding='max_length',  # 填充到最大长度\n    truncation=True,    # 超过长度则截断\n    return_tensors='pt')\ntokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:11.742669Z","iopub.execute_input":"2025-06-04T23:45:11.743367Z","iopub.status.idle":"2025-06-04T23:45:12.463901Z","shell.execute_reply.started":"2025-06-04T23:45:11.743339Z","shell.execute_reply":"2025-06-04T23:45:12.463150Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984a40bd108c4dc78e7b42d738a28277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"880e4103e9de4543817348c9090c3f2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"521dee61aa454c369666ece8bc067e8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bda36165aea347ac9232d6cf7c065dbf"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:12.464619Z","iopub.execute_input":"2025-06-04T23:45:12.464815Z","iopub.status.idle":"2025-06-04T23:45:12.469927Z","shell.execute_reply.started":"2025-06-04T23:45:12.464799Z","shell.execute_reply":"2025-06-04T23:45:12.469214Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# 加载数据集\nds = load_dataset('doushabao4766/msra_ner_k_V3')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:12.470994Z","iopub.execute_input":"2025-06-04T23:45:12.471287Z","iopub.status.idle":"2025-06-04T23:45:14.649676Z","shell.execute_reply.started":"2025-06-04T23:45:12.471269Z","shell.execute_reply":"2025-06-04T23:45:14.648977Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae5027abf0f47aaae55f83fcdaa08f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-42717a92413393f9.parquet:   0%|          | 0.00/13.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4688b28de38d4fae9b5210080376a585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-8899cab5fdab45bc.parquet:   0%|          | 0.00/946k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"572bbb270bc34d8fbd99e18aa5974ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976a85485e114c188beb7876cabc4e9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de5efb2818384404be011cfcfe3db9c7"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"for row in ds[\"test\"]:\n    print(row['tokens'])\n    print(row['ner_tags'])\n    print(row['knowledge'])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:14.651749Z","iopub.execute_input":"2025-06-04T23:45:14.652043Z","iopub.status.idle":"2025-06-04T23:45:14.656805Z","shell.execute_reply.started":"2025-06-04T23:45:14.652024Z","shell.execute_reply":"2025-06-04T23:45:14.656098Z"}},"outputs":[{"name":"stdout","text":"['中', '共', '中', '央', '致', '中', '国', '致', '公', '党', '十', '一', '大', '的', '贺', '词', '各', '位', '代', '表', '、', '各', '位', '同', '志', '：', '在', '中', '国', '致', '公', '党', '第', '十', '一', '次', '全', '国', '代', '表', '大', '会', '隆', '重', '召', '开', '之', '际', '，', '中', '国', '共', '产', '党', '中', '央', '委', '员', '会', '谨', '向', '大', '会', '表', '示', '热', '烈', '的', '祝', '贺', '，', '向', '致', '公', '党', '的', '同', '志', '们', '致', '以', '亲', '切', '的', '问', '候', '！']\n[3, 4, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n同志：同志可以指：志同道合、有志一同的人同志(政治)：清末民初以来，革命党人、共产党人互称彼此的称谓。在社会主义国家也用于人与人之间的称呼。同志(LGBT)：同性恋者的代称。有时也泛称同性恋、双性恋、跨性别等LGBT群体。\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 获取实体映射词典","metadata":{}},{"cell_type":"code","source":"label_list = ds[\"train\"].features[\"ner_tags\"].feature.names\nlabel_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:14.657566Z","iopub.execute_input":"2025-06-04T23:45:14.657875Z","iopub.status.idle":"2025-06-04T23:45:14.677454Z","shell.execute_reply.started":"2025-06-04T23:45:14.657856Z","shell.execute_reply":"2025-06-04T23:45:14.676790Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# 数据预处理函数","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"],\n                                 is_split_into_words=True , \n                                 max_length = 91 , \n                                 padding='max_length',  # 填充到最大长度\n                                 truncation=True,    # 超过长度则截断\n                                 return_tensors='pt')\n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\ntokenized_datasets = ds.map(tokenize_and_align_labels, batched=True)\n\n\ntokenized_datasets\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:14.678190Z","iopub.execute_input":"2025-06-04T23:45:14.678662Z","iopub.status.idle":"2025-06-04T23:45:28.663887Z","shell.execute_reply.started":"2025-06-04T23:45:14.678638Z","shell.execute_reply":"2025-06-04T23:45:28.663337Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a3f21add3d74ce3bc9c564f7ae30c6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"072ce6fa84ed4580a6171dc860df3bfa"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# 加载模型","metadata":{}},{"cell_type":"code","source":"# 获取模型\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese' , num_labels = len(label_list))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:28.664600Z","iopub.execute_input":"2025-06-04T23:45:28.664810Z","iopub.status.idle":"2025-06-04T23:45:30.996540Z","shell.execute_reply.started":"2025-06-04T23:45:28.664786Z","shell.execute_reply":"2025-06-04T23:45:30.996043Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03776cd8e7734f7db54ad241a61f98a3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# 使用TrainingArguments 构建训练参数","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n    num_train_epochs = 3,    # 训练 epoch\n    save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n    per_device_train_batch_size=64,  # 训练批次\n    per_device_eval_batch_size=64,\n    report_to='tensorboard',  # 训练输出记录\n    eval_strategy=\"epoch\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:30.997249Z","iopub.execute_input":"2025-06-04T23:45:30.997476Z","iopub.status.idle":"2025-06-04T23:45:31.030726Z","shell.execute_reply.started":"2025-06-04T23:45:30.997450Z","shell.execute_reply":"2025-06-04T23:45:31.030245Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# 构建 Train","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:31.031368Z","iopub.execute_input":"2025-06-04T23:45:31.031535Z","iopub.status.idle":"2025-06-04T23:45:31.736783Z","shell.execute_reply.started":"2025-06-04T23:45:31.031521Z","shell.execute_reply":"2025-06-04T23:45:31.736069Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# 模型训练以及保存模型","metadata":{}},{"cell_type":"code","source":"trainer.train()\n\n# 保存模型\nmodel.save_pretrained(\"./ner_model\")\ntokenizer.save_pretrained(\"./ner_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T23:45:31.737628Z","iopub.execute_input":"2025-06-04T23:45:31.737838Z","iopub.status.idle":"2025-06-05T00:05:10.785411Z","shell.execute_reply.started":"2025-06-04T23:45:31.737821Z","shell.execute_reply":"2025-06-05T00:05:10.784795Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1056' max='1056' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1056/1056 19:34, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.023258</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.041100</td>\n      <td>0.023055</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.009000</td>\n      <td>0.023872</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('./ner_model/tokenizer_config.json',\n './ner_model/special_tokens_map.json',\n './ner_model/vocab.txt',\n './ner_model/added_tokens.json',\n './ner_model/tokenizer.json')"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"------------\n# 实体抽取","metadata":{}},{"cell_type":"code","source":"# 实体抽取函数\ndef exttract_entities(text):\n    # 加载模型与分词器\n    model = AutoModelForTokenClassification.from_pretrained('./ner_model')\n    tokenizer = AutoTokenizer.from_pretrained('./ner_model')\n\n    # 分词和预测\n    inputs = tokenizer(text , return_tensors = 'pt' , truncation = True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # 获取预测结果\n    predictions = torch.argmax(outputs.logits , dim = 2)\n    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    predicted_labels = [label_list[p] for p in predictions[0].numpy()]\n\n    # 提取实体\n    entities = []\n    current_entity = None\n    for token, label in zip(tokens, predicted_labels):\n        if label.startswith(\"B-\"):\n            if current_entity:\n                entities.append(current_entity)\n            current_entity = {\"entity\": label[2:], \"content\": token.replace(\"##\", \"\")}\n        elif label.startswith(\"I-\") and current_entity:\n            current_entity[\"content\"] += token.replace(\"##\", \"\")\n        else:\n            if current_entity:\n                entities.append(current_entity)\n                current_entity = None\n    \n    return entities","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T00:06:50.016261Z","iopub.execute_input":"2025-06-05T00:06:50.017085Z","iopub.status.idle":"2025-06-05T00:06:50.025247Z","shell.execute_reply.started":"2025-06-05T00:06:50.017052Z","shell.execute_reply":"2025-06-05T00:06:50.024500Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# 测试示例\ntext = \"双方确定了今后发展中美关系的指导方针。\"\nprint(exttract_entities(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T00:06:53.596991Z","iopub.execute_input":"2025-06-05T00:06:53.597293Z","iopub.status.idle":"2025-06-05T00:06:54.074088Z","shell.execute_reply.started":"2025-06-05T00:06:53.597271Z","shell.execute_reply":"2025-06-05T00:06:54.073295Z"}},"outputs":[{"name":"stdout","text":"[{'entity': 'LOC', 'content': '中'}, {'entity': 'LOC', 'content': '美'}]\n","output_type":"stream"}],"execution_count":20}]}