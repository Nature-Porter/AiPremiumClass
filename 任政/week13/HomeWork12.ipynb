{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T06:25:36.767893Z",
     "start_time": "2025-06-19T06:25:36.760411Z"
    }
   },
   "source": [
    "import ollama\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline,set_seed\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 调用 Ollama 本地模型生成文本",
   "id": "61a40531d769f8d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T05:10:47.388759Z",
     "start_time": "2025-06-19T05:09:52.770053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 确保模型名称与 Ollama 中模型名称一致\n",
    "response = ollama.generate('deepseek-r1:1.5b', \"今天是2⽉14⽇，请列举历史上的今天发⽣过的10件⼤事件\")\n",
    "print(response['response'])"
   ],
   "id": "89879e6db7ad2bea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，今天是2月14日。用户让我列举历史上发生过的一十件关于这个日期的事件。那我得先确认一下2月14日到底是哪一年，这样才能准确查找相关的历史事件。\n",
      "\n",
      "首先，2月14日确实是中国的传统节日“元宵节”，同时也是春节的一部分。不过，在某些地区，春节和元宵节的时间可能会有所不同，所以需要明确地区的情况。但通常情况下，春节是每年的农历二月初八或三月初八，而元宵节则是二月初十。\n",
      "\n",
      "接下来，我应该列出过去十年中在2月14日或附近发生的重大事件。这些事件可能包括政治、经济、军事、文化、科技等方面的变化。\n",
      "\n",
      "考虑到2023年和之前的年份，比如2022年是鼠年，那么2月14日可能与2022年的春节时间有关。但具体有没有发生什么重要事？可能没有特别突出的事件，所以需要寻找有重大影响的历史事件。\n",
      "\n",
      "比如，1985年香港回归祖国，这是个重要的历史时刻，但离2月14日还较远。那么之前的例子可能更接近当前的时间。比如，1971年的“和平解放”，但这也不算最近。\n",
      "\n",
      "另一个可能是2012年的春节，但那是在2月12日，距离2月14日还有两天，可能不算最近的事件。所以需要寻找最近且与2/14相关的重要历史事件。\n",
      "\n",
      "可能的话，我可以从中国的传统节日和一些重要政治时刻来寻找线索，看看有没有在2/14或附近发生过重大事件的例子。\n",
      "\n",
      "不过，在没有具体数据的情况下，可能很难准确列出十件重要的事件。因此，我只能根据已知的信息和大致的时间范围，尽量列举可能相关的事件，但这些事件的具体内容可能需要进一步验证。\n",
      "\n",
      "综上所述，虽然无法确切列出所有与2月14日相关的重要历史事件，但由于时间限制和信息有限，我只能基于现有的信息进行推测。\n",
      "</think>\n",
      "\n",
      "以下是历史上发生过的与2月14日相关的重要事件：\n",
      "\n",
      "1. **1973年**：朝鲜战争爆发（2/12），中国正式加入世界和平联盟。\n",
      "\n",
      "2. **1985年**：香港回归祖国（2/14）[1]\n",
      "\n",
      "3. **1989年**：台湾统一，台湾成为中国的一部分（2/14）[2]\n",
      "\n",
      "4. **1997年**：世界卫生组织宣布男性的出生率高于女性，引发争议（2/15）\n",
      "\n",
      "5. **2001年**：九-one事件（2/16），中国军队在台湾地区遭遇暴徒袭击。\n",
      "\n",
      "6. **2003年**：“9·11”恐怖主义事件发生（2/17）[3]\n",
      "\n",
      "7. **2004年**：美国对伊拉克实施了对峙军行动，导致目标Countrycollide[4]\n",
      "\n",
      "8. **2005年**：印度洋海啸，导致大量人员伤亡（2/17）\n",
      "\n",
      "9. **2006年**：“9·11”事件受到调查，引发对美国政府行为的讨论（2/18）\n",
      "\n",
      "10. **2008年**：台湾地区宣布恢复香港回归，成为中国的第三个省份[5]\n",
      "\n",
      "这些事件主要发生在2月14日左右的时间附近。\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用 Hugging Face 调用本地模型 模型生成文本",
   "id": "ec1c06d338ce0d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T06:29:39.864041Z",
     "start_time": "2025-06-19T06:29:24.752046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator = pipeline('text-generation', model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' , device = -1)  # 地址写transformer库的地址\n",
    "set_seed(42)  # 设置随机种子以确保结果可复现\n",
    "# 生成文本\n",
    "response = generator(\"今天是2⽉14⽇，请列举历史上的今天发⽣过的⼗件⼤事件\",num_return_sequences=10, truncation=True )\n",
    "print(response[0]['generated_text'])  # 输出生成的文本"
   ],
   "id": "3d0ccfe252ef9b25",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\modeling_utils.py\", line 309, in _wrapper\n    return func(*args, **kwargs)\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4451, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: 页面文件太小，无法完成操作。 (os error 1455)\n\nwhile loading with Qwen2ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\modeling_utils.py\", line 309, in _wrapper\n    return func(*args, **kwargs)\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4451, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: 页面文件太小，无法完成操作。 (os error 1455)\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m generator \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdeepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 地址斜transformer库的地址\u001B[39;00m\n\u001B[0;32m      2\u001B[0m set_seed(\u001B[38;5;241m42\u001B[39m)  \u001B[38;5;66;03m# 设置随机种子以确保结果可复现\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 生成文本\u001B[39;00m\n",
      "File \u001B[1;32mD:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    941\u001B[0m     model_classes \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m: targeted_task[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m: targeted_task[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m]}\n\u001B[1;32m--> 942\u001B[0m     framework, model \u001B[38;5;241m=\u001B[39m infer_framework_load_model(\n\u001B[0;32m    943\u001B[0m         adapter_path \u001B[38;5;28;01mif\u001B[39;00m adapter_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m model,\n\u001B[0;32m    944\u001B[0m         model_classes\u001B[38;5;241m=\u001B[39mmodel_classes,\n\u001B[0;32m    945\u001B[0m         config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[0;32m    946\u001B[0m         framework\u001B[38;5;241m=\u001B[39mframework,\n\u001B[0;32m    947\u001B[0m         task\u001B[38;5;241m=\u001B[39mtask,\n\u001B[0;32m    948\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs,\n\u001B[0;32m    949\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m    950\u001B[0m     )\n\u001B[0;32m    952\u001B[0m model_config \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    953\u001B[0m hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_commit_hash\n",
      "File \u001B[1;32mD:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\pipelines\\base.py:305\u001B[0m, in \u001B[0;36minfer_framework_load_model\u001B[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m class_name, trace \u001B[38;5;129;01min\u001B[39;00m all_traceback\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    304\u001B[0m             error \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhile loading with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, an error is thrown:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtrace\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 305\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    306\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not load model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with any of the following classes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_tuple\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. See the original errors:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00merror\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         )\n\u001B[0;32m    309\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    310\u001B[0m     framework \u001B[38;5;241m=\u001B[39m infer_framework(model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Could not load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\modeling_utils.py\", line 309, in _wrapper\n    return func(*args, **kwargs)\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4451, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: 页面文件太小，无法完成操作。 (os error 1455)\n\nwhile loading with Qwen2ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 292, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\modeling_utils.py\", line 309, in _wrapper\n    return func(*args, **kwargs)\n  File \"D:\\360Downloads\\Anaconda\\envs\\PythonProject\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4451, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: 页面文件太小，无法完成操作。 (os error 1455)\n\n\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 通过HTTP调用本地模型生成文本",
   "id": "4fe612e829852b04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T06:26:40.320602Z",
     "start_time": "2025-06-19T06:25:43.315537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = 'http://localhost:11434/api/generate'\n",
    "data = {\n",
    "    \"model\": \"deepseek-r1:1.5b\",\n",
    "    \"prompt\": \"彩虹为什么有七种颜色？\",\n",
    "    \"stream\": False # 是否流式输出\n",
    "    }\n",
    "response = requests.post(url, json=data)\n",
    "# print(response.json()) # 输出生成的文本\n",
    "print(json.loads(response.text)['response'])"
   ],
   "id": "c1a8cd646ad9bac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，这个问题听起来挺有意思的。彩虹为什么会只有七种颜色呢？我知道每种颜色代表一种光线的波长范围，比如红色是红光，橙色是远红光，依此类推，直到紫色是最短的光。\n",
      "\n",
      "首先，我需要了解自然界的光谱情况。通常情况下，光可以分成七种颜色：红、橙、黄、绿、蓝、靛、紫，对应不同的波长。这些颜色是我们看到的颜色。\n",
      "\n",
      "那为什么彩虹只有七种颜色呢？是不是因为人类的眼睛只能感知这七种颜色？或者说，光线在不同介质中的折射和反射可能让颜色数量减少？\n",
      "\n",
      "我记得在水里看彩虹的时候，看起来比空气里更亮一些，这是因为水的折射导致了更多的颜色出现。不过，问题是我对彩虹的颜色数量不太确定。\n",
      "\n",
      "另外，我听说过颜色是由光的波长决定的，而光线在不同介质中的传播可能会改变颜色的数量。比如，光线从空气进入水中，由于折射和反射的原因，可能会产生更多的颜色或者更少的颜色。\n",
      "\n",
      "还有一种可能是自然界的光谱受到各种因素的影响，比如光线的强度、角度等等。如果这些因素发生变化，可能会影响彩虹的颜色数量。\n",
      "\n",
      "我还记得有些地方提到过，彩虹其实是多个同心圆的排列，每个同心圆代表一种颜色。这可能是因为不同角度的阳光被光线折射和反射，形成圆形图像，而不同的颜色对应着不同的角度分布。\n",
      "\n",
      "当然，这些都是我的初步想法，可能还不够准确。我需要进一步查阅相关的资料来确认这些信息是否正确。\n",
      "</think>\n",
      "\n",
      "彩虹的颜色数量是七种，分别对应红、橙、黄、绿、蓝、靛、紫这七种光谱颜色。以下是对此现象的详细解答：\n",
      "\n",
      "1. **自然光谱**：自然界中存在多种光线，波长范围从红光（约700 nm）到紫光（约350 nm），共计七色。\n",
      "\n",
      "2. **人类眼睛感知**：我们能够感知这七种颜色，因为它们落在视网膜上的光线经过角化质体（视网膜下的晶体结构）被加工成可感知的色散结构，随后形成明暗变化的光带。\n",
      "\n",
      "3. **空气中的彩虹**：\n",
      "   - 在空气中观察到的彩虹是单一光束的折射和反射形成的。\n",
      "   - 这种情况下，光线从太阳发出后经过空气层（如大气、海市蜃效应等）后，形成了七种颜色的圆形图像。\n",
      "\n",
      "4. **水中的彩虹**：在水中（如湖面或池塘）观察到的彩虹比空气中更明亮，因为光线进入水中时发生折射和反射，产生更多的颜色层次。\n",
      "   - 这种情况主要由于光的多普勒效应和折射现象的影响。\n",
      "\n",
      "5. **自然条件影响**：\n",
      "   - 光线强度、角度、介质深度等因素都会影响观察到的颜色数量。例如，较浅或较深的光线可能会导致更少的颜色出现，而不同介质（如空气、水）可能会影响颜色分布的清晰度。\n",
      "\n",
      "总结来说，彩虹的颜色来源于光谱颜色的不同折射和反射效果，而自然条件的变化会影响观察的清晰度。这使得我们能够在不同的环境中感知到七种颜色形成的图案。\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 利用OpenAI远程调用智谱模型",
   "id": "80fa6168b31fa7ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T05:13:31.677220Z",
     "start_time": "2025-06-19T05:13:20.988053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查找并加载.env文件中的环境变量\n",
    "load_dotenv(find_dotenv())\n",
    "client = OpenAI(\n",
    "    # 获取.env文件中的环境变量APY_KEY\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url=os.getenv(\"BASE_URL\"),\n",
    "    )\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"glm-4-flash-250414\",\n",
    "    # 可以修改下面的内容，让模型生成你想要的文本\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个知识渊博的助手，擅长提供历史事件信息。\"},\n",
    "        {\"role\": \"user\", \"content\": \"今天是2⽉14⽇，请列举历史上的今天发⽣过的⼗件⼤事件\"},\n",
    "        ]\n",
    "    )\n",
    "# 输出生成文本\n",
    "# strip()是去掉字符串前后的空格和换行符\n",
    "print(completion.choices[0].message.content.strip())"
   ],
   "id": "c7273368ec83075d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天是情人节，历史上确实发生了一些重要的事件。以下是一些历史上的今天（2月14日）发生的大事件：\n",
      "\n",
      "1. **1478年**：列奥纳多·达·芬奇出生，意大利文艺复兴时期的画家、雕塑家、科学家、发明家、数学家、工程师、解剖学家、作家。\n",
      "2. **1542年**：英国国王亨利八世的第一任妻子凯瑟琳·阿瓦隆去世。\n",
      "3. **1755年**：英国哲学家、作家大卫·休谟出版了他的著作《人性论》。\n",
      "4. **1848年**：美国废奴主义者约翰·布朗在阿肯色州发动起义，试图解放被奴役的黑人。\n",
      "5. **1879年**：英国物理学家詹姆斯·克拉克·麦克斯韦出生，他是电磁理论的奠基人之一。\n",
      "6. **1889年**：法国作家奥古斯特·罗达·杜·波瓦西耶（笔名乔治·桑）去世。\n",
      "7. **1909年**：美国探险家罗伯特·皮尔里宣布发现了北极。\n",
      "8. **1912年**：美国作家、诗人埃德娜·圣文森特·米莱出生。\n",
      "9. **1931年**：美国最高法院通过《苏特莱奇案》，确立了对“煽动性言论”的宪法限制。\n",
      "10. **1946年**：美国心理学家卡尔·罗杰斯出生，他是人本主义心理学的奠基人之一。\n",
      "\n",
      "这些事件涵盖了科学、文学、政治等多个领域，展示了历史上的今天发生的各种重要事件。\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
