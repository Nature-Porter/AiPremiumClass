{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b062aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9a5143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备为 cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'当前设备为',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fa5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, dropout, hidden_form = 'concat'):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 定义嵌入层\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        # 定义LSTM层\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, dropout=dropout, num_layers=2,\n",
    "                          batch_first=True, bidirectional=True)\n",
    "        self.hidden_form = hidden_form\n",
    "\n",
    "    def forward(self, token_seq):\n",
    "        # token_seq: [batch_size, seq_len]\n",
    "        # embedded: [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.embedding(token_seq)\n",
    "        # outputs: [batch_size, seq_len, hidden_dim * 2]\n",
    "        # hidden: [4, batch_size, hidden_dim]\n",
    "        outputs, (h_n, c_n)  = self.rnn(embedded)\n",
    "        if (self.hidden_form == 'concat'):\n",
    "            hidden_concat = torch.cat([h_n[0], h_n[1]], dim=1)\n",
    "            hidden_concat = hidden_concat.unsqueeze(0).repeat(2, 1, 1)\n",
    "            return hidden_concat, outputs\n",
    "        elif (self.hidden_form == 'add'):\n",
    "            hidden_sum = h_n.sum(dim=0)\n",
    "            hidden_sum = hidden_sum.unsqueeze(0).repeat(2, 1, 1)\n",
    "            return hidden_sum, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af89249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, enc_output, dec_output):\n",
    "        a_t = torch.bmm(enc_output, dec_output.permute(0, 2, 1))\n",
    "        # 1.计算 结合解码token和编码token，关联的权重\n",
    "        a_t = torch.softmax(a_t, dim=1)\n",
    "        # 2.计算 关联权重和编码token 贡献值\n",
    "        c_t = torch.bmm(a_t.permute(0, 2, 1), enc_output)\n",
    "        return c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140a2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, dropout, hidden_form = 'concat'):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 定义嵌入层\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        # 定义LSTM层\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim * 2, dropout=dropout,\n",
    "                          num_layers=2,batch_first=True)\n",
    "        # 定义线性层\n",
    "        self.fc = nn.Linear(hidden_dim * 2, input_dim)  # 解码词典中词汇概率\n",
    "        # attention层\n",
    "        self.atteniton = Attention()\n",
    "        # attention结果转换线性层\n",
    "        self.atteniton_fc = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.hidden_form = hidden_form\n",
    "\n",
    "    def forward(self, token_seq, hidden_state, enc_output):\n",
    "        # token_seq: [batch_size, seq_len]\n",
    "        # embedded: [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.embedding(token_seq)\n",
    "        # outputs: [batch_size, seq_len, hidden_dim * 2]\n",
    "        # hidden: [2, batch_size, hidden_dim * 2]\n",
    "        if (self.hidden_form == 'add'):\n",
    "            hidden_state = hidden_state.repeat(1, 1, 2)\n",
    "        dec_output, (h_n, c_n) = self.rnn(embedded, (hidden_state, torch.zeros_like(hidden_state)))\n",
    "        \n",
    "        # attention运算\n",
    "        c_t = self.atteniton(enc_output, dec_output)\n",
    "        # [attention, dec_output]\n",
    "        cat_output = torch.cat((c_t, dec_output), dim=-1)\n",
    "        # 线性运算\n",
    "        out = torch.tanh(self.atteniton_fc(cat_output))\n",
    "\n",
    "        # out: [batch_size, seq_len, hidden_dim * 2]\n",
    "        logits = self.fc(out)\n",
    "        return logits, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41dc8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(file_in_path,file_out_path):\n",
    "    \"\"\"\n",
    "    读取记录数据并返回数据集合\n",
    "    \"\"\"\n",
    "    encode_data = open_data(file_in_path)\n",
    "    decode_data = open_data(file_out_path)\n",
    "    \n",
    "    # 编码数据与解码数据长度是否一致\n",
    "    assert len(encode_data) == len(decode_data), '上下联原始数据长度不一致'\n",
    "    return encode_data, decode_data\n",
    "\n",
    "def open_data(file_path):\n",
    "    \"\"\"\n",
    "    打开文件录入行信息\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path,encoding='utf-8') as f1:\n",
    "            # 读取记录行\n",
    "            lines = f1.read().split('\\n')\n",
    "            for line in lines:\n",
    "                if line == ' ':\n",
    "                    continue\n",
    "                tokens = line.split()\n",
    "                data.append(tokens)\n",
    "    return data\n",
    "\n",
    "def words_to_vocab(words_list):\n",
    "        \"\"\"\n",
    "        从输入的单词列表中构建一个词汇表(vocabulary)\n",
    "        \"\"\"\n",
    "        no_repeat_tokens = set()\n",
    "        for word in words_list:\n",
    "            no_repeat_tokens.update(list(word))  \n",
    "        tokens = ['PAD','UNK'] + list(no_repeat_tokens)\n",
    "        vocabs = { tk:i for i, tk in enumerate(tokens)}\n",
    "        return vocabs\n",
    "\n",
    "def dump_vocab(path,data_in_vocab,data_out_vocab):\n",
    "    with open(path,'wb') as f:\n",
    "        pickle.dump((data_in_vocab, data_out_vocab),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(enc_voc, dec_voc):\n",
    "    def batch_process(data):\n",
    "        \"\"\"\n",
    "        批次数据处理并返回\n",
    "        \"\"\"\n",
    "        enc_ids, dec_ids, labels = [],[],[]\n",
    "        for enc,dec in data:\n",
    "            # token -> token index\n",
    "            enc_idx = [enc_voc[tk] for tk in enc]\n",
    "            dec_idx = [dec_voc[tk] for tk in dec]\n",
    "\n",
    "            enc_ids.append(torch.tensor(enc_idx).long())\n",
    "            dec_ids.append(torch.tensor(dec_idx[:-1]).long())\n",
    "            labels.append(torch.tensor(dec_idx[1:]).long())\n",
    "        # 构建张量\n",
    "        enc_input = pad_sequence(enc_ids, batch_first=True)\n",
    "        dec_input = pad_sequence(dec_ids, batch_first=True)\n",
    "        targets = pad_sequence(labels, batch_first=True)\n",
    "        return enc_input, dec_input, targets\n",
    "    return batch_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e58281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_emb_size,\n",
    "                 dec_emb_size,\n",
    "                 emb_dim,\n",
    "                 hidden_size,\n",
    "                 dropout=0.5,\n",
    "                 hidden_form = 'concat'\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(enc_emb_size, emb_dim, hidden_size, dropout=dropout, hidden_form = hidden_form)\n",
    "        self.decoder = Decoder(dec_emb_size, emb_dim, hidden_size, dropout=dropout, hidden_form = hidden_form)\n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "        encoder_state, outputs = self.encoder(enc_input)\n",
    "        output,hidden = self.decoder(dec_input, encoder_state, outputs)\n",
    "\n",
    "        return output,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aacf271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "train_data_in_path = './couplet/train/in.txt'\n",
    "train_data_out_path = './couplet/train/out.txt'\n",
    "\n",
    "test_data_in_path = './couplet/test/in.txt'\n",
    "test_data_out_path = './couplet/test/out.txt'\n",
    "\n",
    "# 路径参数\n",
    "model_path='./model/couplet.pt',\n",
    "logs_path = 'D:/logs/'\n",
    "\n",
    "# 配置文件\n",
    "hidden_form1 = 'concat'\n",
    "lr1=1e-3\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9376ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集上联 770492\n",
      "训练集下联 770492\n",
      "测试集上联 4001\n",
      "测试集下联 4001\n"
     ]
    }
   ],
   "source": [
    "train_data_in ,train_data_out = load_data(train_data_in_path,train_data_out_path)\n",
    "test_data_in ,test_data_out = load_data(test_data_in_path,test_data_out_path)\n",
    "\n",
    "# 构建词汇表\n",
    "train_data_in_vocab  = words_to_vocab(train_data_in)\n",
    "train_data_out_vocab = words_to_vocab(train_data_out)\n",
    "test_data_in_vocab  = words_to_vocab(test_data_in)\n",
    "test_data_out_vocab = words_to_vocab(test_data_out)\n",
    "\n",
    "dump_vocab('./couplet/train/vocab.bin',train_data_in_vocab, train_data_out_vocab)\n",
    "dump_vocab('./couplet/test/vocab.bin',test_data_in_vocab, test_data_out_vocab)\n",
    "\n",
    "print(f'训练集上联',len(train_data_in))\n",
    "print(f'训练集下联',len(train_data_out))\n",
    "print(f'测试集上联',len(test_data_in))\n",
    "print(f'测试集下联',len(test_data_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88421325",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logs_path)\n",
    "\n",
    "model = Seq2Seq(\n",
    "        enc_emb_size=len(train_data_in),\n",
    "        dec_emb_size=len(train_data_out),\n",
    "        emb_dim=200,\n",
    "        hidden_size=256,\n",
    "        dropout=0.4,\n",
    "        hidden_form = hidden_form1\n",
    "    )\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 优化器、损失函数\n",
    "optimizer = optim.AdamW(model.parameters(), weight_decay=0.01, lr=lr1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "822af8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0608:  15%|█▌        | 3673/24078 [7:29:35<41:12:35,  7.27s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "        list(zip(train_data_in,train_data_out)),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_process(train_data_in_vocab, train_data_out_vocab)\n",
    ")\n",
    "loss_cnt = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    tpbar = tqdm(dataloader)\n",
    "    for enc_input, dec_input, targets in tpbar:\n",
    "        enc_input = enc_input.to(device)\n",
    "        dec_input = dec_input.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        logits, _ = model(enc_input, dec_input)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tpbar.set_description(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "        writer.add_scalar('Loss/train', loss.item(), loss_cnt)\n",
    "        loss_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5eeca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f2165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1fe61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
