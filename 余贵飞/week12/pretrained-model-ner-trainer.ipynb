{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:31:01.865548Z",
     "iopub.status.busy": "2025-06-08T08:31:01.865250Z",
     "iopub.status.idle": "2025-06-08T08:31:05.249331Z",
     "shell.execute_reply": "2025-06-08T08:31:05.248226Z",
     "shell.execute_reply.started": "2025-06-08T08:31:01.865520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:05:34.145820Z",
     "iopub.status.busy": "2025-06-08T06:05:34.145141Z",
     "iopub.status.idle": "2025-06-08T06:06:03.817514Z",
     "shell.execute_reply": "2025-06-08T06:06:03.816753Z",
     "shell.execute_reply.started": "2025-06-08T06:05:34.145787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 06:05:48.013502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749362748.273245      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749362748.333366      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:03.819114Z",
     "iopub.status.busy": "2025-06-08T06:06:03.818611Z",
     "iopub.status.idle": "2025-06-08T06:06:06.391924Z",
     "shell.execute_reply": "2025-06-08T06:06:06.391182Z",
     "shell.execute_reply.started": "2025-06-08T06:06:03.819094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1fb9017f284caeb31d1567921650ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a697b3431c645ca85d0d9fa4fb15053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-28T14:40:30.715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:06.393151Z",
     "iopub.status.busy": "2025-06-08T06:06:06.392911Z",
     "iopub.status.idle": "2025-06-08T06:06:06.875140Z",
     "shell.execute_reply": "2025-06-08T06:06:06.874420Z",
     "shell.execute_reply.started": "2025-06-08T06:06:06.393133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511e67147e4141f99e70845fdba13d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f06cefaa894622aaf8f722e23eae4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0748c368cc85490a840f182c5518770c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-28T14:40:30.715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "message= \"命名实体识别\"\n",
    "label = torch.tensor([0,1,0,2,5,4])\n",
    "\n",
    "model_input = tokenizer([message], return_tensors='pt')\n",
    "result = model(**model_input)\n",
    "\n",
    "print(result.loss)\n",
    "print(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:06.877007Z",
     "iopub.status.busy": "2025-06-08T06:06:06.876800Z",
     "iopub.status.idle": "2025-06-08T06:06:08.810818Z",
     "shell.execute_reply": "2025-06-08T06:06:08.810256Z",
     "shell.execute_reply.started": "2025-06-08T06:06:06.876991Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdcb622b9984d14a949aac72acb50a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed09875fe7c14cf0bd5dcf60f49bdc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e594cf1ab6c241f6bf8baef97d9d7900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-a33d0e4276aef9b4.parquet:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338ca9512ad44d7d8306ab475646027f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-07f476b71c5edde6.parquet:   0%|          | 0.00/178k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5053f3ed19a94d06a6836fb6436f4f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61741aa0626b49f7b280933c6dcc4801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载hf中dataset\n",
    "ds = load_dataset('nlhappy/CLUE-NER')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实体映射数据集词典准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:08.811660Z",
     "iopub.status.busy": "2025-06-08T06:06:08.811457Z",
     "iopub.status.idle": "2025-06-08T06:06:08.816272Z",
     "shell.execute_reply": "2025-06-08T06:06:08.815602Z",
     "shell.execute_reply.started": "2025-06-08T06:06:08.811636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "           'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T03:17:43.276344Z",
     "iopub.status.busy": "2025-06-08T03:17:43.275631Z",
     "iopub.status.idle": "2025-06-08T03:17:43.281357Z",
     "shell.execute_reply": "2025-06-08T03:17:43.280655Z",
     "shell.execute_reply.started": "2025-06-08T03:17:43.276315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'position': 1,\n",
       " 'name': 2,\n",
       " 'government': 3,\n",
       " 'movie': 4,\n",
       " 'book': 5,\n",
       " 'organization': 6,\n",
       " 'company': 7,\n",
       " 'address': 8,\n",
       " 'scene': 9,\n",
       " 'game': 10}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T03:17:46.174592Z",
     "iopub.status.busy": "2025-06-08T03:17:46.174296Z",
     "iopub.status.idle": "2025-06-08T03:17:46.179846Z",
     "shell.execute_reply": "2025-06-08T03:17:46.179124Z",
     "shell.execute_reply.started": "2025-06-08T03:17:46.174567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-POSITION',\n",
       " 'I-POSITION',\n",
       " 'B-NAME',\n",
       " 'I-NAME',\n",
       " 'B-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'B-MOVIE',\n",
       " 'I-MOVIE',\n",
       " 'B-BOOK',\n",
       " 'I-BOOK',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COMPANY',\n",
       " 'I-COMPANY',\n",
       " 'B-ADDRESS',\n",
       " 'I-ADDRESS',\n",
       " 'B-SCENE',\n",
       " 'I-SCENE',\n",
       " 'B-GAME',\n",
       " 'I-GAME']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:08.817189Z",
     "iopub.status.busy": "2025-06-08T06:06:08.816947Z",
     "iopub.status.idle": "2025-06-08T06:06:10.195060Z",
     "shell.execute_reply": "2025-06-08T06:06:10.194365Z",
     "shell.execute_reply.started": "2025-06-08T06:06:08.817167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8699b5b351264b5aa9319995b89d6953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3505b45f8f3497283032a391b6796b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "           'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "\n",
    "def entity_tags_proc(item):\n",
    "    # item即是dataset中记录\n",
    "    text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "    tags = [0] * text_len    # 初始值为‘O’\n",
    "    # 遍历实体列表，所有实体类别标记填入tags\n",
    "    entites = item['ents']\n",
    "    for ent in entites:\n",
    "        indices = ent['indices']  # 实体索引\n",
    "        label = ent['label']   # 实体名\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "    return {'ent_tag': tags}\n",
    "\n",
    "# 使用自定义回调函数处理数据集记录\n",
    "ds1 = ds.map(entity_tags_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T03:18:05.913711Z",
     "iopub.status.busy": "2025-06-08T03:18:05.913028Z",
     "iopub.status.idle": "2025-06-08T03:18:05.918387Z",
     "shell.execute_reply": "2025-06-08T03:18:05.917526Z",
     "shell.execute_reply.started": "2025-06-08T03:18:05.913686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，\n",
      "[13, 14, 14, 14, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "for row in ds1['train']:\n",
    "    print(row['text'])\n",
    "    print(row['ent_tag'])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-28T14:40:30.716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文bert分词在日期时间和英文转换token过程中，出现合并。影响ner标注准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-28T14:40:30.716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "token_index = tokenizer.encode('2000年2月add', add_special_tokens=False)\n",
    "print(token_index)\n",
    "tokens = tokenizer.decode(token_index)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-28T14:40:30.716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_data = tokenizer([list('2000年2月add')], add_special_tokens=False, truncation=True, \n",
    "                       is_split_into_words=True)\n",
    "print(input_data)\n",
    "\n",
    "tokens = tokenizer.decode(token_index)\n",
    "print(tokens) # 返回token对应逐个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:10.196174Z",
     "iopub.status.busy": "2025-06-08T06:06:10.195896Z",
     "iopub.status.idle": "2025-06-08T06:06:15.863234Z",
     "shell.execute_reply": "2025-06-08T06:06:15.862502Z",
     "shell.execute_reply.started": "2025-06-08T06:06:10.196148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8ec67fa59f4c46a326fbfc4c4a1e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4603818a9c354451aa1b4f636a0b1475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "           'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "\n",
    "def entity_tags_proc(item):\n",
    "    # item即是dataset中记录\n",
    "    text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "    tags = [0] * text_len    # 初始值为‘O’\n",
    "    # 遍历实体列表，所有实体类别标记填入tags\n",
    "    entites = item['ents']\n",
    "    for ent in entites:\n",
    "        indices = ent['indices']  # 实体索引\n",
    "        label = ent['label']   # 实体名\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "    return {'ent_tag': tags}\n",
    "\n",
    "# 使用自定义回调函数处理数据集记录\n",
    "ds1 = ds.map(entity_tags_proc)\n",
    "\n",
    "def data_input_proc(item):\n",
    "    # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "    batch_texts = [list(text) for text in item['text']]\n",
    "    # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "    input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n",
    "                           is_split_into_words=True, padding='max_length')\n",
    "    input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "    return input_data\n",
    "    \n",
    "\n",
    "ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:15.864992Z",
     "iopub.status.busy": "2025-06-08T06:06:15.864784Z",
     "iopub.status.idle": "2025-06-08T06:06:15.870920Z",
     "shell.execute_reply": "2025-06-08T06:06:15.870259Z",
     "shell.execute_reply.started": "2025-06-08T06:06:15.864976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 512 512 512\n"
     ]
    }
   ],
   "source": [
    "for item in ds2['train']:\n",
    "    print(len(item['input_ids']), len(item['token_type_ids']), len(item['attention_mask']), len(item['labels']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:25.135372Z",
     "iopub.status.busy": "2025-06-08T06:06:25.134720Z",
     "iopub.status.idle": "2025-06-08T06:06:25.140102Z",
     "shell.execute_reply": "2025-06-08T06:06:25.139427Z",
     "shell.execute_reply.started": "2025-06-08T06:06:25.135348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 记录转换为pytorch\n",
    "ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "# ds_new = ds2.with_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型手动训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-08T03:27:15.003244Z",
     "iopub.status.busy": "2025-06-08T03:27:15.002637Z",
     "iopub.status.idle": "2025-06-08T03:27:15.009537Z",
     "shell.execute_reply": "2025-06-08T03:27:15.008908Z",
     "shell.execute_reply.started": "2025-06-08T03:27:15.003220Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert.embeddings.word_embeddings.weight',\n",
       " 'bert.embeddings.position_embeddings.weight',\n",
       " 'bert.embeddings.token_type_embeddings.weight',\n",
       " 'bert.embeddings.LayerNorm.weight',\n",
       " 'bert.embeddings.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.attention.self.query.weight',\n",
       " 'bert.encoder.layer.0.attention.self.query.bias',\n",
       " 'bert.encoder.layer.0.attention.self.key.weight',\n",
       " 'bert.encoder.layer.0.attention.self.key.bias',\n",
       " 'bert.encoder.layer.0.attention.self.value.weight',\n",
       " 'bert.encoder.layer.0.attention.self.value.bias',\n",
       " 'bert.encoder.layer.0.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.0.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.0.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.0.output.dense.weight',\n",
       " 'bert.encoder.layer.0.output.dense.bias',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.attention.self.query.weight',\n",
       " 'bert.encoder.layer.1.attention.self.query.bias',\n",
       " 'bert.encoder.layer.1.attention.self.key.weight',\n",
       " 'bert.encoder.layer.1.attention.self.key.bias',\n",
       " 'bert.encoder.layer.1.attention.self.value.weight',\n",
       " 'bert.encoder.layer.1.attention.self.value.bias',\n",
       " 'bert.encoder.layer.1.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.1.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.1.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.1.output.dense.weight',\n",
       " 'bert.encoder.layer.1.output.dense.bias',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.attention.self.query.weight',\n",
       " 'bert.encoder.layer.2.attention.self.query.bias',\n",
       " 'bert.encoder.layer.2.attention.self.key.weight',\n",
       " 'bert.encoder.layer.2.attention.self.key.bias',\n",
       " 'bert.encoder.layer.2.attention.self.value.weight',\n",
       " 'bert.encoder.layer.2.attention.self.value.bias',\n",
       " 'bert.encoder.layer.2.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.2.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.2.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.2.output.dense.weight',\n",
       " 'bert.encoder.layer.2.output.dense.bias',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.attention.self.query.weight',\n",
       " 'bert.encoder.layer.3.attention.self.query.bias',\n",
       " 'bert.encoder.layer.3.attention.self.key.weight',\n",
       " 'bert.encoder.layer.3.attention.self.key.bias',\n",
       " 'bert.encoder.layer.3.attention.self.value.weight',\n",
       " 'bert.encoder.layer.3.attention.self.value.bias',\n",
       " 'bert.encoder.layer.3.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.3.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.3.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.3.output.dense.weight',\n",
       " 'bert.encoder.layer.3.output.dense.bias',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.attention.self.query.weight',\n",
       " 'bert.encoder.layer.4.attention.self.query.bias',\n",
       " 'bert.encoder.layer.4.attention.self.key.weight',\n",
       " 'bert.encoder.layer.4.attention.self.key.bias',\n",
       " 'bert.encoder.layer.4.attention.self.value.weight',\n",
       " 'bert.encoder.layer.4.attention.self.value.bias',\n",
       " 'bert.encoder.layer.4.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.4.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.4.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.4.output.dense.weight',\n",
       " 'bert.encoder.layer.4.output.dense.bias',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.attention.self.query.weight',\n",
       " 'bert.encoder.layer.5.attention.self.query.bias',\n",
       " 'bert.encoder.layer.5.attention.self.key.weight',\n",
       " 'bert.encoder.layer.5.attention.self.key.bias',\n",
       " 'bert.encoder.layer.5.attention.self.value.weight',\n",
       " 'bert.encoder.layer.5.attention.self.value.bias',\n",
       " 'bert.encoder.layer.5.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.5.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.5.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.5.output.dense.weight',\n",
       " 'bert.encoder.layer.5.output.dense.bias',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.attention.self.query.weight',\n",
       " 'bert.encoder.layer.6.attention.self.query.bias',\n",
       " 'bert.encoder.layer.6.attention.self.key.weight',\n",
       " 'bert.encoder.layer.6.attention.self.key.bias',\n",
       " 'bert.encoder.layer.6.attention.self.value.weight',\n",
       " 'bert.encoder.layer.6.attention.self.value.bias',\n",
       " 'bert.encoder.layer.6.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.6.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.6.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.6.output.dense.weight',\n",
       " 'bert.encoder.layer.6.output.dense.bias',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.attention.self.query.weight',\n",
       " 'bert.encoder.layer.7.attention.self.query.bias',\n",
       " 'bert.encoder.layer.7.attention.self.key.weight',\n",
       " 'bert.encoder.layer.7.attention.self.key.bias',\n",
       " 'bert.encoder.layer.7.attention.self.value.weight',\n",
       " 'bert.encoder.layer.7.attention.self.value.bias',\n",
       " 'bert.encoder.layer.7.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.7.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.7.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.7.output.dense.weight',\n",
       " 'bert.encoder.layer.7.output.dense.bias',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.attention.self.query.weight',\n",
       " 'bert.encoder.layer.8.attention.self.query.bias',\n",
       " 'bert.encoder.layer.8.attention.self.key.weight',\n",
       " 'bert.encoder.layer.8.attention.self.key.bias',\n",
       " 'bert.encoder.layer.8.attention.self.value.weight',\n",
       " 'bert.encoder.layer.8.attention.self.value.bias',\n",
       " 'bert.encoder.layer.8.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.8.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.8.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.8.output.dense.weight',\n",
       " 'bert.encoder.layer.8.output.dense.bias',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.attention.self.query.weight',\n",
       " 'bert.encoder.layer.9.attention.self.query.bias',\n",
       " 'bert.encoder.layer.9.attention.self.key.weight',\n",
       " 'bert.encoder.layer.9.attention.self.key.bias',\n",
       " 'bert.encoder.layer.9.attention.self.value.weight',\n",
       " 'bert.encoder.layer.9.attention.self.value.bias',\n",
       " 'bert.encoder.layer.9.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.9.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.9.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.9.output.dense.weight',\n",
       " 'bert.encoder.layer.9.output.dense.bias',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.attention.self.query.weight',\n",
       " 'bert.encoder.layer.10.attention.self.query.bias',\n",
       " 'bert.encoder.layer.10.attention.self.key.weight',\n",
       " 'bert.encoder.layer.10.attention.self.key.bias',\n",
       " 'bert.encoder.layer.10.attention.self.value.weight',\n",
       " 'bert.encoder.layer.10.attention.self.value.bias',\n",
       " 'bert.encoder.layer.10.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.10.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.10.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.10.output.dense.weight',\n",
       " 'bert.encoder.layer.10.output.dense.bias',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.attention.self.query.weight',\n",
       " 'bert.encoder.layer.11.attention.self.query.bias',\n",
       " 'bert.encoder.layer.11.attention.self.key.weight',\n",
       " 'bert.encoder.layer.11.attention.self.key.bias',\n",
       " 'bert.encoder.layer.11.attention.self.value.weight',\n",
       " 'bert.encoder.layer.11.attention.self.value.bias',\n",
       " 'bert.encoder.layer.11.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.11.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.11.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.11.output.dense.weight',\n",
       " 'bert.encoder.layer.11.output.dense.bias',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.bias',\n",
       " 'classifier.weight',\n",
       " 'classifier.bias']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, params in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:06:35.278576Z",
     "iopub.status.busy": "2025-06-08T06:06:35.278158Z",
     "iopub.status.idle": "2025-06-08T06:06:35.812127Z",
     "shell.execute_reply": "2025-06-08T06:06:35.811332Z",
     "shell.execute_reply.started": "2025-06-08T06:06:35.278547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# dataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n",
    "\n",
    "\n",
    "# 模型创建\n",
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                        num_labels=21,\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model.to('cuda')\n",
    "\n",
    "# 模型参数分组\n",
    "param_optimizer = list(model.named_parameters())\n",
    "bert_params, classifier_params = [],[]\n",
    "\n",
    "for name,params in param_optimizer:\n",
    "    if 'bert' in name:\n",
    "        bert_params.append(params)\n",
    "    else:\n",
    "        classifier_params.append(params)\n",
    "\n",
    "param_groups = [\n",
    "    {'params':bert_params, 'lr':1e-5},\n",
    "    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n",
    "]\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.AdamW(param_groups) # 优化器\n",
    "\n",
    "# 学习率调度器\n",
    "train_steps = len(train_dl) * 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=100, \n",
    "                                            num_training_steps=train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:07:05.787651Z",
     "iopub.status.busy": "2025-06-08T06:07:05.787096Z",
     "iopub.status.idle": "2025-06-08T06:07:05.802725Z",
     "shell.execute_reply": "2025-06-08T06:07:05.802155Z",
     "shell.execute_reply.started": "2025-06-08T06:07:05.787629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512]) torch.Size([16, 512]) torch.Size([16, 512]) torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "for item in train_dl:\n",
    "    print(item['input_ids'].shape, \n",
    "          item['token_type_ids'].shape, \n",
    "          item['attention_mask'].shape, \n",
    "          item['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:27:59.000401Z",
     "iopub.status.busy": "2025-06-08T06:27:58.999711Z",
     "iopub.status.idle": "2025-06-08T06:28:00.647575Z",
     "shell.execute_reply": "2025-06-08T06:28:00.646628Z",
     "shell.execute_reply.started": "2025-06-08T06:27:59.000365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/672 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/3988290948.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                           \u001b[0;34mf'bert_lr:{scheduler.get_lr()[0]} '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                           \u001b[0;34mf'classifier_lr:{scheduler.get_lr()[1]} '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                           f'Loss:{loss.item():.4f}')\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE='cuda'\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    tpbar = tqdm(train_dl)\n",
    "    for items in tpbar:\n",
    "        items = {k:v.to(DEVICE) for k,v in items.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**items)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "        tpbar.set_description(f'Epoch:{epoch+1} ' + \n",
    "                          f'bert_lr:{scheduler.get_lr()[0]} ' + \n",
    "                          f'classifier_lr:{scheduler.get_lr()[1]} '+\n",
    "                          f'Loss:{loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持混合精度训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:28:59.592474Z",
     "iopub.status.busy": "2025-06-08T06:28:59.591891Z",
     "iopub.status.idle": "2025-06-08T06:28:59.969676Z",
     "shell.execute_reply": "2025-06-08T06:28:59.969099Z",
     "shell.execute_reply.started": "2025-06-08T06:28:59.592454Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# dataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n",
    "\n",
    "\n",
    "# 模型创建\n",
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                        num_labels=21,\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model.to('cuda')\n",
    "\n",
    "# 模型参数分组\n",
    "param_optimizer = list(model.named_parameters())\n",
    "bert_params, classifier_params = [],[]\n",
    "\n",
    "for name,params in param_optimizer:\n",
    "    if 'bert' in name:\n",
    "        bert_params.append(params)\n",
    "    else:\n",
    "        classifier_params.append(params)\n",
    "\n",
    "param_groups = [\n",
    "    {'params':bert_params, 'lr':1e-5},\n",
    "    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n",
    "]\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.AdamW(param_groups) # 优化器\n",
    "\n",
    "# 学习率调度器\n",
    "train_steps = len(train_dl) * 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=100, \n",
    "                                            num_training_steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T06:30:31.703371Z",
     "iopub.status.busy": "2025-06-08T06:30:31.702681Z",
     "iopub.status.idle": "2025-06-08T06:53:51.954872Z",
     "shell.execute_reply": "2025-06-08T06:53:51.954130Z",
     "shell.execute_reply.started": "2025-06-08T06:30:31.703349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 bert_lr:8.245398773006135e-06 classifier_lr:0.0008245398773006136 Loss:0.0283: 100%|██████████| 672/672 [04:44<00:00,  2.36it/s] \n",
      "Epoch:2 bert_lr:6.184049079754602e-06 classifier_lr:0.0006184049079754601 Loss:0.0179: 100%|██████████| 672/672 [04:39<00:00,  2.40it/s] \n",
      "Epoch:3 bert_lr:4.122699386503068e-06 classifier_lr:0.0004122699386503068 Loss:0.0215: 100%|██████████| 672/672 [04:38<00:00,  2.42it/s]  \n",
      "Epoch:4 bert_lr:2.061349693251534e-06 classifier_lr:0.0002061349693251534 Loss:0.0115: 100%|██████████| 672/672 [04:38<00:00,  2.41it/s]  \n",
      "Epoch:5 bert_lr:0.0 classifier_lr:0.0 Loss:0.0132: 100%|██████████| 672/672 [04:39<00:00,  2.41it/s]                                      \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "DEVICE='cuda'\n",
    "\n",
    "# 梯度计算缩放器\n",
    "scaler = torch.GradScaler()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    tpbar = tqdm(train_dl)\n",
    "    for items in tpbar:\n",
    "        items = {k:v.to(DEVICE) for k,v in items.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda'):\n",
    "            outputs = model(**items)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 缩放loss后，调用backward\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "    \n",
    "        tpbar.set_description(f'Epoch:{epoch+1} ' + \n",
    "                          f'bert_lr:{scheduler.get_lr()[0]} ' + \n",
    "                          f'classifier_lr:{scheduler.get_lr()[1]} '+\n",
    "                          f'Loss:{loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持分布式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile ddp_simple.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# 定义训练循环\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # 定义模型并将其移动到对应的 GPU 设备端\n",
    "    model = models.resnet50().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # 损失函数及优化器\n",
    "    criterion = nn.CrossEntropyLoss().to(rank)\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)\n",
    "    \n",
    "    # 定义数据集Dataset的转换和图像增强\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    # 分布式训练采样器\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "    \n",
    "    # 在训练开始时创建一次\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        ddp_model.train()\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(rank), labels.to(rank)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = ddp_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(ddp_model.parameters(), 1)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n",
    "    \n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python ddp_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer模型训练\n",
    "### TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:46:15.879149Z",
     "iopub.status.busy": "2025-06-08T08:46:15.878799Z",
     "iopub.status.idle": "2025-06-08T08:46:15.888079Z",
     "shell.execute_reply": "2025-06-08T08:46:15.887345Z",
     "shell.execute_reply.started": "2025-06-08T08:46:15.879121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ner_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ner_ddp.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 数据集\n",
    "    ds = load_dataset('nlhappy/CLUE-NER')\n",
    "    # entity_index\n",
    "    entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "               'company', 'scene', 'book', 'organization', 'government'})\n",
    "    tags = ['O']\n",
    "    for entity in entites[1:]:\n",
    "        tags.append('B-' + entity.upper())\n",
    "        tags.append('I-' + entity.upper())\n",
    "    \n",
    "    entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "    \n",
    "    def entity_tags_proc(item):\n",
    "        # item即是dataset中记录\n",
    "        text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "        tags = [0] * text_len    # 初始值为‘O’\n",
    "        # 遍历实体列表，所有实体类别标记填入tags\n",
    "        entites = item['ents']\n",
    "        for ent in entites:\n",
    "            indices = ent['indices']  # 实体索引\n",
    "            label = ent['label']   # 实体名\n",
    "            tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "            for idx in indices[1:]:\n",
    "                tags[idx] = entity_index[label] * 2\n",
    "        return {'ent_tag': tags}\n",
    "    \n",
    "    # 使用自定义回调函数处理数据集记录\n",
    "    ds1 = ds.map(entity_tags_proc)\n",
    "    \n",
    "    def data_input_proc(item):\n",
    "        # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "        batch_texts = [list(text) for text in item['text']]\n",
    "        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "        input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n",
    "                               is_split_into_words=True, padding='max_length')\n",
    "        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "        return input_data\n",
    "        \n",
    "    \n",
    "    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n",
    "    \n",
    "    \n",
    "    local_rank = rank\n",
    "    \n",
    "    id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "    lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                            num_labels=21,\n",
    "                                                            id2label=id2lbl,\n",
    "                                                            label2id=lbl2id)\n",
    "    model.to(local_rank)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "        num_train_epochs = 3,    # 训练 epoch\n",
    "        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "        per_device_train_batch_size=16,  # 训练批次\n",
    "        per_device_eval_batch_size=16,\n",
    "        report_to='tensorboard',  # 训练输出记录\n",
    "        eval_strategy=\"epoch\",\n",
    "        local_rank=local_rank,   # 当前进程 RANK\n",
    "        fp16=True,               # 使用混合精度\n",
    "        lr_scheduler_type='linear',  # 动态学习率\n",
    "        warmup_steps=100,        # 预热步数\n",
    "        ddp_find_unused_parameters=False  # 优化DDP性能\n",
    "    )\n",
    "    \n",
    "    def compute_metric(result):\n",
    "        # result 是一个tuple (predicts, labels)\n",
    "        \n",
    "        # 获取评估对象\n",
    "        seqeval = evaluate.load('seqeval')\n",
    "        predicts,labels = result\n",
    "        predicts = np.argmax(prdicts, axis=2)\n",
    "        \n",
    "        # 准备评估数据\n",
    "        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                     for ps,ls in zip(predicts,labels)]\n",
    "        labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                     for ps,ls in zip(predicts,labels)]\n",
    "        results = seqeval.compute(predictions=predicts, references=labels)\n",
    "    \n",
    "        return results\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=ds2['train'],\n",
    "        eval_dataset=ds2['validation'],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metric\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T08:46:23.773414Z",
     "iopub.status.busy": "2025-06-08T08:46:23.772787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-08 08:46:28.971282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749372388.994218     381 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749372389.001484     381 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-08 08:46:39.164195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749372399.189729     395 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749372399.198344     395 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-08 08:46:39.206287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749372399.230075     396 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749372399.237017     396 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Map: 100%|███████████████████████| 10748/10748 [00:01<00:00, 9359.50 examples/s]\n",
      "Map: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 9030.15 examples/s]\n",
      "Map: 100%|███████████████████████| 10748/10748 [00:01<00:00, 8900.02 examples/s]\n",
      "Map: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 8924.48 examples/s]\n",
      "Map: 100%|███████████████████████| 10748/10748 [00:05<00:00, 1847.77 examples/s]\n",
      "Map: 100%|███████████████████████| 10748/10748 [00:05<00:00, 1842.79 examples/s]\n",
      "Map: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1797.83 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1874.83 examples/s]\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "  0%|                                                  | 0/1008 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "  3%|█▏                                       | 28/1008 [01:35<57:22,  3.51s/it]"
     ]
    }
   ],
   "source": [
    "!python ner_ddp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
