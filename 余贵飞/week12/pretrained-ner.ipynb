{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d9034e",
   "metadata": {},
   "source": [
    "### 1.导入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03d69182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:14:49.367632Z",
     "iopub.status.busy": "2025-06-03T13:14:49.367334Z",
     "iopub.status.idle": "2025-06-03T13:14:49.372703Z",
     "shell.execute_reply": "2025-06-03T13:14:49.371863Z",
     "shell.execute_reply.started": "2025-06-03T13:14:49.367611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 导入相关依赖包\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification, AutoModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4aeb1",
   "metadata": {},
   "source": [
    "### 2.初始化预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d03cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:26.792958Z",
     "iopub.status.busy": "2025-06-03T13:10:26.792702Z",
     "iopub.status.idle": "2025-06-03T13:10:26.931045Z",
     "shell.execute_reply": "2025-06-03T13:10:26.930457Z",
     "shell.execute_reply.started": "2025-06-03T13:10:26.792939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型使用 模型名称为：google-bert/bert-base-chinese 最终预测类别为21个对应最后输出层神经元个数\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-chinese\", num_labels=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35142c73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:31.016666Z",
     "iopub.status.busy": "2025-06-03T13:10:31.016386Z",
     "iopub.status.idle": "2025-06-03T13:10:31.023161Z",
     "shell.execute_reply": "2025-06-03T13:10:31.022325Z",
     "shell.execute_reply.started": "2025-06-03T13:10:31.016644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型结构\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d69f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:34.747804Z",
     "iopub.status.busy": "2025-06-03T13:10:34.747501Z",
     "iopub.status.idle": "2025-06-03T13:10:38.795567Z",
     "shell.execute_reply": "2025-06-03T13:10:38.794685Z",
     "shell.execute_reply.started": "2025-06-03T13:10:34.747779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用AutoModel 加载预训练模型对对比 AutoModelForTokenClassification 加在模型的区别, 使用AutoModel 下载预训练模型只能加载最原始的bert模型结构，在模型最后的输出后面没有\n",
    "# 用于做分类的线性层可供使用\n",
    "model1 = AutoModel.from_pretrained(\"google-bert/bert-base-chinese\", num_labels=21)\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe398d",
   "metadata": {},
   "source": [
    "### 3.初始化分词器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0956e9b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:38.797456Z",
     "iopub.status.busy": "2025-06-03T13:10:38.796883Z",
     "iopub.status.idle": "2025-06-03T13:10:39.332782Z",
     "shell.execute_reply": "2025-06-03T13:10:39.332221Z",
     "shell.execute_reply.started": "2025-06-03T13:10:38.797435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8dbf921ee44672af7ce740f6e5e393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b845534ea06e4cc99910a781361536a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dc58c08a1446af81a38a0c014891ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用AutoTokenizer 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6a2e339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:42.536295Z",
     "iopub.status.busy": "2025-06-03T13:10:42.535352Z",
     "iopub.status.idle": "2025-06-03T13:10:42.542567Z",
     "shell.execute_reply": "2025-06-03T13:10:42.541711Z",
     "shell.execute_reply.started": "2025-06-03T13:10:42.536254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看分词器结构\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2d0e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:45.631043Z",
     "iopub.status.busy": "2025-06-03T13:10:45.630544Z",
     "iopub.status.idle": "2025-06-03T13:10:45.643710Z",
     "shell.execute_reply": "2025-06-03T13:10:45.642983Z",
     "shell.execute_reply.started": "2025-06-03T13:10:45.631020Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  791, 1921, 3221,  702, 1962, 1921, 3698,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101, 4684, 2970, 2900, 1403, 2769,  812, 6206, 1217, 6770, 4638, 3563,\n",
       "         1798, 1399, 2772, 2100, 4669,  100, 2497,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词器测试\n",
    "input_data = tokenizer([\"今天是个好天气\", \"直接指向我们要加载的模型名或存盘⽬录\"], return_tensors = 'pt', padding=True, truncation=True, max_length=512)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38fcdd7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:48.435406Z",
     "iopub.status.busy": "2025-06-03T13:10:48.435102Z",
     "iopub.status.idle": "2025-06-03T13:10:48.664677Z",
     "shell.execute_reply": "2025-06-03T13:10:48.663768Z",
     "shell.execute_reply.started": "2025-06-03T13:10:48.435385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1462, 1399, 2141,  860, 6399, 1166,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "None\n",
      "tensor([[[-2.4484e-01,  1.7518e-02, -3.2333e-01,  4.1861e-01,  5.3770e-01,\n",
      "           6.0092e-02,  1.2795e-01,  8.0413e-03,  3.6765e-01, -1.8829e-01,\n",
      "          -4.0653e-01,  3.6950e-01,  5.0703e-01,  4.7109e-01, -2.4483e-02,\n",
      "           7.9691e-01,  2.0414e-01,  1.7849e-01,  7.4815e-01, -2.1226e-01,\n",
      "           6.9242e-01],\n",
      "         [ 3.5343e-01,  4.4996e-01, -3.3618e-01,  5.8534e-01,  5.7982e-01,\n",
      "          -1.0071e-01,  4.0454e-01,  5.7865e-02,  2.7064e-01,  1.9377e-01,\n",
      "           1.8542e-01, -1.0717e-02,  1.5382e-01,  2.1916e-01,  4.4539e-01,\n",
      "           8.2503e-01, -1.5035e-01,  5.3618e-01,  3.0218e-01, -1.0195e-01,\n",
      "           5.8157e-01],\n",
      "         [-6.2226e-03,  1.5144e-01, -6.5257e-01,  1.2281e+00,  3.3460e-01,\n",
      "           2.2465e-01,  7.8376e-01,  5.7116e-01,  6.1610e-01,  2.5007e-01,\n",
      "          -1.6747e-02,  3.6158e-01, -5.3519e-02,  3.2189e-01,  2.2236e-01,\n",
      "           1.4259e-01, -4.1168e-01,  4.9813e-01, -1.6288e-01,  1.0026e+00,\n",
      "           3.9959e-01],\n",
      "         [ 2.0557e-01,  1.8189e-02, -2.8749e-01,  1.2025e+00,  5.3977e-01,\n",
      "           2.7404e-01,  5.5125e-01, -8.4771e-02,  9.5501e-01, -1.4726e-01,\n",
      "           4.3151e-01,  1.2988e-01,  3.7424e-01,  2.1889e-01, -5.8165e-02,\n",
      "           4.8558e-01, -2.9446e-01,  3.9076e-01,  1.7270e-02, -6.5910e-02,\n",
      "           3.3007e-01],\n",
      "         [ 3.4973e-01, -2.2899e-01, -5.3575e-01,  1.1443e+00,  4.7326e-01,\n",
      "           2.3552e-01,  8.7048e-01,  2.4231e-01,  8.8810e-01,  2.7091e-01,\n",
      "          -1.4541e-01,  3.0121e-01,  6.8209e-04,  3.9287e-01,  2.1182e-01,\n",
      "           3.0677e-01, -1.6639e-01,  3.5468e-01,  4.0698e-01,  6.4728e-01,\n",
      "           7.6580e-01],\n",
      "         [-3.3530e-01, -3.3423e-01, -4.2567e-01,  5.6247e-01,  1.4536e-01,\n",
      "           1.4056e-01,  2.1469e-01,  7.3184e-02,  5.8138e-02, -3.2431e-01,\n",
      "          -2.0950e-01, -7.7515e-02,  1.8729e-01,  8.0142e-01,  1.9061e-01,\n",
      "           6.2374e-01, -4.8212e-01,  1.7331e-01, -3.0253e-01,  5.1089e-01,\n",
      "           2.3033e-01],\n",
      "         [-2.8831e-01, -5.1962e-01,  3.3635e-01,  9.2702e-01,  1.4832e-01,\n",
      "           3.4361e-01,  9.8702e-01,  3.8836e-01,  4.7900e-01, -2.5148e-01,\n",
      "          -4.4026e-01,  2.2399e-01,  6.4050e-02,  4.2944e-01,  7.3008e-02,\n",
      "           4.9700e-01, -2.1686e-01,  8.9131e-02, -4.7639e-01,  1.3461e-01,\n",
      "           7.7115e-01],\n",
      "         [-5.7396e-01,  4.7988e-01, -2.4769e-01,  6.5396e-01,  9.2112e-01,\n",
      "           4.9969e-01,  1.9144e-01,  1.8085e-01,  6.0651e-01, -3.8463e-02,\n",
      "          -5.7078e-01,  5.7528e-01,  4.9548e-01,  2.2288e-01, -8.7071e-01,\n",
      "           9.6056e-02, -1.2391e-01,  4.0790e-02, -5.3301e-01,  2.6143e-01,\n",
      "          -3.6843e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "message= \"命名实体识别\"\n",
    "label = torch.tensor([0,1,0,2,5,4])\n",
    "\n",
    "model_input = tokenizer1([message], return_tensors='pt')\n",
    "result = model(**model_input)\n",
    "print(model_input)\n",
    "\n",
    "print(result.loss)\n",
    "print(result.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f899902",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08d275a2-88f4-4ede-9569-495da685cb7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:52.236616Z",
     "iopub.status.busy": "2025-06-03T13:10:52.236325Z",
     "iopub.status.idle": "2025-06-03T13:10:53.732793Z",
     "shell.execute_reply": "2025-06-03T13:10:53.732063Z",
     "shell.execute_reply.started": "2025-06-03T13:10:52.236594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3221b34d624f508e9c7a1e2fc6bcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24650fa1d1004909b1eea9fc057feafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9182fe1cbee2467e95acf3566adc4ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-a33d0e4276aef9b4.parquet:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65072ec9795b4b4a88d431dfa8291514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-07f476b71c5edde6.parquet:   0%|          | 0.00/178k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd456e26af054d56954bf8209c6355ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7073722a8baa4660b1b239599e008286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载dataset\n",
    "ds = load_dataset('nlhappy/CLUE-NER')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1dca287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:55.670400Z",
     "iopub.status.busy": "2025-06-03T13:10:55.670083Z",
     "iopub.status.idle": "2025-06-03T13:10:55.677042Z",
     "shell.execute_reply": "2025-06-03T13:10:55.676025Z",
     "shell.execute_reply.started": "2025-06-03T13:10:55.670376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，',\n",
       " 'ents': [{'indices': [9, 10, 11],\n",
       "   'is_continuous': True,\n",
       "   'label': 'name',\n",
       "   'text': '叶老桂'},\n",
       "  {'indices': [0, 1, 2, 3],\n",
       "   'is_continuous': True,\n",
       "   'label': 'company',\n",
       "   'text': '浙商银行'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据结构\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf5bd3cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:10:58.518003Z",
     "iopub.status.busy": "2025-06-03T13:10:58.517722Z",
     "iopub.status.idle": "2025-06-03T13:10:58.523813Z",
     "shell.execute_reply": "2025-06-03T13:10:58.523218Z",
     "shell.execute_reply.started": "2025-06-03T13:10:58.517981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '彭小军认为，国内银行现在走的是台湾的发卡模式，先通过跑马圈地再在圈的地里面选择客户，',\n",
       " 'ents': [{'indices': [15, 16],\n",
       "   'is_continuous': True,\n",
       "   'label': 'address',\n",
       "   'text': '台湾'},\n",
       "  {'indices': [0, 1, 2],\n",
       "   'is_continuous': True,\n",
       "   'label': 'name',\n",
       "   'text': '彭小军'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef2514a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:02.053684Z",
     "iopub.status.busy": "2025-06-03T13:11:02.053401Z",
     "iopub.status.idle": "2025-06-03T13:11:02.764857Z",
     "shell.execute_reply": "2025-06-03T13:11:02.764140Z",
     "shell.execute_reply.started": "2025-06-03T13:11:02.053662Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie', 'address', 'scene', 'position', 'book', 'name', 'game', 'company', 'organization', 'government'}\n"
     ]
    }
   ],
   "source": [
    "# 通过代码获取数据中标签的种类\n",
    "# 对于训练数据集\n",
    "train_data = ds['train']\n",
    "labels_set = set()\n",
    "for item in train_data:\n",
    "    # 遍历训练数据集\n",
    "    info_list = item['ents']\n",
    "    for i in range(len(info_list)):\n",
    "       # 取出标注数据\n",
    "       labels_set.add(info_list[i][\"label\"])\n",
    "print(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6cbe6f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:07.022564Z",
     "iopub.status.busy": "2025-06-03T13:11:07.021913Z",
     "iopub.status.idle": "2025-06-03T13:11:07.124441Z",
     "shell.execute_reply": "2025-06-03T13:11:07.123724Z",
     "shell.execute_reply.started": "2025-06-03T13:11:07.022538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie', 'scene', 'address', 'position', 'book', 'name', 'game', 'company', 'organization', 'government'}\n"
     ]
    }
   ],
   "source": [
    "# 通过代码获取数据中标签的种类\n",
    "# 对于验证数据集\n",
    "train_data = ds['validation']\n",
    "labels_set = set()\n",
    "for item in train_data:\n",
    "    # 遍历训练数据集\n",
    "    info_list = item['ents']\n",
    "    for i in range(len(info_list)):\n",
    "       # 取出标注数据\n",
    "       labels_set.add(info_list[i][\"label\"])\n",
    "print(labels_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32165fae",
   "metadata": {},
   "source": [
    "### 类别映射词典准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93c32286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:18.897048Z",
     "iopub.status.busy": "2025-06-03T13:11:18.896352Z",
     "iopub.status.idle": "2025-06-03T13:11:18.902621Z",
     "shell.execute_reply": "2025-06-03T13:11:18.901709Z",
     "shell.execute_reply.started": "2025-06-03T13:11:18.897023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'movie', 'scene', 'address', 'position', 'book', 'name', 'game', 'company', 'organization', 'government']\n",
      "['O', 'B-MOVIE', 'I-MOVIE', 'B-SCENE', 'I-SCENE', 'B-ADDRESS', 'I-ADDRESS', 'B-POSITION', 'I-POSITION', 'B-BOOK', 'I-BOOK', 'B-NAME', 'I-NAME', 'B-GAME', 'I-GAME', 'B-COMPANY', 'I-COMPANY', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-GOVERNMENT', 'I-GOVERNMENT']\n",
      "{'O': 0, 'movie': 1, 'scene': 2, 'address': 3, 'position': 4, 'book': 5, 'name': 6, 'game': 7, 'company': 8, 'organization': 9, 'government': 10}\n"
     ]
    }
   ],
   "source": [
    "# 组装类别词典列表（使用序列标注，三位标注法）(最后tags 中标签的数量即为模型最终输出线性层输出的数量，用于最终的分类)\n",
    "entitys = ['O'] + list(labels_set)\n",
    "print(entitys)\n",
    "\n",
    "tags = []\n",
    "# 添加标签前缀\n",
    "for item in entitys:\n",
    "    if item != 'O':\n",
    "        # 其余标签都添加 B- 标识命名实体开始标签\n",
    "        tags.append(\"B-\" + item.upper())\n",
    "        # 添加 I- 标识命名实体本身\n",
    "        tags.append(\"I-\" + item.upper())\n",
    "    else:\n",
    "        # 非命名实体的用“O” 标识\n",
    "        tags.append(item)\n",
    "print(tags)\n",
    "\n",
    "# 给实体类别添加索引值并组装为字典\n",
    "entity_index = {entity:i for i, entity in enumerate(entitys)}\n",
    "print(entity_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6153de3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:22.081098Z",
     "iopub.status.busy": "2025-06-03T13:11:22.080370Z",
     "iopub.status.idle": "2025-06-03T13:11:23.529141Z",
     "shell.execute_reply": "2025-06-03T13:11:23.528241Z",
     "shell.execute_reply.started": "2025-06-03T13:11:22.081070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c38932635c45c2801a69f4ce38130a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c93087f707429cbc8bbef24dbc4399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 定义数据集数据-标签映射函数，处理数据集数据\n",
    "def entity_tags_precess_map(item):\n",
    "    # 先取出数据中文本\n",
    "    text = item['text']\n",
    "    # 根据文本长度初始化整个文本标记的的初始值\n",
    "    # 均初始化为非命名实体标记字符\n",
    "    text_tags = len(text) * [0]\n",
    "    # 遍历文本标记信息\n",
    "    for tag_info in item['ents']:\n",
    "        # 获取命名实体类别标签\n",
    "        label = tag_info['label']\n",
    "        # 获取命名实体在实体类别字典中的索引\n",
    "        label_index = entity_index[label]\n",
    "        # 获取当前数据中当前命名实体在 tags list 中的标签\n",
    "        # 开始位置\n",
    "        start = label_index * 2 - 1\n",
    "        # 实体本身\n",
    "        inside = label_index * 2\n",
    "        # 命名实体开始位置\n",
    "        tag_text_index_list = tag_info['indices']\n",
    "        text_tags[tag_text_index_list[0]] = start\n",
    "        # 获取命名实体剩余的部分\n",
    "        for tag_index in tag_text_index_list[1:]:\n",
    "            # 替换 text_tags 中命名实体对应的标签值\n",
    "            text_tags[tag_index] = inside\n",
    "    # 此映射函数需要再原来的dataset 字典中添加一个文本序列标注的list\n",
    "    return {'ent_tag':text_tags}\n",
    "\n",
    "# 使用自定义的回调函数处理数据集\n",
    "ds1 = ds.map(entity_tags_precess_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27751b15-7152-46a1-9884-157043fe5376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:26.827627Z",
     "iopub.status.busy": "2025-06-03T13:11:26.827343Z",
     "iopub.status.idle": "2025-06-03T13:11:26.832669Z",
     "shell.execute_reply": "2025-06-03T13:11:26.832067Z",
     "shell.execute_reply.started": "2025-06-03T13:11:26.827605Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看处理过的数据集发现字典中多了一个 “ent_tag” 的属性\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcb90f13-e41a-4487-8335-2f73666d24dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:30.084218Z",
     "iopub.status.busy": "2025-06-03T13:11:30.083482Z",
     "iopub.status.idle": "2025-06-03T13:11:30.089982Z",
     "shell.execute_reply": "2025-06-03T13:11:30.089200Z",
     "shell.execute_reply.started": "2025-06-03T13:11:30.084191Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 继续查询数据详情\n",
    "ds1['train'][0]['ent_tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86c118-bc7d-4453-804b-55fe1e7210f9",
   "metadata": {},
   "source": [
    "#### 中文bert分词在日期时间和英文转换token过程中，出现合并。影响ner标注准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "673850a5-ab46-40e8-9246-58bf5804fa03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:38.196535Z",
     "iopub.status.busy": "2025-06-03T13:11:38.195672Z",
     "iopub.status.idle": "2025-06-03T13:11:38.201689Z",
     "shell.execute_reply": "2025-06-03T13:11:38.200879Z",
     "shell.execute_reply.started": "2025-06-03T13:11:38.196500Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8202, 2399, 123, 3299, 11643, 8778, 8199]\n",
      "2000 年 2 月 hahah\n"
     ]
    }
   ],
   "source": [
    "token_index = tokenizer.encode('2000年2月hahah', add_special_tokens=False)\n",
    "print(token_index)\n",
    "tokens = tokenizer.decode(token_index)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bead3743-545b-4a7a-be77-c14802cafeec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:41.029911Z",
     "iopub.status.busy": "2025-06-03T13:11:41.029637Z",
     "iopub.status.idle": "2025-06-03T13:11:41.035973Z",
     "shell.execute_reply": "2025-06-03T13:11:41.035401Z",
     "shell.execute_reply.started": "2025-06-03T13:11:41.029892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8202, 2399, 123, 3299, 10253]], 'token_type_ids': [[0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tokenizer(['2000年2月add'], add_special_tokens=False, truncation=True)\n",
    "print(input_data)\n",
    "\n",
    "input_data.word_ids(0) # 返回批次中指定token对应原始文本的索引映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fcd2945-7c05-44a7-a587-d79a46fe4c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:45.357509Z",
     "iopub.status.busy": "2025-06-03T13:11:45.357228Z",
     "iopub.status.idle": "2025-06-03T13:11:47.063299Z",
     "shell.execute_reply": "2025-06-03T13:11:47.062408Z",
     "shell.execute_reply.started": "2025-06-03T13:11:45.357488Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673fdf6e8f2842e6a8250068601d4997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935a0ee502f3403eb2385af914f196ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始文本转换模型需要token_idx,生成和token_idx对齐label\n",
    "def data_input_proc(item):\n",
    "    # 输入文本转换模型输入token索引\n",
    "    input_data = tokenizer(item['text'], truncation=True, add_special_tokens=False, max_length=512)\n",
    "    adjust_labels = []  # 所有修正后label索引列表\n",
    "    # 上一步骤生成ner_tag中索引和token对齐\n",
    "    for k in range(len(input_data['input_ids'])):\n",
    "        # 每条记录token对应word_ids\n",
    "        word_ids = input_data.word_ids(k)\n",
    "        # 批次ner_tag长度和token长度对齐\n",
    "        tags = item['ent_tag'][k]\n",
    "        \n",
    "        adjusted_label_ids = []\n",
    "        i, prev_wid = -1,-1\n",
    "        for wid in word_ids:\n",
    "            if (wid != prev_wid):   #  word_ids [1,1,1,2,3,4,5] -> [0,1,2,3,4,5,6]\n",
    "                i += 1 # token对应检索位置+1\n",
    "                prev_wid = wid\n",
    "            adjusted_label_ids.append(tags[i])\n",
    "        adjust_labels.append(adjusted_label_ids)                \n",
    "    # 修正后label添加到input_data\n",
    "    input_data['labels'] = adjust_labels\n",
    "    return input_data\n",
    "    \n",
    "\n",
    "ds2 = ds1.map(data_input_proc, batched=True)  # batched 每次传入自定义方法样本数量多个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af1cd5d4-f323-479c-bfd9-af0e0be5bf58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:50.524612Z",
     "iopub.status.busy": "2025-06-03T13:11:50.524260Z",
     "iopub.status.idle": "2025-06-03T13:11:50.530942Z",
     "shell.execute_reply": "2025-06-03T13:11:50.530153Z",
     "shell.execute_reply.started": "2025-06-03T13:11:50.524585Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bedd127e-da58-4d59-b460-63c6fc6e55cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:55.228569Z",
     "iopub.status.busy": "2025-06-03T13:11:55.228279Z",
     "iopub.status.idle": "2025-06-03T13:11:55.235003Z",
     "shell.execute_reply": "2025-06-03T13:11:55.234137Z",
     "shell.execute_reply.started": "2025-06-03T13:11:55.228547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2['train'][0]['ent_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddeda0df-3f66-4cf2-8e08-e974d9390b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:11:59.370053Z",
     "iopub.status.busy": "2025-06-03T13:11:59.369455Z",
     "iopub.status.idle": "2025-06-03T13:11:59.374923Z",
     "shell.execute_reply": "2025-06-03T13:11:59.374091Z",
     "shell.execute_reply.started": "2025-06-03T13:11:59.370028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 记录转换为pytorch\n",
    "ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb4be84f-231e-46cb-afa3-6719b7e53bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:12:02.286822Z",
     "iopub.status.busy": "2025-06-03T13:12:02.286002Z",
     "iopub.status.idle": "2025-06-03T13:12:02.294298Z",
     "shell.execute_reply": "2025-06-03T13:12:02.293466Z",
     "shell.execute_reply.started": "2025-06-03T13:12:02.286794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([3851, 1555, 7213, 6121,  821,  689,  928, 6587, 6956, 1383, 5439, 3424,\n",
      "        1300, 1894, 1156,  794, 1369,  671,  702, 6235, 2428, 2190,  758, 6887,\n",
      "        7305, 3546, 6822, 6121,  749, 6237, 6438,  511, 1383, 5439, 3424, 6371,\n",
      "         711, 8024, 2190, 4680, 1184, 1744, 1079, 1555,  689, 7213, 6121, 5445,\n",
      "        6241, 8024]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]), 'labels': tensor([15, 16, 16, 16,  0,  0,  0,  0,  0, 11, 12, 12,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])}\n"
     ]
    }
   ],
   "source": [
    " for item in ds2['train']:\n",
    "     print(item)\n",
    "     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79729e32-5337-41fb-bd66-1eb246a9bdc9",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4edc7e-1fd7-48be-9d7a-f32a500b3769",
   "metadata": {},
   "source": [
    "#### TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc859949-22f9-476b-964d-5601836ca25a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:12:07.152303Z",
     "iopub.status.busy": "2025-06-03T13:12:07.152003Z",
     "iopub.status.idle": "2025-06-03T13:12:07.187973Z",
     "shell.execute_reply": "2025-06-03T13:12:07.187219Z",
     "shell.execute_reply.started": "2025-06-03T13:12:07.152280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "    num_train_epochs = 3,    # 训练 epoch\n",
    "    save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "    per_device_train_batch_size=32,  # 训练批次\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to='tensorboard',  # 训练输出记录\n",
    "    eval_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b19aa-ca04-4040-82ff-fe3553b6a6ae",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4868039-7b91-4394-b47c-aabd247552e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:12:10.615311Z",
     "iopub.status.busy": "2025-06-03T13:12:10.614567Z",
     "iopub.status.idle": "2025-06-03T13:12:10.619211Z",
     "shell.execute_reply": "2025-06-03T13:12:10.618432Z",
     "shell.execute_reply.started": "2025-06-03T13:12:10.615272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-MOVIE', 'I-MOVIE', 'B-SCENE', 'I-SCENE', 'B-ADDRESS', 'I-ADDRESS', 'B-POSITION', 'I-POSITION', 'B-BOOK', 'I-BOOK', 'B-NAME', 'I-NAME', 'B-GAME', 'I-GAME', 'B-COMPANY', 'I-COMPANY', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-GOVERNMENT', 'I-GOVERNMENT']\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ee7ebce-99d0-4b56-b2d1-4bdb492c1ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:12:13.221124Z",
     "iopub.status.busy": "2025-06-03T13:12:13.220552Z",
     "iopub.status.idle": "2025-06-03T13:12:13.359659Z",
     "shell.execute_reply": "2025-06-03T13:12:13.358847Z",
     "shell.execute_reply.started": "2025-06-03T13:12:13.221101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-MOVIE', 2: 'I-MOVIE', 3: 'B-SCENE', 4: 'I-SCENE', 5: 'B-ADDRESS', 6: 'I-ADDRESS', 7: 'B-POSITION', 8: 'I-POSITION', 9: 'B-BOOK', 10: 'I-BOOK', 11: 'B-NAME', 12: 'I-NAME', 13: 'B-GAME', 14: 'I-GAME', 15: 'B-COMPANY', 16: 'I-COMPANY', 17: 'B-ORGANIZATION', 18: 'I-ORGANIZATION', 19: 'B-GOVERNMENT', 20: 'I-GOVERNMENT'}\n",
      "{'O': 0, 'B-MOVIE': 1, 'I-MOVIE': 2, 'B-SCENE': 3, 'I-SCENE': 4, 'B-ADDRESS': 5, 'I-ADDRESS': 6, 'B-POSITION': 7, 'I-POSITION': 8, 'B-BOOK': 9, 'I-BOOK': 10, 'B-NAME': 11, 'I-NAME': 12, 'B-GAME': 13, 'I-GAME': 14, 'B-COMPANY': 15, 'I-COMPANY': 16, 'B-ORGANIZATION': 17, 'I-ORGANIZATION': 18, 'B-GOVERNMENT': 19, 'I-GOVERNMENT': 20}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "print(id2lbl)\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "print(lbl2id)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                        num_labels=21,\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef395e-fb4d-4a93-bb88-7520ebdcb910",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69252884-b150-437f-bd89-92f2c059b751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:18:52.597789Z",
     "iopub.status.busy": "2025-06-03T13:18:52.596970Z",
     "iopub.status.idle": "2025-06-03T13:18:52.603470Z",
     "shell.execute_reply": "2025-06-03T13:18:52.602637Z",
     "shell.execute_reply.started": "2025-06-03T13:18:52.597763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# metric 方法\n",
    "def compute_metric(result):\n",
    "    # result 是一个tuple (predicts, labels)\n",
    "    \n",
    "    # 获取评估对象\n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predicts,labels = result\n",
    "    predicts = np.argmax(predicts, axis=2)\n",
    "    \n",
    "    # 准备评估数据\n",
    "    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    results = seqeval.compute(predictions=predicts, references=labels)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3431e914-b532-4be1-806a-dbb68c75bfdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:18:55.607256Z",
     "iopub.status.busy": "2025-06-03T13:18:55.606943Z",
     "iopub.status.idle": "2025-06-03T13:18:55.634360Z",
     "shell.execute_reply": "2025-06-03T13:18:55.633761Z",
     "shell.execute_reply.started": "2025-06-03T13:18:55.607232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds2['train'],\n",
    "    eval_dataset=ds2['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68012c6f-bab1-4a54-94ee-a9e2bee574ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:18:59.566864Z",
     "iopub.status.busy": "2025-06-03T13:18:59.566564Z",
     "iopub.status.idle": "2025-06-03T13:22:31.278575Z",
     "shell.execute_reply": "2025-06-03T13:22:31.277981Z",
     "shell.execute_reply.started": "2025-06-03T13:18:59.566843Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [504/504 03:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Address</th>\n",
       "      <th>Book</th>\n",
       "      <th>Company</th>\n",
       "      <th>Game</th>\n",
       "      <th>Government</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Name</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Position</th>\n",
       "      <th>Scene</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.336759</td>\n",
       "      <td>{'precision': 0.4251968503937008, 'recall': 0.5806451612903226, 'f1': 0.4909090909090909, 'number': 372}</td>\n",
       "      <td>{'precision': 0.5957446808510638, 'recall': 0.5419354838709678, 'f1': 0.5675675675675675, 'number': 155}</td>\n",
       "      <td>{'precision': 0.585, 'recall': 0.582089552238806, 'f1': 0.5835411471321695, 'number': 402}</td>\n",
       "      <td>{'precision': 0.37800687285223367, 'recall': 0.40441176470588236, 'f1': 0.3907637655417407, 'number': 272}</td>\n",
       "      <td>{'precision': 0.5972696245733788, 'recall': 0.7142857142857143, 'f1': 0.650557620817844, 'number': 245}</td>\n",
       "      <td>{'precision': 0.6832298136645962, 'recall': 0.6707317073170732, 'f1': 0.676923076923077, 'number': 164}</td>\n",
       "      <td>{'precision': 0.7707865168539326, 'recall': 0.7236286919831224, 'f1': 0.7464635473340587, 'number': 474}</td>\n",
       "      <td>{'precision': 0.5844504021447721, 'recall': 0.5989010989010989, 'f1': 0.5915875169606511, 'number': 364}</td>\n",
       "      <td>{'precision': 0.6846153846153846, 'recall': 0.6312056737588653, 'f1': 0.6568265682656826, 'number': 423}</td>\n",
       "      <td>{'precision': 0.6091954022988506, 'recall': 0.4840182648401826, 'f1': 0.5394402035623409, 'number': 219}</td>\n",
       "      <td>0.586587</td>\n",
       "      <td>0.602913</td>\n",
       "      <td>0.594638</td>\n",
       "      <td>0.900386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.307914</td>\n",
       "      <td>{'precision': 0.4859335038363171, 'recall': 0.510752688172043, 'f1': 0.4980340760157274, 'number': 372}</td>\n",
       "      <td>{'precision': 0.5786163522012578, 'recall': 0.5935483870967742, 'f1': 0.5859872611464967, 'number': 155}</td>\n",
       "      <td>{'precision': 0.6177884615384616, 'recall': 0.6393034825870647, 'f1': 0.6283618581907091, 'number': 402}</td>\n",
       "      <td>{'precision': 0.41114982578397213, 'recall': 0.4338235294117647, 'f1': 0.42218246869409665, 'number': 272}</td>\n",
       "      <td>{'precision': 0.6485507246376812, 'recall': 0.7306122448979592, 'f1': 0.6871401151631479, 'number': 245}</td>\n",
       "      <td>{'precision': 0.708029197080292, 'recall': 0.5914634146341463, 'f1': 0.6445182724252492, 'number': 164}</td>\n",
       "      <td>{'precision': 0.7291666666666666, 'recall': 0.7383966244725738, 'f1': 0.7337526205450734, 'number': 474}</td>\n",
       "      <td>{'precision': 0.5894206549118388, 'recall': 0.6428571428571429, 'f1': 0.6149802890932983, 'number': 364}</td>\n",
       "      <td>{'precision': 0.6908212560386473, 'recall': 0.6761229314420804, 'f1': 0.6833930704898447, 'number': 423}</td>\n",
       "      <td>{'precision': 0.5605381165919282, 'recall': 0.5707762557077626, 'f1': 0.5656108597285068, 'number': 219}</td>\n",
       "      <td>0.606289</td>\n",
       "      <td>0.623948</td>\n",
       "      <td>0.614992</td>\n",
       "      <td>0.908035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.320696</td>\n",
       "      <td>{'precision': 0.47858942065491183, 'recall': 0.510752688172043, 'f1': 0.494148244473342, 'number': 372}</td>\n",
       "      <td>{'precision': 0.5515151515151515, 'recall': 0.5870967741935483, 'f1': 0.5687499999999999, 'number': 155}</td>\n",
       "      <td>{'precision': 0.5995260663507109, 'recall': 0.6293532338308457, 'f1': 0.6140776699029127, 'number': 402}</td>\n",
       "      <td>{'precision': 0.45, 'recall': 0.4632352941176471, 'f1': 0.45652173913043476, 'number': 272}</td>\n",
       "      <td>{'precision': 0.6321428571428571, 'recall': 0.7224489795918367, 'f1': 0.6742857142857143, 'number': 245}</td>\n",
       "      <td>{'precision': 0.6838709677419355, 'recall': 0.6463414634146342, 'f1': 0.664576802507837, 'number': 164}</td>\n",
       "      <td>{'precision': 0.7151639344262295, 'recall': 0.7362869198312236, 'f1': 0.7255717255717256, 'number': 474}</td>\n",
       "      <td>{'precision': 0.5788177339901478, 'recall': 0.6456043956043956, 'f1': 0.6103896103896104, 'number': 364}</td>\n",
       "      <td>{'precision': 0.6893424036281179, 'recall': 0.7186761229314421, 'f1': 0.7037037037037037, 'number': 423}</td>\n",
       "      <td>{'precision': 0.5502183406113537, 'recall': 0.5753424657534246, 'f1': 0.5625, 'number': 219}</td>\n",
       "      <td>0.599755</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.616087</td>\n",
       "      <td>0.909766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.4251968503937008, 'recall': 0.5806451612903226, 'f1': 0.4909090909090909, 'number': 372}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5957446808510638, 'recall': 0.5419354838709678, 'f1': 0.5675675675675675, 'number': 155}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.585, 'recall': 0.582089552238806, 'f1': 0.5835411471321695, 'number': 402}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.37800687285223367, 'recall': 0.40441176470588236, 'f1': 0.3907637655417407, 'number': 272}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5972696245733788, 'recall': 0.7142857142857143, 'f1': 0.650557620817844, 'number': 245}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6832298136645962, 'recall': 0.6707317073170732, 'f1': 0.676923076923077, 'number': 164}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7707865168539326, 'recall': 0.7236286919831224, 'f1': 0.7464635473340587, 'number': 474}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5844504021447721, 'recall': 0.5989010989010989, 'f1': 0.5915875169606511, 'number': 364}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6846153846153846, 'recall': 0.6312056737588653, 'f1': 0.6568265682656826, 'number': 423}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6091954022988506, 'recall': 0.4840182648401826, 'f1': 0.5394402035623409, 'number': 219}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.4859335038363171, 'recall': 0.510752688172043, 'f1': 0.4980340760157274, 'number': 372}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5786163522012578, 'recall': 0.5935483870967742, 'f1': 0.5859872611464967, 'number': 155}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6177884615384616, 'recall': 0.6393034825870647, 'f1': 0.6283618581907091, 'number': 402}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.41114982578397213, 'recall': 0.4338235294117647, 'f1': 0.42218246869409665, 'number': 272}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6485507246376812, 'recall': 0.7306122448979592, 'f1': 0.6871401151631479, 'number': 245}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.708029197080292, 'recall': 0.5914634146341463, 'f1': 0.6445182724252492, 'number': 164}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7291666666666666, 'recall': 0.7383966244725738, 'f1': 0.7337526205450734, 'number': 474}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5894206549118388, 'recall': 0.6428571428571429, 'f1': 0.6149802890932983, 'number': 364}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6908212560386473, 'recall': 0.6761229314420804, 'f1': 0.6833930704898447, 'number': 423}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5605381165919282, 'recall': 0.5707762557077626, 'f1': 0.5656108597285068, 'number': 219}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'precision': 0.47858942065491183, 'recall': 0.510752688172043, 'f1': 0.494148244473342, 'number': 372}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5515151515151515, 'recall': 0.5870967741935483, 'f1': 0.5687499999999999, 'number': 155}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5995260663507109, 'recall': 0.6293532338308457, 'f1': 0.6140776699029127, 'number': 402}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.45, 'recall': 0.4632352941176471, 'f1': 0.45652173913043476, 'number': 272}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6321428571428571, 'recall': 0.7224489795918367, 'f1': 0.6742857142857143, 'number': 245}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6838709677419355, 'recall': 0.6463414634146342, 'f1': 0.664576802507837, 'number': 164}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7151639344262295, 'recall': 0.7362869198312236, 'f1': 0.7255717255717256, 'number': 474}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5788177339901478, 'recall': 0.6456043956043956, 'f1': 0.6103896103896104, 'number': 364}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6893424036281179, 'recall': 0.7186761229314421, 'f1': 0.7037037037037037, 'number': 423}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5502183406113537, 'recall': 0.5753424657534246, 'f1': 0.5625, 'number': 219}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=504, training_loss=0.1891273157227607, metrics={'train_runtime': 211.1088, 'train_samples_per_second': 152.736, 'train_steps_per_second': 2.387, 'total_flos': 820469833815600.0, 'train_loss': 0.1891273157227607, 'epoch': 3.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e2eba-f16e-4b47-bfc6-c7520f48ccac",
   "metadata": {},
   "source": [
    "#### 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d290c2e1-466e-471d-b7fe-c401450dda42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:24:19.931646Z",
     "iopub.status.busy": "2025-06-03T13:24:19.930977Z",
     "iopub.status.idle": "2025-06-03T13:24:23.536910Z",
     "shell.execute_reply": "2025-06-03T13:24:23.536331Z",
     "shell.execute_reply.started": "2025-06-03T13:24:19.931604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.predict(ds2['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec9ced6f-f4ef-4d53-8157-4df4026a3b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:24:33.555592Z",
     "iopub.status.busy": "2025-06-03T13:24:33.554969Z",
     "iopub.status.idle": "2025-06-03T13:24:33.563069Z",
     "shell.execute_reply": "2025-06-03T13:24:33.562238Z",
     "shell.execute_reply.started": "2025-06-03T13:24:33.555569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[[ 1.16269815e+00,  3.01405728e-01, -3.46002847e-01, ...,\n",
       "         -2.03423882e+00, -8.17227066e-01, -1.54515016e+00],\n",
       "        [ 1.93018639e+00, -1.13778794e+00,  5.87029994e-01, ...,\n",
       "         -3.47192168e-01, -1.24619222e+00, -4.27877307e-01],\n",
       "        [ 2.18126941e+00, -1.36515689e+00,  5.32690763e-01, ...,\n",
       "         -4.56831336e-01, -1.27404177e+00, -4.77858573e-01],\n",
       "        ...,\n",
       "        [ 1.14714851e+01, -1.34443128e+00, -8.45166862e-01, ...,\n",
       "         -9.28908408e-01, -1.24595869e+00, -1.49832118e+00],\n",
       "        [ 1.15317984e+01, -1.41182327e+00, -9.25910592e-01, ...,\n",
       "         -8.41526985e-01, -1.17196071e+00, -1.39698303e+00],\n",
       "        [ 1.13740597e+01, -1.31516242e+00, -1.07594752e+00, ...,\n",
       "         -9.01089251e-01, -1.13462996e+00, -1.51564324e+00]],\n",
       "\n",
       "       [[ 3.79712415e+00, -7.27360606e-01, -1.17418075e+00, ...,\n",
       "         -1.40646279e+00, -1.42037654e+00, -2.13151813e+00],\n",
       "        [ 4.21313572e+00, -2.13888931e+00, -2.38957942e-01, ...,\n",
       "          1.37498474e+00, -2.11680460e+00, -2.71993965e-01],\n",
       "        [ 1.10370369e+01, -2.28468728e+00, -1.03881979e+00, ...,\n",
       "          1.33472073e+00, -1.24514997e+00, -1.70677707e-01],\n",
       "        ...,\n",
       "        [ 1.15011883e+01, -1.56867075e+00, -1.11223578e+00, ...,\n",
       "          3.23346525e-01, -1.08890915e+00, -8.61687958e-01],\n",
       "        [ 1.15757618e+01, -1.58814240e+00, -1.07367051e+00, ...,\n",
       "          2.54935861e-01, -1.07227302e+00, -8.31791520e-01],\n",
       "        [ 1.16636086e+01, -1.51622343e+00, -8.99926245e-01, ...,\n",
       "          8.59665573e-02, -1.12499130e+00, -8.24264467e-01]],\n",
       "\n",
       "       [[ 9.08833966e-02,  3.01881409e+00,  1.01581976e-01, ...,\n",
       "         -1.63449848e+00, -6.44312441e-01, -7.11387992e-01],\n",
       "        [ 4.79919501e-02, -2.58023143e-01,  3.83242369e+00, ...,\n",
       "         -7.96897471e-01, -1.67469251e+00, -8.85879517e-01],\n",
       "        [-7.30255619e-02, -6.75436497e-01,  4.09708786e+00, ...,\n",
       "         -8.31181705e-01, -1.60165048e+00, -8.65136385e-01],\n",
       "        ...,\n",
       "        [-1.49505630e-01, -6.92480147e-01,  4.07696915e+00, ...,\n",
       "         -7.80514538e-01, -1.50923264e+00, -8.63424182e-01],\n",
       "        [-1.18278392e-01, -7.64568269e-01,  4.10367489e+00, ...,\n",
       "         -7.68538952e-01, -1.49356687e+00, -8.63421679e-01],\n",
       "        [-4.30339091e-02, -7.26728439e-01,  4.06129980e+00, ...,\n",
       "         -7.55376935e-01, -1.47184384e+00, -9.07212436e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.11234407e+01, -1.39320278e+00, -7.97840714e-01, ...,\n",
       "         -5.90020657e-01, -8.75529408e-01, -5.27743399e-01],\n",
       "        [ 1.12049770e+01, -1.41260099e+00, -9.82769370e-01, ...,\n",
       "         -4.56000358e-01, -8.45198512e-01, -3.49952519e-01],\n",
       "        [ 1.11000509e+01, -1.23929763e+00, -1.05932903e+00, ...,\n",
       "         -5.90669155e-01, -6.27693772e-01, -4.67605293e-01],\n",
       "        ...,\n",
       "        [ 1.09190397e+01, -1.24507320e+00, -1.06302488e+00, ...,\n",
       "         -7.03143418e-01, -4.52307075e-01, -3.84752810e-01],\n",
       "        [ 2.60956526e+00, -9.27570164e-01, -8.25258613e-01, ...,\n",
       "         -1.70234787e+00,  8.11658001e+00,  2.33510804e+00],\n",
       "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
       "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n",
       "\n",
       "       [[ 1.16124172e+01, -1.25210261e+00, -9.53989804e-01, ...,\n",
       "         -5.33194780e-01, -1.18425453e+00, -1.19536233e+00],\n",
       "        [ 1.15796862e+01, -1.30328441e+00, -9.44723248e-01, ...,\n",
       "         -5.28874099e-01, -1.24945128e+00, -1.17269373e+00],\n",
       "        [ 1.16038933e+01, -1.35125995e+00, -1.00385475e+00, ...,\n",
       "         -5.11278927e-01, -1.15222383e+00, -1.10973275e+00],\n",
       "        ...,\n",
       "        [ 1.14765549e+01, -1.24964094e+00, -1.00740623e+00, ...,\n",
       "         -6.01667702e-01, -1.30192876e+00, -1.37801111e+00],\n",
       "        [ 1.14295263e+01, -1.24346817e+00, -1.14333773e+00, ...,\n",
       "         -6.27361655e-01, -1.30727518e+00, -1.37548780e+00],\n",
       "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
       "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n",
       "\n",
       "       [[ 9.92918491e+00, -2.00068498e+00, -1.49913073e+00, ...,\n",
       "         -5.17313004e-01, -4.14191037e-01, -3.38989019e-01],\n",
       "        [ 9.95521832e+00, -2.17118549e+00, -1.88257766e+00, ...,\n",
       "         -1.36582449e-01, -7.13332772e-01, -1.41829744e-01],\n",
       "        [ 6.75489521e+00, -1.34435987e+00, -1.87003922e+00, ...,\n",
       "         -6.85838938e-01, -7.37698972e-02, -9.65986729e-01],\n",
       "        ...,\n",
       "        [ 6.77306318e+00, -1.19057250e+00, -1.85090852e+00, ...,\n",
       "         -9.57795620e-01, -1.42583877e-01, -1.10208905e+00],\n",
       "        [ 7.06290722e+00, -1.98905194e+00, -1.31216002e+00, ...,\n",
       "          1.19134450e+00, -1.38274705e+00,  3.53419602e-01],\n",
       "        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n",
       "         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]]],\n",
       "      dtype=float32), label_ids=array([[  11,   12,   12, ..., -100, -100, -100],\n",
       "       [  11,   12,    0, ..., -100, -100, -100],\n",
       "       [  13,   14,   14, ..., -100, -100, -100],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., -100, -100, -100],\n",
       "       [   0,    0,    0, ..., -100, -100, -100],\n",
       "       [   0,    0,    0, ..., -100, -100, -100]]), metrics={'test_loss': 0.3206961750984192, 'test_ADDRESS': {'precision': 0.47858942065491183, 'recall': 0.510752688172043, 'f1': 0.494148244473342, 'number': 372}, 'test_BOOK': {'precision': 0.5515151515151515, 'recall': 0.5870967741935483, 'f1': 0.5687499999999999, 'number': 155}, 'test_COMPANY': {'precision': 0.5995260663507109, 'recall': 0.6293532338308457, 'f1': 0.6140776699029127, 'number': 402}, 'test_GAME': {'precision': 0.45, 'recall': 0.4632352941176471, 'f1': 0.45652173913043476, 'number': 272}, 'test_GOVERNMENT': {'precision': 0.6321428571428571, 'recall': 0.7224489795918367, 'f1': 0.6742857142857143, 'number': 245}, 'test_MOVIE': {'precision': 0.6838709677419355, 'recall': 0.6463414634146342, 'f1': 0.664576802507837, 'number': 164}, 'test_NAME': {'precision': 0.7151639344262295, 'recall': 0.7362869198312236, 'f1': 0.7255717255717256, 'number': 474}, 'test_ORGANIZATION': {'precision': 0.5788177339901478, 'recall': 0.6456043956043956, 'f1': 0.6103896103896104, 'number': 364}, 'test_POSITION': {'precision': 0.6893424036281179, 'recall': 0.7186761229314421, 'f1': 0.7037037037037037, 'number': 423}, 'test_SCENE': {'precision': 0.5502183406113537, 'recall': 0.5753424657534246, 'f1': 0.5625, 'number': 219}, 'test_overall_precision': 0.5997548268464603, 'test_overall_recall': 0.6333333333333333, 'test_overall_f1': 0.6160868880843696, 'test_overall_accuracy': 0.909765502866076, 'test_runtime': 3.586, 'test_samples_per_second': 374.508, 'test_steps_per_second': 5.856})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71d2e38b-fd0a-4150-8ce8-4afb6c55e3ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:26:02.062494Z",
     "iopub.status.busy": "2025-06-03T13:26:02.061765Z",
     "iopub.status.idle": "2025-06-03T13:26:02.070072Z",
     "shell.execute_reply": "2025-06-03T13:26:02.069361Z",
     "shell.execute_reply.started": "2025-06-03T13:26:02.062470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价格高昂的大钻和翡翠消费为何如此火？通灵珠宝总裁沈东军认为，这与原料稀缺有直接关系。“\n",
      "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        15, 16, 16, 16,  7,  8, 11, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0])\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0   15   16   16   16    7    8   11   12   12    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0 -100 -100 -100 -100 -100 -100 -100]\n"
     ]
    }
   ],
   "source": [
    "print(ds1['validation'][13]['text'])\n",
    "print(ds2['validation'][13]['labels'])\n",
    "print(result.label_ids[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0989602c-c067-49fd-9a76-e9fd808a1049",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-COMPANY',\n",
       " 'I-COMPANY',\n",
       " 'I-COMPANY',\n",
       " 'I-COMPANY',\n",
       " 'B-POSITION',\n",
       " 'I-POSITION',\n",
       " 'B-NAME',\n",
       " 'I-NAME',\n",
       " 'I-NAME',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tags[p] for p,l in zip(result.label_ids[13],ds2['validation'][13]['labels'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer1 = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=ds_orignal['train'],\n",
    "#     eval_dataset=ds_orignal['test'],\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metric\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "study-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
