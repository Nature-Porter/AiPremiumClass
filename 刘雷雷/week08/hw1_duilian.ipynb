{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 使用中文对联数据集训练带有attention的seq2seq模型，利用tensorboard跟踪。\n",
    "# https://www.kaggle.com/datasets/jiaminggogogo/chinese-couplets\n",
    "# 2. 尝试encoder hidden state不同的返回形式（concat和add）\n",
    "# 3. 编写并实现seq2seq attention版的推理实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------process--------------------\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "current_dir = f\"/kaggle/\"\n",
    "\n",
    "\n",
    "def read_data(in_file, out_file):\n",
    "    \"\"\"读取训练数据返回数据集合\"\"\"\n",
    "    enc_data, dec_data = [], []\n",
    "\n",
    "    in_ = open(in_file, encoding=\"utf-8\")\n",
    "    out_ = open(out_file, encoding=\"utf-8\")\n",
    "\n",
    "    for enc, dec in zip(in_, out_):\n",
    "        # 分词\n",
    "        enc_tks = enc.strip()\n",
    "        dec_tks = dec.strip()\n",
    "        # 保存\n",
    "        enc_data.append(enc_tks)\n",
    "        dec_data.append(dec_tks)\n",
    "\n",
    "    assert len(enc_data) == len(dec_data), \"输入输出数据长度不一致\"\n",
    "    return enc_data, dec_data\n",
    "\n",
    "\n",
    "def get_proc(vocab):\n",
    "\n",
    "    # 嵌套函数定义\n",
    "    # 外部函数变量生命周期会延续到内部函数调用结束 （闭包）\n",
    "\n",
    "    def batch_proc(data):\n",
    "        \"\"\"\n",
    "        批次数据处理并返回\n",
    "        \"\"\"\n",
    "        enc_ids, dec_ids, labels = [], [], []\n",
    "        for enc, dec in data:\n",
    "            # token -> token index 首尾添加起始和结束token\n",
    "            enc_idx = [vocab[\"<s>\"]] + [vocab[tk] for tk in enc] + [vocab[\"</s>\"]]\n",
    "            dec_idx = [vocab[\"<s>\"]] + [vocab[tk] for tk in dec] + [vocab[\"</s>\"]]\n",
    "\n",
    "            # encoder_input\n",
    "            enc_ids.append(torch.tensor(enc_idx))\n",
    "            # decoder_input\n",
    "            dec_ids.append(torch.tensor(dec_idx[:-1]))\n",
    "            # label\n",
    "            labels.append(torch.tensor(dec_idx[1:]))\n",
    "\n",
    "        # 数据转换张量 [batch, max_token_len]\n",
    "        # 用批次中最长token序列构建张量\n",
    "        enc_input = pad_sequence(enc_ids, batch_first=True)\n",
    "        dec_input = pad_sequence(dec_ids, batch_first=True)\n",
    "        targets = pad_sequence(labels, batch_first=True)\n",
    "\n",
    "        # 返回数据都是模型训练和推理的需要\n",
    "        return enc_input, dec_input, targets\n",
    "\n",
    "    # 返回回调函数\n",
    "    return batch_proc\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        # 添加未知词处理\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.unk_index = vocab.get(self.unk_token, len(vocab))\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, vocab_file):\n",
    "        with open(vocab_file, encoding=\"utf-8\") as f:\n",
    "            words = [tk.strip() for tk in f if tk.strip()]\n",
    "\n",
    "        # 确保包含所有必需的特殊标记\n",
    "        special_tokens = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
    "        for token in special_tokens:\n",
    "            if token not in words:\n",
    "                words.insert(0, token)\n",
    "\n",
    "        vocab_dict = {word: idx for idx, word in enumerate(words)}\n",
    "        return cls(vocab_dict)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        \"\"\"安全获取token索引，未知词返回<unk>索引\"\"\"\n",
    "        return self.vocab.get(token, self.unk_index)\n",
    "\n",
    "\n",
    "# ---------测试-------\n",
    "# 加载词典\n",
    "vocab_file = f\"{current_dir}/input/chinese-couplets/couplet/vocabs\"\n",
    "vocab = Vocabulary.from_file(vocab_file)\n",
    "# 读取数据\n",
    "enc_train_file = f\"{current_dir}/input/chinese-couplets/couplet/train/in.txt\"\n",
    "dec_train_file = f\"{current_dir}/input/chinese-couplets/couplet/train/out.txt\"\n",
    "enc_data, dec_data = read_data(enc_train_file, dec_train_file)\n",
    "print(f\"enc len is :{len(enc_data)}\")\n",
    "print(f\"dec len is :{len(dec_data)}\")\n",
    "print(f\"词汇数量: {len(vocab.vocab)}\")\n",
    "# 编码+解码\n",
    "dataset = list(zip(enc_data, dec_data))\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,  # 批量大小\n",
    "    shuffle=True,  # 是否打乱数据\n",
    "    collate_fn=get_proc(vocab),\n",
    ")\n",
    "# 处理缓存为json\n",
    "with open(f\"{current_dir}/working/encoder.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enc_data, f)\n",
    "with open(f\"{current_dir}/working/decoder.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dec_data, f)\n",
    "\n",
    "# # 加载\n",
    "# with open(\"encoder.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for enc in enc_data:\n",
    "#         str_json = json.dumps(enc)\n",
    "#         f.write(str_json + \"\\n\")\n",
    "print(\"数据加载和处理完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------EncoderDecoderAttentionModel--------------------\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dim, emb_dim, hidden_dim, num_layers=2, state_type=\"concat\"\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # 定义嵌入层\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        # 定义gru层\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # 自定义返回state类型 [concat add ]\n",
    "        self.state_type = state_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, token_seq):\n",
    "        # token_seq: [batch_size, seq_len ]\n",
    "        # embedded : [batch_size, seq_len, emb_dim ]\n",
    "        embedded = self.embedding(token_seq)\n",
    "\n",
    "        # outputs: [batch_size, seq_len, hidden_dim * 2]\n",
    "        # hidden : [2, batch_size, hidden_dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        if self.state_type == \"concat\":\n",
    "            hidden = outputs[:, -1, :]\n",
    "        elif self.state_dict == \"add\":\n",
    "            hidden = torch.sum(hidden, dim=0)\n",
    "            outputs = outputs[..., : self.hidden_dim] + outputs[..., self.hidden_dim :]\n",
    "        else:\n",
    "            raise ValueError(\"state_type must be 'concat' or 'add'\")\n",
    "\n",
    "        return hidden, outputs\n",
    "\n",
    "\n",
    "# Decoder Attention 机制\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, enc_output, dec_output):\n",
    "        # a_t = ht @ h_s\n",
    "        a_t = torch.bmm(enc_output, dec_output.permute(0, 2, 1))\n",
    "\n",
    "        # 1. 计算 结合解码token 和编码 token 关联权重\n",
    "        a_t = torch.softmax(a_t, dim=-1)\n",
    "\n",
    "        # 2. 计算 关联权重和编码token的 贡献值\n",
    "        c_t = torch.bmm(a_t.permute(0, 2, 1), enc_output)\n",
    "\n",
    "        return c_t\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, emb_dim, hidden_dim, dropout=0.5, state_type=\"concat\"\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        if state_type == \"concat\":\n",
    "            hidden_dim = hidden_dim * 2  # 双向GRU的hidden_dim需要乘以2\n",
    "\n",
    "        # 定义嵌入层\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        # 定义gru层\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # 定义全连接层\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        # 定义注意力层\n",
    "        self.attention = Attention()\n",
    "\n",
    "        # attention结果转换线性层\n",
    "        self.attention_fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        # dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, token_seq, hidden_state, enc_output):\n",
    "        # token_seq: [batch_size, seq_len ]\n",
    "        # embedded : [batch_size, seq_len, emb_dim ]\n",
    "        embedded = self.embedding(token_seq)\n",
    "\n",
    "        # outputs: [batch_size, seq_len, hidden_dim]\n",
    "        # hidden : [1, batch_size, hidden_dim]\n",
    "        dec_output, hidden = self.rnn(embedded, hidden_state.unsqueeze(0))\n",
    "\n",
    "        # 计算注意力\n",
    "        c_t = self.attention(enc_output, dec_output)\n",
    "        cat_output = torch.cat((c_t, dec_output), dim=-1)\n",
    "        # 通过注意力全连接层\n",
    "        out = torch.tanh(self.attention_fc(cat_output))\n",
    "        # dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # out : [batch_size, seq_len, hidden_dim*2 ]\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "\n",
    "# Seq2Seq Attention Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        enc_emb_size,\n",
    "        dec_emb_size,\n",
    "        emb_dim,\n",
    "        hidden_size,\n",
    "        dropout=0.5,\n",
    "        state_type=\"concat\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 定义编码器\n",
    "        self.encoder = Encoder(\n",
    "            enc_emb_size, emb_dim, hidden_size, state_type=state_type\n",
    "        )\n",
    "\n",
    "        # 定义解码器\n",
    "        self.decoder = Decoder(\n",
    "            dec_emb_size, emb_dim, hidden_size, dropout, state_type=state_type\n",
    "        )\n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "        # 编码器输出\n",
    "        enc_hidden_state, enc_output = self.encoder(enc_input)\n",
    "\n",
    "        # 解码器输出\n",
    "        dec_output, hidden = self.decoder(dec_input, enc_hidden_state, enc_output)\n",
    "\n",
    "        return dec_output, hidden\n",
    "\n",
    "\n",
    "# --------------------测试--------------------\n",
    "input_dim = 200  # 编码器词汇大小\n",
    "emb_dim = 256  # 嵌入维度\n",
    "hidden_dim = 256  # 隐藏层大小\n",
    "dropout = 0.5  # dropout比率\n",
    "batch_size = 5  # 批量大小\n",
    "seq_len = 10  # 序列长度\n",
    "\n",
    "# 测试 encoder\n",
    "encoder = Encoder(input_dim, emb_dim, hidden_dim, state_type=\"concat\")\n",
    "token_seq = torch.randint(0, input_dim, (batch_size, seq_len))  # 随机生成token序列\n",
    "hidden_state = encoder(token_seq)\n",
    "print(\"Encoder output shape:\", hidden_state[0].shape)  # 输出隐藏状态形状\n",
    "print(\"Encoder output shape:\", hidden_state[1].shape)  # 输出编码器输出形状\n",
    "\n",
    "# 测试 decoder\n",
    "decoder = Decoder(input_dim, emb_dim, hidden_dim, dropout, state_type=\"concat\")\n",
    "token_seq = torch.randint(0, input_dim, (batch_size, seq_len))  # 随机生成token序列\n",
    "logits, hidden = decoder(token_seq, hidden_state[0], hidden_state[1])\n",
    "print(\"Decoder logits shape:\", logits.shape)  # 输出解码器输出形状\n",
    "print(\"Decoder hidden state shape:\", hidden.shape)  # 输出解码器隐藏状态形状\n",
    "\n",
    "# 测试 Seq2Seq 模型\n",
    "seq2seq = Seq2Seq(\n",
    "    enc_emb_size=input_dim,\n",
    "    dec_emb_size=input_dim,\n",
    "    emb_dim=emb_dim,\n",
    "    hidden_size=hidden_dim,\n",
    "    dropout=dropout,\n",
    ")\n",
    "logits, hidden = seq2seq(\n",
    "    enc_input=torch.randint(0, input_dim, (batch_size, seq_len)),\n",
    "    dec_input=torch.randint(0, input_dim, (batch_size, seq_len)),\n",
    ")\n",
    "print(\"Seq2Seq logits shape:\", logits.shape)  # 输出Seq2Seq模型输出形状\n",
    "print(\"Seq2Seq hidden state shape:\", hidden.shape)  # 输出Seq2Seq隐藏状态形状\n",
    "# 输出Seq2Seq编码器输出形状\n",
    "print(\"Seq2Seq encoder output shape:\", hidden[0].shape)  # 输出Seq2Seq编码器输出形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------seq2seq_train--------------------\n",
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# 测试\n",
    "writer = SummaryWriter(log_dir=os.path.join(current_dir, \"logs\"))\n",
    "train_loss_cnt = 0.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 1.加载数据\n",
    "vocab_file = f\"{current_dir}/input/chinese-couplets/couplett/vocabs\"\n",
    "vocab = Vocabulary.from_file(vocab_file)\n",
    "\n",
    "with open(f\"{current_dir}/working/encoder.json\") as f:\n",
    "    enc_data = json.load(f)\n",
    "with open(f\"{current_dir}/working/decoder.json\") as f:\n",
    "    dec_data = json.load(f)\n",
    "\n",
    "ds = list(zip(enc_data, dec_data))\n",
    "dl = DataLoader(ds, batch_size=256, shuffle=True, collate_fn=get_proc(vocab))\n",
    "# 构建训练模型\n",
    "model = Seq2Seq(\n",
    "    enc_emb_size=len(vocab.vocab),\n",
    "    dec_emb_size=len(vocab.vocab),\n",
    "    emb_dim=200,\n",
    "    hidden_size=250,\n",
    "    dropout=0.5,\n",
    "    state_type=\"concat\",\n",
    ").to(device)\n",
    "# 优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 2.训练模型\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    tpbar = tqdm(dl)\n",
    "    for enc_input, dec_input, targets in tpbar:\n",
    "        enc_input = enc_input.to(device)\n",
    "        dec_input = dec_input.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # 前向传播\n",
    "        logits, _ = model(enc_input, dec_input)\n",
    "        # 计算损失\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 更新进度条\n",
    "        tpbar.set_description(f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}\")\n",
    "        # TensorBoard记录\n",
    "        writer.add_scalar(\"Loss/train_step\", loss.item(), train_loss_cnt)\n",
    "        train_loss_cnt += loss.item()\n",
    "# 3.保存模型和词典\n",
    "torch.save(model.state_dict(), f\"{current_dir}/working/seq2seq_concat_state.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------seq2seq_infer--------------------\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "enc_test_file = f\"{current_dir}/input/chinese-couplets/couplet/test/in.txt\"\n",
    "dec_test_file = f\"{current_dir}/input/chinese-couplets/couplet/test/out.txt\"\n",
    "enc_data, dec_data = read_data(enc_test_file, dec_test_file)\n",
    "# 加载训练好的模型和词典\n",
    "state_dict = torch.load(f\"{current_dir}/working/seq2seq_concat_state.bin\")\n",
    "vocab_file = f\"{current_dir}/input/chinese-couplets/couplett/vocabs\"\n",
    "\n",
    "vocab = Vocabulary.from_file(vocab_file)\n",
    "model = Seq2Seq(\n",
    "    enc_emb_size=len(vocab.vocab),\n",
    "    dec_emb_size=len(vocab.vocab),\n",
    "    emb_dim=200,\n",
    "    hidden_size=250,\n",
    "    dropout=0.5,\n",
    "    state_type=\"concat\",\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "# 创建解码器反向字典\n",
    "dvoc_inv = {v: k for k, v in vocab.vocab.items()}\n",
    "# 随机选取测试样本\n",
    "rnd_idx = random.randint(0, len(enc_data) - 1)\n",
    "enc_input = enc_data[rnd_idx]\n",
    "dec_output = dec_data[rnd_idx]\n",
    "enc_idx = torch.tensor([[vocab.vocab[tk] for tk in enc_input]])\n",
    "print(enc_idx.shape)\n",
    "# 推理\n",
    "max_dec_len = len(enc_input)\n",
    "model.eval()  # 设置模型为评估模式\n",
    "with torch.no_grad():\n",
    "    # 编码器输出\n",
    "    hidden_state, enc_outputs = model.encoder(enc_idx)\n",
    "    # 初始化解码器输入\n",
    "    dec_input = torch.tensor([[vocab[\"<s>\"]]])  # <s> token作为开始符\n",
    "    # 循环decoder\n",
    "    dec_tokens = []\n",
    "    while len(dec_tokens) < max_dec_len:\n",
    "        # 解码器输出\n",
    "        logits, hidden_state = model.decoder(dec_input, hidden_state, enc_outputs)\n",
    "        # 下个 token index\n",
    "        next_token = logits.argmax(dim=-1)  # 获取最大概率的token index\n",
    "        if dvoc_inv[next_token.squeeze().item()] == \"</s>\":\n",
    "            break\n",
    "        # 更新解码器输入\n",
    "        dec_tokens.append(next_token.squeeze().item())  # 添加到解码结果中\n",
    "        # decoder 的下一个输入 = token_index\n",
    "        dec_input = next_token\n",
    "        hidden_state = hidden_state.view(1, -1)  # 确保hidden_state形状正确\n",
    "# 输出解码结果\n",
    "print(f\"上联：\", \"\".join(enc_input))\n",
    "print(\"模型预测下联：\", \"\".join([dvoc_inv[tk] for tk in dec_tokens]))\n",
    "print(\"真实下联：\", \"\".join(dec_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
