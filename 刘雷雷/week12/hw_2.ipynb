{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 利用课堂案例，实现分布式DDP模型训练。存盘后加载实现推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (G:\\Hmsoft\\Anaconda\\package\\envs\\py312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (G:\\Hmsoft\\Anaconda\\package\\envs\\py312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (G:\\Hmsoft\\Anaconda\\package\\envs\\py312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification,AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate\n",
    "import seqeval\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-chinese\",num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[[-0.4554, -0.3599,  0.2053, -0.8422,  0.1384,  0.4004,  0.3453],\n",
      "         [-0.0770, -0.1642,  0.1621, -1.4575,  0.0242,  0.0445,  0.3897],\n",
      "         [-0.5516, -0.4103, -0.3800, -1.0367,  0.2667, -0.0371, -0.0798],\n",
      "         [ 0.2075,  0.0971,  0.3932, -1.4048, -0.0305,  0.1208,  0.1933],\n",
      "         [-0.3234, -0.2060,  0.1865, -1.3074, -0.0425, -0.0970,  0.3460],\n",
      "         [-0.3628,  0.0504,  0.3074, -1.0486, -0.0066,  0.3090, -0.2814],\n",
      "         [-0.4035, -0.0642, -0.1623, -0.9808,  0.3029, -0.1835, -0.0783],\n",
      "         [-0.9058, -0.6511, -0.1494, -0.6673,  0.3933,  0.8644, -0.0998]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 8, 7])\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "message = \"命名实体识别\"\n",
    "label = torch.tensor([0, 1, 0, 2, 5, 4])\n",
    "\n",
    "model_input = tokenizer([message], return_tensors=\"pt\")\n",
    "result = model(**model_input)\n",
    "print(result.loss)\n",
    "print(result.logits)\n",
    "print(result.logits.shape)  # (batch_size, sequence_length, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'nlhappy/CLUE-NER' on the Hub (LocalEntryNotFoundError)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 加载 hf中的dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlhappy/CLUE-NER\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m ds\n",
      "File \u001b[1;32mg:\\Hmsoft\\Anaconda\\package\\envs\\py312\\Lib\\site-packages\\datasets\\load.py:2062\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2057\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2058\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2059\u001b[0m )\n\u001b[0;32m   2061\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2062\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2063\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2064\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2065\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2066\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2067\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2068\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2069\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2070\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2071\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2072\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2073\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2074\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2075\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2076\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2077\u001b[0m )\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mg:\\Hmsoft\\Anaconda\\package\\envs\\py312\\Lib\\site-packages\\datasets\\load.py:1782\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1781\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 1782\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[0;32m   1783\u001b[0m     path,\n\u001b[0;32m   1784\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1785\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1786\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1787\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1788\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1789\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1790\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   1791\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39m_require_default_config_name,\n\u001b[0;32m   1792\u001b[0m     _require_custom_configs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(config_kwargs),\n\u001b[0;32m   1793\u001b[0m )\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mg:\\Hmsoft\\Anaconda\\package\\envs\\py312\\Lib\\site-packages\\datasets\\load.py:1664\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1660\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1661\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1662\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1663\u001b[0m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1664\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trust_remote_code:\n\u001b[0;32m   1666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1667\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1668\u001b[0m     )\n",
      "File \u001b[1;32mg:\\Hmsoft\\Anaconda\\package\\envs\\py312\\Lib\\site-packages\\datasets\\load.py:1551\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LocalEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1544\u001b[0m         e\u001b[38;5;241m.\u001b[39m__cause__,\n\u001b[0;32m   1545\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1549\u001b[0m         ),\n\u001b[0;32m   1550\u001b[0m     ):\n\u001b[1;32m-> 1551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: Couldn't reach 'nlhappy/CLUE-NER' on the Hub (LocalEntryNotFoundError)"
     ]
    }
   ],
   "source": [
    "# 加载 hf中的dataset\n",
    "ds = load_dataset('nlhappy/CLUE-NER')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实体映射数据集词典准备\n",
    "entities = [\"O\"] + list(\n",
    "    {\n",
    "        \"movie\",\n",
    "        \"name\",\n",
    "        \"game\",\n",
    "        \"address\",\n",
    "        \"position\",\n",
    "        \"company\",\n",
    "        \"scene\",\n",
    "        \"book\",\n",
    "        \"organization\",\n",
    "        \"government\",\n",
    "    }\n",
    ")\n",
    "tags = [\"O\"]\n",
    "\n",
    "for entity in entities:\n",
    "    tags.append(\"B-\" + entity.upper())\n",
    "    tags.append(\"I-\" + entity.upper())\n",
    "\n",
    "entity_index = {entity: i for i, entity in enumerate(entities)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实体映射数据集词典准备\n",
    "entities = [\"O\"] + list(\n",
    "    {\n",
    "        \"movie\",\n",
    "        \"name\",\n",
    "        \"game\",\n",
    "        \"address\",\n",
    "        \"position\",\n",
    "        \"company\",\n",
    "        \"scene\",\n",
    "        \"book\",\n",
    "        \"organization\",\n",
    "        \"government\",\n",
    "    }\n",
    ")\n",
    "tags = [\"O\"]\n",
    "\n",
    "for entity in entities:\n",
    "    tags.append(\"B-\" + entity.upper())\n",
    "    tags.append(\"I-\" + entity.upper())\n",
    "\n",
    "entity_index = {entity: i for i, entity in enumerate(entities)}\n",
    "\n",
    "\n",
    "def entity_tags_proc(item):\n",
    "    # item 即是dataset中记录\n",
    "    text_len = len(item[\"text\"])\n",
    "    tags = [0] * text_len\n",
    "\n",
    "    # 遍历实体列表,所有实体类别标记填入tags\n",
    "    entities = item[\"ents\"]\n",
    "    for ent in entities:\n",
    "        indices = ent[\"indices\"]\n",
    "        label = ent[\"label\"]\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "\n",
    "    return {\"ent_tag\": tags}\n",
    "\n",
    "\n",
    "ds1 = ds.map(entity_tags_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index = tokenizer.encode(\"2000年2月add\", add_special_tokens=False)\n",
    "print(token_index)\n",
    "tokens = tokenizer.decode(token_index)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tokenizer(\n",
    "    [list(\"2000年2月add\")],\n",
    "    add_special_tokens=False,\n",
    "    truncation=True,\n",
    "    is_split_into_words=True,\n",
    ")\n",
    "print(input_data)\n",
    "tokens = tokenizer.decode(token_index)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [\"O\"] + list(\n",
    "    {\n",
    "        \"movie\",\n",
    "        \"name\",\n",
    "        \"game\",\n",
    "        \"address\",\n",
    "        \"position\",\n",
    "        \"company\",\n",
    "        \"scene\",\n",
    "        \"book\",\n",
    "        \"organization\",\n",
    "        \"government\",\n",
    "    }\n",
    ")\n",
    "tags = [\"O\"]\n",
    "for entity in entities:\n",
    "    tags.append(\"B-\" + entity.upper())\n",
    "    tags.append(\"I-\" + entity.upper())\n",
    "entity_index = {entity: i for i, entity in enumerate(entities)}\n",
    "\n",
    "\n",
    "def entity_tags_proc(item):\n",
    "    text_len = len(item[\"text\"])\n",
    "    tags = [0] * text_len\n",
    "\n",
    "    entities = item[\"ents\"]\n",
    "    for ent in entities:\n",
    "        indices = ent[\"indices\"]\n",
    "        label = ent[\"label\"]\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "\n",
    "    return {\"ent_tag\": tags}\n",
    "\n",
    "\n",
    "ds1 = ds.map(entity_tags_proc)\n",
    "\n",
    "\n",
    "def data_input_proc(item):\n",
    "    # 输入文本先拆分为字符,再转换为模型输入的token索引\n",
    "    batch_texts = [list(text) for text in item[\"text\"]]\n",
    "    input_data = tokenizer(\n",
    "        batch_texts,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "        max_length=512,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    input_data[\"labels\"] = [tag + [0] * (512 - len(tag)) for tag in item[\"ent_tag\"]]\n",
    "    return input_data\n",
    "\n",
    "\n",
    "ds2 = ds1.map(data_input_proc, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ds2[\"train\"]:\n",
    "    print(\n",
    "        len(item[\"input_ids\"]),\n",
    "        len(item[\"token_type_ids\"]),\n",
    "        len(item[\"attention_mask\"]),\n",
    "        len(item[\"labels\"]),\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记录转换为pytorch\n",
    "ds2.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "[name for name, params in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    ds2[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "id2lbl = {i: tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag: i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-chinese\",\n",
    "    num_labels=23,\n",
    "    id2label=id2lbl,\n",
    "    label2id=lbl2id,\n",
    ")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# 模型参数分组\n",
    "param_optimizer = list(model.named_parameters())\n",
    "bert_params, classifier_params = [], []\n",
    "\n",
    "for name, param in param_optimizer:\n",
    "    if \"bert\" in name:\n",
    "        bert_params.append(param)\n",
    "    else:\n",
    "        classifier_params.append(param)\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": bert_params, \"lr\": 1e-5},\n",
    "    {\"params\": classifier_params, \"weight_decay\": 0.1, \"lr\": 1e-4},\n",
    "]\n",
    "optimizer = optim.AdamW(param_groups)\n",
    "# 学习率调度器\n",
    "train_steps = len(train_dl) * 5\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=100, num_training_steps=train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_epoch(model, train_dl, optimizer, scheduler, epoch=5):\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        tpbar = tqdm(train_dl)\n",
    "        for items in tpbar:\n",
    "            items = {k: v.to(\"cuda\") for k, v in items.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**items)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            tpbar.set_description(\n",
    "                f\"Epoch:{epoch+1}\"\n",
    "                + f\"bert_lr:{scheduler.get_lr()[0]}\"\n",
    "                + f\"classifier_lr:{scheduler.get_lr()[1]}\"\n",
    "                + f\"loss:{loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "train_epoch(model, train_dl, optimizer, scheduler, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 支持混合精度训练\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "scaler = torch.GradScaler()\n",
    "\n",
    "\n",
    "def train_epoch_amp(model, train_dl, optimizer, scheduler, epoch=5):\n",
    "    for epoch in range(epoch):\n",
    "        model.train()\n",
    "        tpbar = tqdm(train_dl)\n",
    "        for items in tpbar:\n",
    "            items = {k: v.to(DEVICE) for k, v in items.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=DEVICE):\n",
    "                outputs = model(**items)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            # 使用混合精度训练\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            tpbar.set_description(\n",
    "                f\"Epoch:{epoch+1}\"\n",
    "                + f\"bert_lr:{scheduler.get_lr()[0]}\"\n",
    "                + f\"classifier_lr:{scheduler.get_lr()[1]}\"\n",
    "                + f\"loss:{loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "train_epoch_amp(model, train_dl, optimizer, scheduler, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ddp_simple.py\n",
    "# 支持分布式训练\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    \n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# 训练函数\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # 定义模型并将其移动到GPU\n",
    "    model = models.resnet50().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss().to(rank)\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)\n",
    "    \n",
    "    # 数据预处理和加载\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    # 分布式训练采样器\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    # 训练循环\n",
    "    for epoch in range(10):\n",
    "        ddp_model.train()\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(rank), labels.to(rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 混合精度训练\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = ddp_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(ddp_model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            print(f\"Rank {rank}, Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ner_ddp.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval  # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 数据集\n",
    "    ds = load_dataset(\"nlhappy/CLUE-NER\")\n",
    "    # entity_index\n",
    "    entites = [\"O\"] + list(\n",
    "        {\n",
    "            \"movie\",\n",
    "            \"name\",\n",
    "            \"game\",\n",
    "            \"address\",\n",
    "            \"position\",\n",
    "            \"company\",\n",
    "            \"scene\",\n",
    "            \"book\",\n",
    "            \"organization\",\n",
    "            \"government\",\n",
    "        }\n",
    "    )\n",
    "    tags = [\"O\"]\n",
    "    for entity in entites[1:]:\n",
    "        tags.append(\"B-\" + entity.upper())\n",
    "        tags.append(\"I-\" + entity.upper())\n",
    "\n",
    "    entity_index = {entity: i for i, entity in enumerate(entites)}\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "\n",
    "    def entity_tags_proc(item):\n",
    "        # item即是dataset中记录\n",
    "        text_len = len(item[\"text\"])  # 根据文本长度生成tags列表\n",
    "        tags = [0] * text_len  # 初始值为‘O’\n",
    "        # 遍历实体列表，所有实体类别标记填入tags\n",
    "        entites = item[\"ents\"]\n",
    "        for ent in entites:\n",
    "            indices = ent[\"indices\"]  # 实体索引\n",
    "            label = ent[\"label\"]  # 实体名\n",
    "            tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "            for idx in indices[1:]:\n",
    "                tags[idx] = entity_index[label] * 2\n",
    "        return {\"ent_tag\": tags}\n",
    "\n",
    "    # 使用自定义回调函数处理数据集记录\n",
    "    ds1 = ds.map(entity_tags_proc)\n",
    "\n",
    "    def data_input_proc(item):\n",
    "        # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "        batch_texts = [list(text) for text in item[\"text\"]]\n",
    "        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "        input_data = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            max_length=512,\n",
    "            is_split_into_words=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        input_data[\"labels\"] = [tag + [0] * (512 - len(tag)) for tag in item[\"ent_tag\"]]\n",
    "        return input_data\n",
    "\n",
    "    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n",
    "\n",
    "    local_rank = rank\n",
    "\n",
    "    id2lbl = {i: tag for i, tag in enumerate(tags)}\n",
    "    lbl2id = {tag: i for i, tag in enumerate(tags)}\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        \"google-bert/bert-base-chinese\", num_labels=21, id2label=id2lbl, label2id=lbl2id\n",
    "    )\n",
    "    model.to(local_rank)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "        num_train_epochs=3,  # 训练 epoch\n",
    "        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "        per_device_train_batch_size=8,  # 训练批次\n",
    "        per_device_eval_batch_size=8,\n",
    "        report_to=\"tensorboard\",  # 训练输出记录\n",
    "        eval_strategy=\"epoch\",\n",
    "        local_rank=local_rank,  # 当前进程 RANK\n",
    "        fp16=True,  # 使用混合精度\n",
    "        lr_scheduler_type=\"linear\",  # 动态学习率\n",
    "        warmup_steps=100,  # 预热步数\n",
    "        ddp_find_unused_parameters=False,  # 优化DDP性能\n",
    "    )\n",
    "\n",
    "    def compute_metric(result):\n",
    "        # result 是一个tuple (predicts, labels)\n",
    "\n",
    "        # 获取评估对象\n",
    "        seqeval = evaluate.load(\"seqeval\")\n",
    "        predicts, labels = result\n",
    "        predicts = np.argmax(predicts, axis=2)\n",
    "\n",
    "        # 准备评估数据\n",
    "        predicts = [\n",
    "            [tags[p] for p, l in zip(ps, ls) if l != -100]\n",
    "            for ps, ls in zip(predicts, labels)\n",
    "        ]\n",
    "        labels = [\n",
    "            [tags[l] for p, l in zip(ps, ls) if l != -100]\n",
    "            for ps, ls in zip(predicts, labels)\n",
    "        ]\n",
    "        results = seqeval.compute(predictions=predicts, references=labels)\n",
    "\n",
    "        return results\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer, padding=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=ds2[\"train\"],\n",
    "        eval_dataset=ds2[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metric,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ner_ddp.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
