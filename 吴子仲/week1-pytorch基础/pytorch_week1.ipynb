{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 创建ndarray数组\u001b[39;00m\n\u001b[0;32m      5\u001b[0m ndarry \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m], \u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "# 创建ndarray数组\n",
    "ndarry = np.array([1,2,3,4], float)\n",
    "ndarry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建多维数组\n",
    "arr = np.array([(1,2,3), (2,3,4), (3,4,5)])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建0数组\n",
    "arr_zero = np.zeros((3,3), dtype=float)\n",
    "arr_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建1数组\n",
    "arr_one = np.ones((3,3), dtype=np.float32)\n",
    "arr_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.9, 1.8, 2.7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建等差序列\n",
    "arr_arange = np.arange(0,3, 0.9)\n",
    "arr_arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建单位矩阵\n",
    "arr_eye = np.eye(4)\n",
    "arr_eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.27589729, 1.47806508, 0.87743632, 0.64207862, 1.07446925,\n",
       "       1.21128044, 1.36518984, 1.81712723, 0.5311751 , 0.87623786])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建[0,1) 随机数\n",
    "np.random.random(3)\n",
    "# 创建均值m，标准差sigma的正态分布随机数\n",
    "m,sigma = 1, 0.5\n",
    "np.random.normal(m, sigma, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 访问多维数组\n",
    "a = arr[0]\n",
    "b = arr[1:]\n",
    "c = arr[:, 1:]\n",
    "d = arr[1][2]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9\n",
      "1.8\n",
      "2.7\n",
      "1 2 [3]\n",
      "2 3 [4]\n",
      "3 4 [5]\n"
     ]
    }
   ],
   "source": [
    "# 遍历一维数组\n",
    "for i in arr_arange:\n",
    "    print(i)\n",
    "# 遍历多维数组\n",
    "for i,j,*k in arr:\n",
    "    print(i,j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndim(维度-秩):  2 \n",
      "shape(大小):  (3, 3) \n",
      "size(元素个数):  9 \n",
      "dtype(元素类型):  int32\n"
     ]
    }
   ],
   "source": [
    "# np数组常用属性\n",
    "print(\"ndim(维度-秩): \", arr.ndim ,\n",
    "      \"\\nshape(大小): \", arr.shape,\n",
    "      \"\\nsize(元素个数): \", arr.size,\n",
    "      \"\\ndtype(元素类型): \", arr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \n",
      " False\n",
      "[1 2 3 2 3 4 3 4 5] \n",
      " [1 2 3 2 3 4 3 4 5]\n",
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]] \n",
      " [[1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "# 检测数值是否在数组中\n",
    "print(3 in arr, \"\\n\", 6 in arr)\n",
    "# 将多维数组转成一维\n",
    "print(arr.reshape(arr.size), \"\\n\", arr.flatten())\n",
    "# 转置\n",
    "print(arr.T, \"\\n\", arr.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3]],\n",
       "\n",
       "       [[2, 3, 4]],\n",
       "\n",
       "       [[3, 4, 5]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 增加维度\n",
    "arr = arr[:, np.newaxis]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a+b= [[2. 3.]\n",
      " [4. 5.]]\n",
      "a-b= [[ 0. -1.]\n",
      " [-2. -3.]]\n",
      "a*b= [[1. 2.]\n",
      " [3. 4.]]\n",
      "a/b [[1.         0.5       ]\n",
      " [0.33333333 0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "# np数学运算\n",
    "a = np.ones((2,2))\n",
    "b = np.array([(1,2), (3,4)])\n",
    "print(\"a+b=\", a + b)\n",
    "print(\"a-b=\", a - b)\n",
    "print(\"a*b=\", a * b)\n",
    "print(\"a/b\", a / b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "求和sum= 10\n",
      "求积prod= 24\n"
     ]
    }
   ],
   "source": [
    "print(\"求和sum=\", b.sum())\n",
    "print(\"求积prod=\", b.prod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均数mean: 2.5\n",
      "方差var: 1.25\n",
      "标准差std: 1.118033988749895\n",
      "最大最小值: 4 | 1\n"
     ]
    }
   ],
   "source": [
    "print(\"平均数mean:\", b.mean())\n",
    "print(\"方差var:\", b.var())\n",
    "print(\"标准差std:\", b.std())\n",
    "print(\"最大最小值:\", b.max(), \"|\", b.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax: 2\n",
      "argmin: 0\n",
      "ceil(向上取整): [2. 4. 5. 4.]\n",
      "floor(向下取整): [1. 3. 4. 3.]\n",
      "rint:(四舍五入) [1. 4. 5. 4.]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1.2, 3.8, 4.9, 3.5])\n",
    "print(\"argmax:\", a.argmax())\n",
    "print(\"argmin:\", a.argmin())\n",
    "print(\"ceil(向上取整):\", np.ceil(a))\n",
    "print(\"floor(向下取整):\", np.floor(a))\n",
    "print(\"rint:(四舍五入)\", np.rint(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2, 3.5, 3.8, 4.9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -7 -10]\n",
      " [-15 -22]]\n",
      "[[ -7 -10]\n",
      " [-15 -22]]\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法（内积）\n",
    "a = np.array([(1,2),(3,4)])\n",
    "b = np.array([(-1, -2),(-3, -4)])\n",
    "print(np.dot(a, b))\n",
    "print(a @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [4 6]]\n",
      "[[2 3]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# 广播机制，自动补全计算时的元素，使shape相同\n",
    "c = np.array([(1,2)])\n",
    "d = np.array([[1],[2]])\n",
    "print(a + c)\n",
    "print(a + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# 张量初始化\n",
    "data = [(1,2),(3,4)]\n",
    "xdata = torch.tensor(data)\n",
    "print(xdata)\n",
    "\n",
    "# ndarr = np.array([(1,2)])\n",
    "# xdata2 = torch.from_numpy(ndarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.1236, 0.5554],\n",
      "        [0.5623, 0.0404]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(xdata)\n",
    "print(x_ones)\n",
    "x_rands = torch.rand_like(xdata, dtype=torch.float)\n",
    "print(x_rands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.0148, 0.2390, 0.8751],\n",
      "        [0.7530, 0.5366, 0.6136]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  1.4737,  1.9474,  2.4211,  2.8947,  3.3684,  3.8421,  4.3158,\n",
       "         4.7895,  5.2632,  5.7368,  6.2105,  6.6842,  7.1579,  7.6316,  8.1053,\n",
       "         8.5789,  9.0526,  9.5263, 10.0000])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于现有tensor构建，但使⽤新值填充\n",
    "m = torch.ones(3,3, dtype=torch.double)\n",
    "n = torch.rand_like(m, dtype=torch.float)\n",
    "# 获取tensor的⼤⼩\n",
    "print(m.size()) # torch.Size([3,3])\n",
    "# 均匀分布\n",
    "torch.rand(3,3)\n",
    "# 标准正态分布\n",
    "torch.randn(3,3)\n",
    "# 离散正态分布\n",
    "torch.normal(mean=.0,std=1.0,size=(3,3))\n",
    "# 线性间隔向量(返回⼀个1维张量，包含在区间start和end上均匀间隔的steps个点)\n",
    "torch.linspace(start=1,end=10,steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "形状: torch.Size([2, 2])\n",
      "类型: torch.int64\n",
      "存储设备: cpu\n"
     ]
    }
   ],
   "source": [
    "# 张量的属性\n",
    "print(\"形状:\",xdata.shape)\n",
    "print(\"类型:\", xdata.dtype)\n",
    "print(\"存储设备:\", xdata.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 设置张量在GPU上运算\n",
    "if torch.cuda.is_available():\n",
    "    tensor = xdata.to('cuda')\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1, 2])\n",
      "First column:  tensor([1, 3])\n",
      "Last column: tensor([2, 4])\n",
      "tensor([[1, 0],\n",
      "        [3, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('First row: ', xdata[0])\n",
    "print('First column: ', xdata[:, 0])\n",
    "print('Last column:', xdata[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.cat([xdata, xdata], dim=1) #默认dim = 0\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5., 11.],\n",
      "        [11., 25.]])\n",
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# 计算两个张量之间矩阵乘法的⼏种⽅式。 y1, y2, y3 最后的值是⼀样的 dot\n",
    "tensor = torch.tensor([(1,2), (3,4)], dtype=torch.float)\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "print(y3)\n",
    "\n",
    "# 计算张量逐元素相乘的⼏种⽅法。 z1, z2, z3 最后的值是⼀样的。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "10.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "print(agg)\n",
    "agg_item = agg.item() # 将单元素张量转化为python值\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) \n",
      "\n",
      "tensor([[6., 7.],\n",
      "        [8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "# In-place操作虽然节省了⼀部分内存，但在计算导数时可能会出现问题，因为它会⽴即丢失历史记录。因此，不⿎励使⽤它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m n \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 定义矩阵 A，向量 b 和常数 c\u001b[39;00m\n\u001b[0;32m      3\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m,requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "# 定义矩阵 A，向量 b 和常数 c\n",
    "A = torch.randn(10, 10,requires_grad=True)\n",
    "b = torch.randn(10,requires_grad=True)\n",
    "c = torch.randn(1,requires_grad=True)\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "# 计算 x^T * A + b * x + c\n",
    "result = torch.matmul(A, x.T) + torch.matmul(b, x) + c\n",
    "# ⽣成计算图节点\n",
    "dot = make_dot(result, params={'A': A, 'b': b, 'c': c, 'x': x})\n",
    "# 绘制计算图\n",
    "dot.render('expression', format='png', cleanup=True, view=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
