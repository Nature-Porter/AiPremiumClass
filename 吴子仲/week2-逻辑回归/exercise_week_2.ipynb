{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 4) (20, 4) (80,) (20,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X,y = load_iris(return_X_y=True)\n",
    "# 取前一百条数据作为样本\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "\n",
    "#数据拆分\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [[ 2.02263828  0.953709   -0.48044643 -0.93901783]], bias: 0, lr: 0.1, epochs: 1000\n"
     ]
    }
   ],
   "source": [
    "# 权重参数\n",
    "theta = np.random.randn(1,4)\n",
    "bias = 0\n",
    "# 超参数\n",
    "lr = 0.1 # 学习率\n",
    "epochs = 1000 # 训练次数\n",
    "print(f\"theta: {theta}, bias: {bias}, lr: {lr}, epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, theta, bias):\n",
    "    # 线性运算\n",
    "    z = np.dot(theta, X.T) + bias\n",
    "    # sigmoid函数\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    return y_hat\n",
    "\n",
    "# sigmoid损失函数\n",
    "def loss(y, y_hat):\n",
    "    e = 1e-8\n",
    "    return - y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)\n",
    "\n",
    "# 计算梯度\n",
    "def cal_gradient(x, y, y_hat):\n",
    "    m = x.shape[-1]\n",
    "    delta_theta = np.dot((y_hat - y), x) / m\n",
    "    delta_bais = np.mean(y_hat - y)\n",
    "    return delta_theta,delta_bais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 6.860273370691426, acc: 0.45\n",
      "epoch: 5, loss: 8.288928306619354, acc: 0.55\n",
      "epoch: 10, loss: 2.451047923893394, acc: 0.4625\n",
      "epoch: 15, loss: 0.001331210718621003, acc: 1.0\n",
      "epoch: 20, loss: 0.0007763101404630805, acc: 1.0\n",
      "epoch: 25, loss: 0.0005494367578342199, acc: 1.0\n",
      "epoch: 30, loss: 0.00042563046887686906, acc: 1.0\n",
      "epoch: 35, loss: 0.0003475447684446924, acc: 1.0\n",
      "epoch: 40, loss: 0.0002937609093393693, acc: 1.0\n",
      "epoch: 45, loss: 0.0002544444410784117, acc: 1.0\n",
      "epoch: 50, loss: 0.00022444144703156476, acc: 1.0\n",
      "epoch: 55, loss: 0.00020078839611039758, acc: 1.0\n",
      "epoch: 60, loss: 0.0001816595799951801, acc: 1.0\n",
      "epoch: 65, loss: 0.00016586854922353412, acc: 1.0\n",
      "epoch: 70, loss: 0.00015261068638000502, acc: 1.0\n",
      "epoch: 75, loss: 0.00014132095575392382, acc: 1.0\n",
      "epoch: 80, loss: 0.00013159086246541536, acc: 1.0\n",
      "epoch: 85, loss: 0.0001231177250118173, acc: 1.0\n",
      "epoch: 90, loss: 0.00011567248063698778, acc: 1.0\n",
      "epoch: 95, loss: 0.00010907857534487842, acc: 1.0\n",
      "epoch: 100, loss: 0.00010319772640343891, acc: 1.0\n",
      "epoch: 105, loss: 9.79200806808323e-05, acc: 1.0\n",
      "epoch: 110, loss: 9.315726246225719e-05, acc: 1.0\n",
      "epoch: 115, loss: 8.883736692520691e-05, acc: 1.0\n",
      "epoch: 120, loss: 8.490129215004334e-05, acc: 1.0\n",
      "epoch: 125, loss: 8.130000985811919e-05, acc: 1.0\n",
      "epoch: 130, loss: 7.799250599050672e-05, acc: 1.0\n",
      "epoch: 135, loss: 7.494420682054495e-05, acc: 1.0\n",
      "epoch: 140, loss: 7.212576206815874e-05, acc: 1.0\n",
      "epoch: 145, loss: 6.951209396169016e-05, acc: 1.0\n",
      "epoch: 150, loss: 6.708164680507765e-05, acc: 1.0\n",
      "epoch: 155, loss: 6.481578939158393e-05, acc: 1.0\n",
      "epoch: 160, loss: 6.269833512820733e-05, acc: 1.0\n",
      "epoch: 165, loss: 6.071515367417144e-05, acc: 1.0\n",
      "epoch: 170, loss: 5.8853854355661094e-05, acc: 1.0\n",
      "epoch: 175, loss: 5.710352633931183e-05, acc: 1.0\n",
      "epoch: 180, loss: 5.545452403482168e-05, acc: 1.0\n",
      "epoch: 185, loss: 5.389828879816121e-05, acc: 1.0\n",
      "epoch: 190, loss: 5.242719996646364e-05, acc: 1.0\n",
      "epoch: 195, loss: 5.103444974423428e-05, acc: 1.0\n",
      "epoch: 200, loss: 4.971393760021998e-05, acc: 1.0\n",
      "epoch: 205, loss: 4.846018071482536e-05, acc: 1.0\n",
      "epoch: 210, loss: 4.726823770132871e-05, acc: 1.0\n",
      "epoch: 215, loss: 4.613364336085471e-05, acc: 1.0\n",
      "epoch: 220, loss: 4.505235265218183e-05, acc: 1.0\n",
      "epoch: 225, loss: 4.402069239286731e-05, acc: 1.0\n",
      "epoch: 230, loss: 4.303531947469768e-05, acc: 1.0\n",
      "epoch: 235, loss: 4.2093184590980154e-05, acc: 1.0\n",
      "epoch: 240, loss: 4.119150064534065e-05, acc: 1.0\n",
      "epoch: 245, loss: 4.032771515161017e-05, acc: 1.0\n",
      "epoch: 250, loss: 3.94994860493066e-05, acc: 1.0\n",
      "epoch: 255, loss: 3.8704660450510946e-05, acc: 1.0\n",
      "epoch: 260, loss: 3.794125591250492e-05, acc: 1.0\n",
      "epoch: 265, loss: 3.72074438923389e-05, acc: 1.0\n",
      "epoch: 270, loss: 3.650153509235727e-05, acc: 1.0\n",
      "epoch: 275, loss: 3.582196644944973e-05, acc: 1.0\n",
      "epoch: 280, loss: 3.516728955648419e-05, acc: 1.0\n",
      "epoch: 285, loss: 3.453616033562085e-05, acc: 1.0\n",
      "epoch: 290, loss: 3.392732980819629e-05, acc: 1.0\n",
      "epoch: 295, loss: 3.333963582761137e-05, acc: 1.0\n",
      "epoch: 300, loss: 3.27719956601702e-05, acc: 1.0\n",
      "epoch: 305, loss: 3.2223399313902304e-05, acc: 1.0\n",
      "epoch: 310, loss: 3.16929035290326e-05, acc: 1.0\n",
      "epoch: 315, loss: 3.1179626354518464e-05, acc: 1.0\n",
      "epoch: 320, loss: 3.068274224519754e-05, acc: 1.0\n",
      "epoch: 325, loss: 3.0201477622273574e-05, acc: 1.0\n",
      "epoch: 330, loss: 2.9735106846421656e-05, acc: 1.0\n",
      "epoch: 335, loss: 2.9282948560033636e-05, acc: 1.0\n",
      "epoch: 340, loss: 2.8844362359001685e-05, acc: 1.0\n",
      "epoch: 345, loss: 2.8418745760725026e-05, acc: 1.0\n",
      "epoch: 350, loss: 2.800553143721844e-05, acc: 1.0\n",
      "epoch: 355, loss: 2.7604184687358924e-05, acc: 1.0\n",
      "epoch: 360, loss: 2.72142011239498e-05, acc: 1.0\n",
      "epoch: 365, loss: 2.683510455496596e-05, acc: 1.0\n",
      "epoch: 370, loss: 2.646644504009461e-05, acc: 1.0\n",
      "epoch: 375, loss: 2.6107797105740394e-05, acc: 1.0\n",
      "epoch: 380, loss: 2.57587581038919e-05, acc: 1.0\n",
      "epoch: 385, loss: 2.5418946701325324e-05, acc: 1.0\n",
      "epoch: 390, loss: 2.5088001487207734e-05, acc: 1.0\n",
      "epoch: 395, loss: 2.4765579688489394e-05, acc: 1.0\n",
      "epoch: 400, loss: 2.4451355983529025e-05, acc: 1.0\n",
      "epoch: 405, loss: 2.4145021405067643e-05, acc: 1.0\n",
      "epoch: 410, loss: 2.384628232508431e-05, acc: 1.0\n",
      "epoch: 415, loss: 2.355485951420077e-05, acc: 1.0\n",
      "epoch: 420, loss: 2.3270487269700882e-05, acc: 1.0\n",
      "epoch: 425, loss: 2.2992912605922777e-05, acc: 1.0\n",
      "epoch: 430, loss: 2.2721894502221542e-05, acc: 1.0\n",
      "epoch: 435, loss: 2.2457203203873482e-05, acc: 1.0\n",
      "epoch: 440, loss: 2.2198619570990625e-05, acc: 1.0\n",
      "epoch: 445, loss: 2.1945934472584563e-05, acc: 1.0\n",
      "epoch: 450, loss: 2.1698948221293186e-05, acc: 1.0\n",
      "epoch: 455, loss: 2.1457470046267884e-05, acc: 1.0\n",
      "epoch: 460, loss: 2.122131760078592e-05, acc: 1.0\n",
      "epoch: 465, loss: 2.0990316502231494e-05, acc: 1.0\n",
      "epoch: 470, loss: 2.0764299901863495e-05, acc: 1.0\n",
      "epoch: 475, loss: 2.054310808231469e-05, acc: 1.0\n",
      "epoch: 480, loss: 2.032658808037325e-05, acc: 1.0\n",
      "epoch: 485, loss: 2.0114593333665414e-05, acc: 1.0\n",
      "epoch: 490, loss: 1.9906983349272695e-05, acc: 1.0\n",
      "epoch: 495, loss: 1.970362339264649e-05, acc: 1.0\n",
      "epoch: 500, loss: 1.9504384195459343e-05, acc: 1.0\n",
      "epoch: 505, loss: 1.9309141681235084e-05, acc: 1.0\n",
      "epoch: 510, loss: 1.9117776707077154e-05, acc: 1.0\n",
      "epoch: 515, loss: 1.8930174820956854e-05, acc: 1.0\n",
      "epoch: 520, loss: 1.8746226032918734e-05, acc: 1.0\n",
      "epoch: 525, loss: 1.8565824599763796e-05, acc: 1.0\n",
      "epoch: 530, loss: 1.8388868821976244e-05, acc: 1.0\n",
      "epoch: 535, loss: 1.8215260852168815e-05, acc: 1.0\n",
      "epoch: 540, loss: 1.804490651444699e-05, acc: 1.0\n",
      "epoch: 545, loss: 1.7877715133689898e-05, acc: 1.0\n",
      "epoch: 550, loss: 1.771359937439544e-05, acc: 1.0\n",
      "epoch: 555, loss: 1.7552475088233477e-05, acc: 1.0\n",
      "epoch: 560, loss: 1.7394261170001952e-05, acc: 1.0\n",
      "epoch: 565, loss: 1.723887942125767e-05, acc: 1.0\n",
      "epoch: 570, loss: 1.7086254421180797e-05, acc: 1.0\n",
      "epoch: 575, loss: 1.69363134043225e-05, acc: 1.0\n",
      "epoch: 580, loss: 1.6788986144708802e-05, acc: 1.0\n",
      "epoch: 585, loss: 1.6644204846089034e-05, acc: 1.0\n",
      "epoch: 590, loss: 1.6501904037599474e-05, acc: 1.0\n",
      "epoch: 595, loss: 1.6362020475009526e-05, acc: 1.0\n",
      "epoch: 600, loss: 1.622449304667547e-05, acc: 1.0\n",
      "epoch: 605, loss: 1.6089262684613354e-05, acc: 1.0\n",
      "epoch: 610, loss: 1.595627227953868e-05, acc: 1.0\n",
      "epoch: 615, loss: 1.582546660045948e-05, acc: 1.0\n",
      "epoch: 620, loss: 1.5696792218038095e-05, acc: 1.0\n",
      "epoch: 625, loss: 1.5570197431645002e-05, acc: 1.0\n",
      "epoch: 630, loss: 1.5445632200102956e-05, acc: 1.0\n",
      "epoch: 635, loss: 1.5323048075514485e-05, acc: 1.0\n",
      "epoch: 640, loss: 1.5202398140387637e-05, acc: 1.0\n",
      "epoch: 645, loss: 1.5083636947662692e-05, acc: 1.0\n",
      "epoch: 650, loss: 1.4966720463536876e-05, acc: 1.0\n",
      "epoch: 655, loss: 1.4851606012938301e-05, acc: 1.0\n",
      "epoch: 660, loss: 1.4738252227517059e-05, acc: 1.0\n",
      "epoch: 665, loss: 1.4626618995970016e-05, acc: 1.0\n",
      "epoch: 670, loss: 1.451666741670192e-05, acc: 1.0\n",
      "epoch: 675, loss: 1.4408359752453377e-05, acc: 1.0\n",
      "epoch: 680, loss: 1.4301659387205095e-05, acc: 1.0\n",
      "epoch: 685, loss: 1.4196530784687798e-05, acc: 1.0\n",
      "epoch: 690, loss: 1.4092939448875233e-05, acc: 1.0\n",
      "epoch: 695, loss: 1.3990851886305988e-05, acc: 1.0\n",
      "epoch: 700, loss: 1.3890235569837111e-05, acc: 1.0\n",
      "epoch: 705, loss: 1.3791058903983425e-05, acc: 1.0\n",
      "epoch: 710, loss: 1.3693291191929845e-05, acc: 1.0\n",
      "epoch: 715, loss: 1.3596902603653135e-05, acc: 1.0\n",
      "epoch: 720, loss: 1.3501864145609597e-05, acc: 1.0\n",
      "epoch: 725, loss: 1.340814763162909e-05, acc: 1.0\n",
      "epoch: 730, loss: 1.3315725654948755e-05, acc: 1.0\n",
      "epoch: 735, loss: 1.3224571561523668e-05, acc: 1.0\n",
      "epoch: 740, loss: 1.3134659424221621e-05, acc: 1.0\n",
      "epoch: 745, loss: 1.3045964018380764e-05, acc: 1.0\n",
      "epoch: 750, loss: 1.29584607980222e-05, acc: 1.0\n",
      "epoch: 755, loss: 1.2872125873265695e-05, acc: 1.0\n",
      "epoch: 760, loss: 1.2786935988479327e-05, acc: 1.0\n",
      "epoch: 765, loss: 1.2702868501482217e-05, acc: 1.0\n",
      "epoch: 770, loss: 1.2619901363285415e-05, acc: 1.0\n",
      "epoch: 775, loss: 1.2538013098914886e-05, acc: 1.0\n",
      "epoch: 780, loss: 1.2457182788757278e-05, acc: 1.0\n",
      "epoch: 785, loss: 1.237739005082393e-05, acc: 1.0\n",
      "epoch: 790, loss: 1.229861502346541e-05, acc: 1.0\n",
      "epoch: 795, loss: 1.2220838348983352e-05, acc: 1.0\n",
      "epoch: 800, loss: 1.2144041157669186e-05, acc: 1.0\n",
      "epoch: 805, loss: 1.2068205052504157e-05, acc: 1.0\n",
      "epoch: 810, loss: 1.1993312094522033e-05, acc: 1.0\n",
      "epoch: 815, loss: 1.1919344788626326e-05, acc: 1.0\n",
      "epoch: 820, loss: 1.1846286069845256e-05, acc: 1.0\n",
      "epoch: 825, loss: 1.1774119290339545e-05, acc: 1.0\n",
      "epoch: 830, loss: 1.1702828206559273e-05, acc: 1.0\n",
      "epoch: 835, loss: 1.16323969672284e-05, acc: 1.0\n",
      "epoch: 840, loss: 1.156281010145194e-05, acc: 1.0\n",
      "epoch: 845, loss: 1.14940525073746e-05, acc: 1.0\n",
      "epoch: 850, loss: 1.1426109441286747e-05, acc: 1.0\n",
      "epoch: 855, loss: 1.1358966507054191e-05, acc: 1.0\n",
      "epoch: 860, loss: 1.1292609645912003e-05, acc: 1.0\n",
      "epoch: 865, loss: 1.1227025126665347e-05, acc: 1.0\n",
      "epoch: 870, loss: 1.1162199536164151e-05, acc: 1.0\n",
      "epoch: 875, loss: 1.1098119770202751e-05, acc: 1.0\n",
      "epoch: 880, loss: 1.1034773024600345e-05, acc: 1.0\n",
      "epoch: 885, loss: 1.0972146786750866e-05, acc: 1.0\n",
      "epoch: 890, loss: 1.0910228827288334e-05, acc: 1.0\n",
      "epoch: 895, loss: 1.0849007192145297e-05, acc: 1.0\n",
      "epoch: 900, loss: 1.0788470194859967e-05, acc: 1.0\n",
      "epoch: 905, loss: 1.07286064091154e-05, acc: 1.0\n",
      "epoch: 910, loss: 1.0669404661573173e-05, acc: 1.0\n",
      "epoch: 915, loss: 1.0610854024815551e-05, acc: 1.0\n",
      "epoch: 920, loss: 1.0552943810707012e-05, acc: 1.0\n",
      "epoch: 925, loss: 1.0495663563880948e-05, acc: 1.0\n",
      "epoch: 930, loss: 1.0439003055343111e-05, acc: 1.0\n",
      "epoch: 935, loss: 1.038295227653745e-05, acc: 1.0\n",
      "epoch: 940, loss: 1.0327501433185943e-05, acc: 1.0\n",
      "epoch: 945, loss: 1.0272640939865515e-05, acc: 1.0\n",
      "epoch: 950, loss: 1.0218361414353432e-05, acc: 1.0\n",
      "epoch: 955, loss: 1.0164653672111607e-05, acc: 1.0\n",
      "epoch: 960, loss: 1.0111508721392857e-05, acc: 1.0\n",
      "epoch: 965, loss: 1.0058917758021154e-05, acc: 1.0\n",
      "epoch: 970, loss: 1.0006872160563434e-05, acc: 1.0\n",
      "epoch: 975, loss: 9.955363485612552e-06, acc: 1.0\n",
      "epoch: 980, loss: 9.904383463248028e-06, acc: 1.0\n",
      "epoch: 985, loss: 9.853923992546914e-06, acc: 1.0\n",
      "epoch: 990, loss: 9.803977137327924e-06, acc: 1.0\n",
      "epoch: 995, loss: 9.754535121973417e-06, acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "for i in range(epochs):\n",
    "    #前向计算\n",
    "    y_hat = forward(X_train, theta, bias)\n",
    "    # 损失计算\n",
    "    loss_val = loss(y_train, y_hat)\n",
    "    # 计算梯度\n",
    "    delta_theta, delta_bias = cal_gradient(X_train, y_train, y_hat)\n",
    "    # 更新参数\n",
    "    theta = theta - lr * delta_theta\n",
    "    bias = bias - lr * delta_bias\n",
    "\n",
    "    # 间隔5步输出一次\n",
    "    if i % 5 == 0:\n",
    "        # 计算准确率\n",
    "        acc = np.mean(np.round(y_hat) == y_train)\n",
    "        print(f\"epoch: {i}, loss: {np.mean(loss_val)}, acc: {acc}\")\n",
    "\n",
    "# 将参数保存到npy文件\n",
    "iris_model_params = {'theta': [], 'bias': 0}\n",
    "iris_model_params['theta'] = theta\n",
    "iris_model_params['bias'] = bias\n",
    "np.save(\"鸢尾花模型参数.npy\", iris_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 1, predict: [1.]\n"
     ]
    }
   ],
   "source": [
    "# 模型推理\n",
    "idx = np.random.randint(len(X_test))\n",
    "x = X_test[idx]\n",
    "y = y_test[idx]\n",
    "\n",
    "predict = np.round(forward(x, theta, bias))\n",
    "print(f\"y: {y}, predict: {predict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [[-2.47106909 -8.93222062 11.7197772   4.4029588 ]], bias: -0.09244774592391142\n",
      "[[2.61595349e-12 3.73139664e-10 3.17453024e-11 1.03486801e-09\n",
      "  1.37096312e-12 2.84039918e-12 3.41472188e-11 2.64150876e-11\n",
      "  3.13601884e-09 3.17479282e-10 6.74247194e-13 1.39787985e-10\n",
      "  3.07588796e-10 3.14489375e-11 5.11488496e-16 1.49229230e-15\n",
      "  2.61484260e-14 4.06301185e-12 2.12878214e-12 8.99636998e-13\n",
      "  1.02459044e-10 3.41355411e-12 3.39125631e-14 1.96823715e-09\n",
      "  4.70348988e-09 3.03753409e-09 2.05717697e-10 6.59623785e-12\n",
      "  4.99153664e-12 1.06814408e-09 2.03814033e-09 2.37148034e-11\n",
      "  1.99778966e-14 1.87461229e-15 4.93098248e-10 4.68537666e-12\n",
      "  3.01562887e-13 1.13012080e-12 3.97625103e-10 2.06316940e-11\n",
      "  1.61132001e-12 2.50509661e-07 6.66240409e-11 2.03135760e-10\n",
      "  1.51781628e-10 7.42004696e-10 1.86996162e-12 1.31213975e-10\n",
      "  8.63249459e-13 1.99889513e-11 9.99999767e-01 9.99999646e-01\n",
      "  9.99999995e-01 9.99999990e-01 9.99999996e-01 9.99999996e-01\n",
      "  9.99999958e-01 9.99920542e-01 9.99999970e-01 9.99999631e-01\n",
      "  9.99999726e-01 9.99999419e-01 9.99999945e-01 9.99999998e-01\n",
      "  9.99690787e-01 9.99998474e-01 9.99999992e-01 9.99999092e-01\n",
      "  1.00000000e+00 9.99999377e-01 9.99999999e-01 9.99995990e-01\n",
      "  1.00000000e+00 9.99999998e-01 9.99999389e-01 9.99999512e-01\n",
      "  9.99999999e-01 1.00000000e+00 9.99999991e-01 9.99671572e-01\n",
      "  9.99999357e-01 9.99996778e-01 9.99996079e-01 1.00000000e+00\n",
      "  9.99999995e-01 9.99999493e-01 9.99999971e-01 9.99999999e-01\n",
      "  9.99997845e-01 9.99999938e-01 9.99999998e-01 9.99999986e-01\n",
      "  9.99999503e-01 9.99958356e-01 9.99999954e-01 9.99998673e-01\n",
      "  9.99999650e-01 9.99999627e-01 9.93153411e-01 9.99999538e-01]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 加载鸢尾花模型参数进行预测\n",
    "iris_model_params = np.load(\"鸢尾花模型参数.npy\", allow_pickle=True).item()\n",
    "theta = iris_model_params['theta']\n",
    "bias = iris_model_params['bias']\n",
    "print(f\"theta: {theta}, bias: {bias}\")\n",
    "\n",
    "# 模型预测\n",
    "X,y = load_iris(return_X_y=True)\n",
    "X_new = X[:100]\n",
    "y_new = y[:100]\n",
    "\n",
    "# 计算准确率\n",
    "acc = np.mean(np.round(forward(X_new, theta, bias)) == y_new)\n",
    "print(f\"acc: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
