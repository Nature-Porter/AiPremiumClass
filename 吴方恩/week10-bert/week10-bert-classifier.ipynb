{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-22T07:04:31.384975Z",
     "iopub.status.busy": "2025-05-22T07:04:31.384678Z",
     "iopub.status.idle": "2025-05-22T07:04:32.580206Z",
     "shell.execute_reply": "2025-05-22T07:04:32.579575Z",
     "shell.execute_reply.started": "2025-05-22T07:04:31.384950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T07:05:40.461029Z",
     "iopub.status.busy": "2025-05-22T07:05:40.460713Z",
     "iopub.status.idle": "2025-05-22T07:05:51.825435Z",
     "shell.execute_reply": "2025-05-22T07:05:51.824862Z",
     "shell.execute_reply.started": "2025-05-22T07:05:40.461010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 Excel 并保存为 CSV\n",
    "df = pd.read_excel(\"/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx\")\n",
    "df.to_csv(\"jd_comment_data.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-22T07:04:38.272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/kaggle/working/jd_comment_data.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = [row for row in reader if row['评价内容(content)'] not in ['此用户未填写评价内容', '您没有填写内容，默认好评'] and row['评价内容(content)'].strip() != '']\n",
    "len(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T07:16:22.886414Z",
     "iopub.status.busy": "2025-05-22T07:16:22.886113Z",
     "iopub.status.idle": "2025-05-22T07:33:31.301797Z",
     "shell.execute_reply": "2025-05-22T07:33:31.301043Z",
     "shell.execute_reply.started": "2025-05-22T07:16:22.886392Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2455680255.py:84: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/5:   0%|          | 0/134 [00:00<?, ?it/s]/tmp/ipykernel_35/2455680255.py:96: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch freeze 1 completed. Avg loss: 0.8155, Val Accuracy: 0.9435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch freeze 2 completed. Avg loss: 0.3721, Val Accuracy: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch freeze 3 completed. Avg loss: 0.3259, Val Accuracy: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch freeze 4 completed. Avg loss: 0.3168, Val Accuracy: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch freeze 5 completed. Avg loss: 0.3136, Val Accuracy: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch finetune 1 completed. Avg loss: 0.2828, Val Accuracy: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch finetune 2 completed. Avg loss: 0.1986, Val Accuracy: 0.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch finetune 3 completed. Avg loss: 0.1762, Val Accuracy: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch finetune 4 completed. Avg loss: 0.1570, Val Accuracy: 0.9463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch finetune 5 completed. Avg loss: 0.1386, Val Accuracy: 0.9462\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "MODEL_PATH = '/kaggle/working/'\n",
    "PRETRAINED_MODEL = 'bert-base-chinese'\n",
    "LOG_DIR = './runs/bert_cls'\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "MAX_LEN = 128\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 数据集定义\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=MAX_LEN)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 加载并预处理数据\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = [row for row in reader if row['评价内容(content)'] not in ['此用户未填写评价内容', '您没有填写内容，默认好评'] and row['评价内容(content)'].strip() != '']\n",
    "    texts = [row['评价内容(content)'] for row in rows]\n",
    "    labels = [int(row['评分（总分5分）(score)']) - 1 for row in rows]  # 标签从0开始\n",
    "    split_idx = int(len(texts) * 0.8)\n",
    "    train_texts, train_labels = texts[:split_idx], labels[:split_idx]\n",
    "    test_texts, test_labels = texts[split_idx:], labels[split_idx:]\n",
    "    return train_texts, train_labels, test_texts, test_labels\n",
    "\n",
    "# 冻结 BERT 编码层\n",
    "def freeze_bert(model):\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# 验证函数\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == batch['labels']).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "    model.train()\n",
    "    return correct / total\n",
    "\n",
    "# 主训练函数\n",
    "def train(freeze=False):\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "    train_texts, train_labels, test_texts, test_labels = load_data('jd_comment_data.csv')\n",
    "\n",
    "    train_dataset = CommentDataset(train_texts, train_labels, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_dataset = CommentDataset(test_texts, test_labels, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=5)\n",
    "\n",
    "    if freeze:\n",
    "        freeze_bert(model)\n",
    "\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "    scaler = GradScaler()\n",
    "    writer = SummaryWriter(log_dir=LOG_DIR + ('/freeze' if freeze else '/finetune'))\n",
    "\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "        for batch in loop:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        torch.save(model.module.state_dict(), os.path.join(MODEL_PATH, f\"{'freeze' if freeze else 'finetune'}_bert_epoch{epoch+1}.pt\"))\n",
    "        # 在每个 epoch 后进行验证\n",
    "        val_acc = evaluate(model, test_loader)\n",
    "        writer.add_scalar(\"eval/accuracy\", val_acc, epoch)\n",
    "        print(f\"Epoch {'freeze' if freeze else 'finetune'} {epoch+1} completed. Avg loss: {total_loss / len(train_loader):.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 冻结 BERT 主体\n",
    "    train(freeze=True)   # 冻结只训练分类头\n",
    "    train(freeze=False)  # 不冻结，完整微调\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T07:38:10.016103Z",
     "iopub.status.busy": "2025-05-22T07:38:10.015332Z",
     "iopub.status.idle": "2025-05-22T07:38:11.132874Z",
     "shell.execute_reply": "2025-05-22T07:38:11.132257Z",
     "shell.execute_reply.started": "2025-05-22T07:38:10.016073Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论: 这款手机性价比很高，值得购买 -> 预测评分: 5 星\n",
      "评论: 包装很差，收到的时候已经坏了 -> 预测评分: 1 星\n",
      "评论: 物流挺快，客服也不错 -> 预测评分: 5 星\n",
      "评论: 太棒了，非常满意的一次购物 -> 预测评分: 5 星\n",
      "评论: 客服回复的太慢了，体验不太好 -> 预测评分: 5 星\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "\n",
    "MODEL_PATH = '/kaggle/working/finetune_bert_epoch5.pt'\n",
    "PRETRAINED_MODEL = 'bert-base-chinese'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载 tokenizer 和 模型\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=5)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def predict(texts):\n",
    "    encodings = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    encodings = {k: v.to(DEVICE) for k, v in encodings.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "    return preds.cpu().numpy().tolist()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_texts = [\n",
    "        \"这款手机性价比很高，值得购买\",\n",
    "        \"包装很差，收到的时候已经坏了\",\n",
    "        \"物流挺快，客服也不错\",\n",
    "        \"太棒了，非常满意的一次购物\",\n",
    "        \"客服回复的太慢了，体验不太好\"\n",
    "    ]\n",
    "    results = predict(test_texts)\n",
    "    for text, label in zip(test_texts, results):\n",
    "        print(f\"评论: {text} -> 预测评分: {label+1} 星\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 134082,
     "sourceId": 318737,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
