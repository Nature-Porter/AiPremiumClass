
欢迎来到经验时代 (Welcome to the Era of Experience)
David Silver, Richard S. Sutton

摘要 (Abstract)
我们正站在人工智能新纪元的门槛上，它有望实现前所未有的能力水平。新一代智能体（agent）将主要通过从经验中学习来获得超人的能力。本文探讨了定义这一即将到来的时代的关键特征。

人类数据时代 (The Era of Human Data)
近年来，人工智能（AI）通过在大量人类生成的数据上进行训练，并用人类专家的示例和偏好进行微调，取得了显著进步。大型语言模型（LLMs）就是这种方法的典范，它们实现了广泛的通用性。如今，单个 LLM 可以执行从写诗、解决物理问题到诊断医疗问题和总结法律文件等任务。

然而，虽然模仿人类足以在许多人类能力上达到称职水平，但这种方法本身尚未、并且很可能无法在许多重要主题和任务上实现超人类智能。在数学、编程和科学等关键领域，从人类数据中提取的知识正迅速接近极限。大多数高质量数据源——那些真正能提升强大智能体性能的——要么已被消耗殆尽，要么即将被耗尽。仅靠从人类数据中进行监督学习的驱动进展速度明显放缓，这表明需要一种新方法。此外，有价值的新见解，如新定理、技术或科学突破，存在于当前人类理解的边界之外，无法被现有的人类数据捕获。

经验时代 (The Era of Experience)
为了取得更重大的进展，需要一种新的数据来源。这种数据必须能够随着智能体变得更强而不断改进；任何静态的合成数据生成程序都将很快被超越。这可以通过允许智能体持续从自身经验中学习来实现，即由智能体与其环境互动产生的数据。AI 正处于一个新时期的开端，在这个时期，经验将成为改进的主导媒介，并最终使当今系统中使用的人类数据规模相形见绌。

这种转变可能已经开始，甚至对于以人类为中心的 AI 典范——大型语言模型也是如此。数学能力就是一个例子。AlphaProof[20] 最近成为首个在国际数学奥林匹克竞赛中获得奖牌的程序，超越了以人类为中心方法的表现[27, 19]。AlphaProof 最初接触了由人类数学家多年创建的约十万个形式化证明，随后其强化学习（RL）算法通过与形式化证明系统的持续互动，额外生成了超过一亿个证明。这种对交互经验的关注使 AlphaProof 能够探索超出既有形式化证明范围的数学可能性，从而发现解决新颖和挑战性问题的方法。非形式化数学也通过用自我生成的数据替代专家生成的数据取得了成功；例如，DeepSeek 最近的工作“强调了强化学习的力量和美感：我们不是明确地教导模型如何解决问题，而是简单地提供正确的激励，它就能自主地发展出高级的问题解决策略。”[10]

我们主张，一旦经验学习的全部潜力被发掘，将产生令人难以置信的新能力。这个经验时代的特征很可能是智能体和环境，除了从大量经验数据中学习外，还将在以下几个维度上突破以人类为中心的 AI 系统的限制：

   智能体将存在于经验流中，而非短暂的交互片段。
   它们的行动和观察将深深植根于环境，而非仅通过人类对话互动。
   它们的奖励将基于其在环境中的体验，而非来自人类的预先判断。
   它们将基于经验进行规划和/或推理，而非仅用人类术语推理。

我们相信，当今的技术，配合适当选择的算法，已经为实现这些突破提供了足够强大的基础。此外，AI 社区对这一议程的追求将刺激这些方向上的新创新，推动 AI 迅速迈向真正的超人类智能体。

经验流 (Streams)
经验型智能体可以在其整个生命周期中持续学习。在人类数据时代，基于语言的 AI 主要关注短暂的交互片段：例如，用户提问，智能体（也许经过几个思考步骤或工具使用动作）后回应。通常，很少或没有信息从一个片段传递到下一个片段，排除了任何随时间推移的适应。此外，智能体仅专注于当前片段内的结果，例如直接回答用户问题。相比之下，人类（和其他动物）存在于持续多年的行动和观察流中。信息在整个流中传递，行为会根据过去的经验进行自我修正和改进。此外，目标可以用延伸到经验流未来的行动和观察来指定。例如，人类可能会选择行动来实现改善健康、学习语言或实现科学突破等长期目标。

强大的智能体应该拥有自己的经验流，像人类一样在长时间尺度上推进。这将使智能体能够采取行动以实现未来目标，并随着时间的推移不断适应新的行为模式。例如，一个连接到用户可穿戴设备的健康和保健智能体可以监测数月内的睡眠模式、活动水平和饮食习惯。然后它可以提供个性化建议、鼓励，并根据长期趋势和用户特定的健康目标调整其指导。类似地，一个个性化教育智能体可以跟踪用户学习新语言的进度，识别知识差距，适应其学习风格，并在数月甚至数年内调整其教学方法。此外，一个科学智能体可以追求雄心勃勃的目标，例如发现新材料或减少二氧化碳。这样的智能体可以在较长时期内分析现实世界的观察结果，开发和运行模拟，并建议进行现实世界的实验或干预。

在每种情况下，智能体采取一系列步骤，以最大化相对于指定目标的长期成功。单个步骤可能不会带来任何直接好处，甚至在短期内可能有害，但总体上可能有助于长期成功。这与当前仅提供对请求即时响应、而无法衡量或优化其行为对环境未来后果的 AI 系统形成鲜明对比。

行动与观察 (Actions and Observations)
经验时代的智能体将在现实世界中自主行动。人类数据时代的 LLMs 主要侧重于人类特权的行动和观察：向用户输出文本，并将用户的文本输入回智能体。这与自然智能体（如动物）通过运动控制和传感器与环境互动的方式明显不同。虽然动物（尤其是人类）可能与其他动物交流，但这是通过与其他感觉运动控制相同的接口进行的，而非特权通道。

人们早已认识到 LLMs 也可以在数字世界中调用行动，例如通过调用 API（例如参见 [43]）。最初，这些能力主要来自工具使用的人类示例，而非智能体的经验。然而，编码和工具使用能力越来越依赖于执行反馈[17, 7, 12]，即智能体实际运行代码并观察发生的情况。最近，新一代原型智能体开始以更通用的方式与计算机交互，即使用人类操作计算机的相同接口[3, 15, 24]。这些变化预示着从完全人类特权的通信，向智能体能够在世界中独立行动的更自主互动的转变。这样的智能体将能够积极探索世界，适应不断变化的环境，并发现可能人类从未想到的策略。

这些更丰富的互动将提供一种自主理解和控制数字世界的手段。智能体可以使用“人类友好”的行动和观察（如用户界面），自然地促进与用户的沟通和协作。智能体也可以采取“机器友好”的行动来执行代码和调用 API，允许智能体自主行动以服务于其目标。在经验时代，智能体还将通过数字接口与现实世界互动。例如，一个科学智能体可以监测环境传感器、远程操作望远镜或控制实验室中的机械臂来自主进行实验。

奖励 (Rewards)
如果经验型智能体不仅能从人类偏好中学习，还能从外部事件和信号中学习，会怎样？

以人类为中心的 LLMs 通常优化基于人类预先判断的奖励：专家观察智能体的行动并决定其是否是好行动，或者在多个备选方案中挑选最佳的智能体行动。例如，专家可以评判健康智能体的建议、教育助手的教学或科学家智能体建议的实验。这些奖励或偏好是在行动后果发生前由人类决定的，而非衡量这些行动对环境的影响，这意味着它们并非直接植根于世界的现实。以这种方式依赖人类预先判断通常会给智能体的性能设置一个难以突破的上限：智能体无法发现人类评估者低估的更好策略。为了发现远超现有人类知识的新思想，有必要使用基于环境的奖励（grounded rewards）：即从环境本身产生的信号。例如，健康助手可以将用户的健康目标转化为基于静息心率、睡眠持续时间和活动水平等信号组合的奖励，而教育助手可以使用考试成绩为语言学习提供基于环境的奖励。同样，一个以减少全球变暖为目标的科学智能体可以使用基于二氧化碳水平经验观察的奖励，而发现更强材料的目标则可以基于材料模拟器测量值（如抗拉强度或杨氏模量）的组合。

基于环境的奖励也可能来自作为智能体环境一部分的人类。例如，人类用户可以报告蛋糕是否美味、锻炼后的疲劳程度或头痛的疼痛程度，使助手智能体能够提供更好的食谱、完善其健身建议或改进其推荐药物。这些奖励衡量的是智能体行动在其环境中的后果，最终应能提供比预先判断拟议蛋糕配方、锻炼计划或治疗方案的专家更好的帮助。

如果奖励不是来自人类数据，那来自哪里？一旦智能体通过丰富的行动和观察空间（见上文）连接到世界，将不乏基于环境的信号来提供奖励基础。事实上，世界充满了各种指标，如成本、错误率、饥饿感、生产力、健康指标、气候指标、利润、销售额、考试成绩、成功、访问量、产量、库存、点赞、收入、快乐/痛苦、经济指标、准确度、功率、距离、速度、效率或能源消耗。此外，还有无数来自特定事件发生或源自原始观察和行动序列特征的额外信号。

原则上，人们可以创建各种不同的智能体，每个智能体优化一个基于环境的信号作为其奖励。有观点认为，即使只有一个这样的奖励信号，只要得到高度有效的优化，也可能足以诱导出广泛的能力性智能[34]。这是因为在复杂环境中实现一个简单目标通常需要掌握多种多样的技能。

然而，追求单一奖励信号表面上似乎不符合通用 AI 的要求，即能够可靠地引导其实现任意用户期望的行为。那么，自主优化基于环境的、非人类的奖励信号是否与现代 AI 系统的要求相悖呢？我们认为不一定如此，通过概述一种可能满足这些要求的方法（其他方法也可能可行）来说明。

其思想是以用户引导的方式，基于环境信号灵活调整奖励。例如，奖励函数可以由一个神经网络定义，该网络将智能体与用户和环境的互动作为输入，输出一个标量奖励。这允许奖励根据用户目标选择或组合来自环境的信号。例如，用户可能指定一个宽泛的目标，如“改善我的健康”，奖励函数可能返回用户心率、睡眠持续时间和步数的函数。或者用户可能指定“帮助我学习西班牙语”的目标，奖励函数可以返回用户的西班牙语考试成绩。

此外，用户可以在学习过程中提供反馈，例如他们的满意度水平，这可用于微调奖励函数。然后，奖励函数可以随时间适应，改进其选择或组合信号的方式，并识别和纠正任何偏差。这也可以理解为一个双层优化过程：顶层优化用户反馈作为目标，底层优化来自环境的基于环境的信号。通过这种方式，少量的人类数据可以促进大量的自主学习。

规划与推理 (Planning and Reasoning)
经验时代会改变智能体的规划和推理方式吗？最近，在使用能够进行推理或用语言“思考”[23, 14, 10]的 LLMs 方面取得了显著进展，它们通过遵循思维链（chain of thought）然后输出响应[16]。从概念上讲，LLMs 可以充当通用计算机[30]：LLM 可以将 token 附加到其自身上下文中，允许其在输出最终结果之前执行任意算法。

在人类数据时代，这些推理方法被明确设计为模仿人类思维过程。例如，LLMs 被提示生成类人的思维链[16]，模仿人类思考轨迹[42]，或者强化与人类示例匹配的思维步骤[18]。推理过程可以进一步微调以产生与人类专家确定的正确答案相匹配的思考轨迹[44]。

然而，人类语言是否提供了通用计算机的最优实例，这非常值得怀疑。肯定存在更有效的思维机制，使用非人类语言，例如可能利用符号、分布式、连续或可微分计算的机制。一个自学习系统原则上可以通过从经验中学习如何思考来发现或改进此类方法。例如，AlphaProof 学会了以与人类数学家截然不同的方式形式化证明复杂定理[20]。

此外，通用计算机的原理仅涉及智能体的内部计算；它并未将其连接到外部世界的现实。一个被训练来模仿人类思想甚至匹配专家答案的智能体，可能会继承数据中根深蒂固的错误思维方法，例如有缺陷的假设或固有偏见。例如，如果一个智能体被训练使用 5000 年前的人类思想和专家答案进行推理，它可能会用万物有灵论来推理一个物理问题；1000 年前它可能会用神学术语推理；300 年前它可能会用牛顿力学推理；50 年前则用量子力学推理。超越每一种思维方法都需要与现实世界互动：提出假设、进行实验、观察结果并相应地更新原则。同样，智能体必须基于真实世界的数据才能推翻错误的思维方法。这种基于环境提供了反馈循环，允许智能体根据现实检验其继承的假设，并发现不受当前主导人类思维模式限制的新原则。没有这种基于环境，智能体无论多么复杂，都将成为现有人类知识的回音室。为了超越这一点，智能体必须积极与世界互动，收集观察数据，并使用该数据迭代地完善其理解，这在很大程度上反映了推动人类科学进步的进程。

将思维直接植根于外部世界的一种可能方法是构建一个世界模型（world model）[37]，该模型预测智能体行动对世界的后果，包括预测奖励。例如，健康助手可能会考虑推荐当地的健身房或健康播客。智能体的世界模型可能会预测用户的静息心率或睡眠模式在此行动后可能如何变化，以及预测与用户的未来对话。这使得智能体能够直接根据其自身行动及其对世界的因果效应进行规划[36, 29]。随着智能体在其经验流中持续与世界互动，其动力学模型会不断更新以纠正预测中的任何错误。给定一个世界模型，智能体可以应用可扩展的规划方法来提高其预测性能。

规划方法和推理方法并不相互排斥：智能体可以在规划过程中应用内部的 LLM 计算来选择每个行动，或者模拟和评估这些行动的后果。

为何是现在？ (Why Now?)
从经验中学习并非新鲜事。强化学习系统先前已经掌握了大量在模拟器中呈现、具有明确奖励信号的复杂任务（大致对应于图1中的“模拟时代”）。例如，RL 方法在双陆棋[39]、围棋[31]、国际象棋[32]、扑克[22, 6]和 Stratego[26]等棋盘游戏中通过自我对弈达到或超越了人类水平；在 Atari[21]、星际争霸 II[40]、Dota 2[4]和 Gran Turismo[41]等视频游戏中；在魔方[1]等灵巧操作任务中；以及在数据中心冷却[13]等资源管理任务中。此外，像 AlphaZero[33] 这样强大的 RL 智能体，在神经网络规模、交互经验数量和思考时间长度方面展现出令人印象深刻且可能无限的扩展性。然而，基于此范式的智能体未能跨越模拟（具有单一、精确定义奖励的封闭问题）与现实（具有多个看似定义不清的奖励的开放问题）之间的鸿沟。



人类数据时代提供了一个有吸引力的解决方案。海量的人类数据语料库包含了针对极其多样化任务的自然语言示例。在此数据上训练的智能体，与模拟时代较为狭隘的成功相比，实现了广泛的能力。因此，经验型 RL 的方法论在很大程度上被抛弃，转而支持更通用的智能体，导致了向以人类为中心的 AI 的广泛转变。

然而，在这种转变中失去了一些东西：智能体自我发现知识的能力。例如，AlphaZero 发现了国际象棋和围棋的根本新策略，改变了人类玩这些游戏的方式[28, 45]。经验时代将把这种能力与人类数据时代实现的任务通用性水平相融合。正如上文所述，当智能体能够在现实世界经验流中自主行动和观察[11]，并且奖励可以灵活地连接到大量基于环境的、现实世界的信号中的任何一个时，这就成为可能。能够在复杂的现实世界行动空间中交互的自主智能体[3, 15, 24]的出现，以及能够在丰富的推理空间中解决开放式问题的强大 RL 方法[20, 10]，表明向经验时代的转变已近在眼前。

强化学习方法 (Reinforcement Learning Methods)
强化学习（RL）有着悠久的历史，深深植根于自主学习，即智能体通过直接与环境互动为自己学习。早期的 RL 研究产生了一套强大的概念和算法。例如，时序差分学习（temporal difference learning）[35] 使智能体能够估计未来奖励，带来了如双陆棋中超人类表现[39]等突破。由乐观或好奇心驱动的探索技术被开发出来，帮助智能体发现创造性的新行为并避免陷入次优的常规[2]。像 Dyna 算法这样的方法使智能体能够构建和学习其世界模型，让它们规划和推理未来行动[36, 29]。选项（Options）和选项内/间学习（inter/intra-option learning） 等概念促进了时间抽象化（temporal abstraction），使智能体能够在更长的时间尺度上推理，并将复杂任务分解为可管理的子目标[38]。

然而，以人类为中心的 LLMs 的兴起，将焦点从自主学习转移到了利用人类知识上。像 RLHF（基于人类反馈的强化学习）[9, 25] 这样的技术，以及使语言模型与人类推理对齐的方法[44]被证明非常有效，推动了 AI 能力的快速发展。这些方法虽然强大，但常常绕过了核心的 RL 概念：RLHF 通过调用人类专家代替机器估计值，绕过了对价值函数的需求；来自人类数据的强先验减少了对探索的依赖；用人类中心术语进行推理降低了对世界模型和时间抽象化的需求。

然而，可以认为这种范式的转变是“倒洗澡水把孩子也倒掉了”。虽然以人类为中心的 RL 实现了前所未有的行为广度，但它也给智能体的性能设置了新的上限：智能体无法超越现有人类知识。此外，人类数据时代主要关注为短期的、脱离环境的、人类交互片段设计的 RL 方法，这些方法不适合长期的、基于环境的自主交互流。

经验时代提供了一个重新审视和改进经典 RL 概念的机会。这个时代将带来思考奖励函数的新方式，这些函数将灵活地基于观察数据。它将重新审视价值函数以及从未完整的长序列流中估计它们的方法。它将带来有原则且实用的现实世界探索方法，以发现与人类先验完全不同的新行为。将开发新颖的世界模型方法以捕捉基于环境互动的复杂性。时间抽象化的新方法将使智能体能够基于经验在更长的时间范围（time horizons） 上进行推理。通过在 RL 的基础上构建，并将其核心原则适应新时代的挑战，我们可以释放自主学习的全部潜力，为真正超人类智能铺平道路。

后果 (Consequences)
经验时代的到来——AI 智能体从与世界互动中学习——预示着一个与我们以前所见截然不同的未来。这种新范式虽然提供了巨大的潜力，但也带来了需要仔细考虑的重要风险和挑战，包括但不限于以下几点。

积极的一面，经验学习将解锁前所未有的能力。在日常生活中，个性化助手将利用连续的经验流，在数月或数年内适应个人的健康、教育或专业需求，以实现长期目标。也许最具变革性的将是科学发现的加速。AI 智能体将自主地在材料科学、医学或硬件设计等领域设计和进行实验。通过持续从自身实验结果中学习，这些智能体可以迅速探索新的知识前沿，以前所未有的速度开发新材料、药物和技术。

然而，这个新时代也带来了重大且新颖的挑战。虽然人类能力的自动化有望提高生产力，但这些改进也可能导致工作岗位流失。智能体甚至可能展现出以前被认为是人类专属领域的能力，如长期问题解决、创新以及对现实世界后果的深刻理解。

此外，虽然对任何 AI 的潜在滥用都存在普遍担忧，但智能体能够在长时间内自主与世界互动以实现长期目标，可能会带来更高的风险。默认情况下，这为人类干预和调解智能体行为提供了更少的机会，因此需要高度的信任和责任标准。远离人类数据和人类思维模式也可能使未来的 AI 系统更难以解释（interpret）。

然而，虽然承认经验学习会增加某些安全风险，并且肯定需要进一步研究以确保安全过渡到经验时代，但我们也应该认识到它也可能带来一些重要的安全益处。

首先，一个经验型智能体感知它所处的环境，其行为可以随着时间推移适应环境的变化。任何预编程的系统，包括固定的 AI 系统，都可能意识不到其环境背景，并在部署后变得不适应变化的世界。例如，关键硬件可能出现故障，大流行可能导致社会快速变化，或新的科学发现可能引发一系列快速的技术发展。相比之下，经验型智能体可以观察并学会规避故障硬件，适应快速的社会变化，或者拥抱并建立在新科学和技术之上。也许更重要的是，智能体可以识别其行为何时引发人类的担忧、不满或痛苦，并自适应地修改其行为以避免这些负面后果。

其次，智能体的奖励函数本身可以通过经验进行调整，例如使用前文描述的双层优化（参见奖励部分）。重要的是，这意味着错位（misaligned） 的奖励函数通常可以通过试错随时间逐步纠正。例如，奖励函数可以基于人类担忧的迹象进行修改，而不是盲目地优化某个信号（如最大化回形针[5]），直到回形针生产耗尽地球资源。这类似于人类为彼此设定目标的方式，如果观察到人们钻系统空子、忽视长期福祉或导致不良负面后果，就会调整这些目标；不过，也像人类目标设定一样，无法保证完美对齐。

最后，依赖于物理经验的进步本质上受到在现实世界执行行动和观察其后果所需时间的限制。例如，新药的开发，即使有 AI 辅助设计，仍然需要无法一夜完成的现实世界试验。这可能为潜在的 AI 自我改进速度提供了一个自然的制动器。

结论 (Conclusion)
经验时代标志着 AI 演进的一个关键时刻。在当今坚实的基础上，超越人类衍生数据的限制，智能体将越来越多地从自身与世界的互动中学习。智能体将通过丰富的观察和行动与环境自主互动。它们将在终身的经验流中持续适应。它们的目标可被引导指向任何基于环境的信号组合。此外，智能体将利用强大的非人类推理，并构建植根于智能体行动对其环境后果的计划。最终，经验数据将在规模和质量上超越人类生成的数据。伴随着 RL 算法的进步，这种范式转变将在许多领域开启超越人类任何个体所拥有的新能力。

致谢 (Acknowledgements)
作者感谢 Thomas Degris, Rohin Shah, Tom Schaul 和 Hado van Hasselt 的有益评论和讨论。

