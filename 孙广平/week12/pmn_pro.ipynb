{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport pickle\n\nfrom transformers import (\n    AutoModelForTokenClassification, \n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    TrainingArguments, \n    Trainer,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset\nimport evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_prepare_data():\n    \"\"\"加载并预处理数据集\"\"\"\n    # 加载数据集\n    ds = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n\n    # 直接从数据集中提取已有标签（避免与原数据不一致）\n    tag_list = [\n        'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'\n    ]\n    # 提取实体类型 PER/ORG/LOC 及 O\n    entity_types = ['O'] + sorted(list(set(tag.split('-')[-1] for tag in tag_list if tag != 'O')))\n\n    entity_index = {entity: i for i, entity in enumerate(entity_types)}\n    \n    return ds, tag_list, entity_index\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_entity_tags(ds, tag_list):\n    \"\"\"转换标签\"\"\"\n    tag2id = {tag: i for i, tag in enumerate(tag_list)}\n    \n    def convert_tags(example):\n        # 将标签字符串转换为对应数字\n        example['labels'] = [tag2id[tag] for tag in example['ner_tags']]\n        return example\n\n    return ds.map(convert_tags)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_data(ds, tokenizer):\n    \"\"\"使用 tokenizer 对 token 序列进行编码，并对标签进行对齐\"\"\"\n\n    def tokenize_and_align_labels(example):\n        # tokenizer 的输入是 tokens\n        tokenized_input = tokenizer(\n            example[\"tokens\"],\n            is_split_into_words=True,\n            truncation=True,\n            max_length=512,\n            padding='max_length'\n        )\n        \n        word_ids = tokenized_input.word_ids()\n        aligned_labels = []\n        label_ids = example[\"labels\"]\n\n        for word_id in word_ids:\n            if word_id is None:\n                aligned_labels.append(-100)  # 用于忽略 loss\n            else:\n                aligned_labels.append(label_ids[word_id])\n\n        tokenized_input[\"labels\"] = aligned_labels\n        return tokenized_input\n\n    return ds.map(tokenize_and_align_labels)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model_and_optimizer(tags, learning_rates=None):\n    \"\"\"创建模型和优化器\"\"\"\n    if learning_rates is None:\n        learning_rates = {'bert': 1e-5, 'classifier': 1e-3}\n    \n    # 创建标签映射\n    id2lbl = {i: tag for i, tag in enumerate(tags)}\n    lbl2id = {tag: i for i, tag in enumerate(tags)}\n    \n    # 加载模型\n    model = AutoModelForTokenClassification.from_pretrained(\n        'google-bert/bert-base-chinese',\n        num_labels=len(tags),\n        id2label=id2lbl,\n        label2id=lbl2id\n    )\n    \n    # 参数分组\n    param_optimizer = list(model.named_parameters())\n    bert_params = [params for name, params in param_optimizer if 'bert' in name]\n    classifier_params = [params for name, params in param_optimizer if 'bert' not in name]\n    \n    param_groups = [\n        {'params': bert_params, 'lr': learning_rates['bert']},\n        {'params': classifier_params, 'weight_decay': 0.1, 'lr': learning_rates['classifier']}\n    ]\n    \n    optimizer = optim.AdamW(param_groups)\n    return model, optimizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_manual(model, optimizer, train_dl, epochs, use_amp=False):\n    \"\"\"训练循环\"\"\"\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    \n    # 学习率调度器\n    train_steps = len(train_dl) * epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=100,\n        num_training_steps=train_steps\n    )\n    \n    # 混合精度训练\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n    \n    for epoch in range(epochs):\n        model.train()\n        pbar = tqdm(train_dl, desc=f'Epoch {epoch+1}/{epochs}')\n        \n        for items in pbar:\n            items = {k: v.to(device) for k, v in items.items()}\n            optimizer.zero_grad()\n            \n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    outputs = model(**items)\n                loss = outputs.loss\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(**items)\n                loss = outputs.loss\n                loss.backward()\n                optimizer.step()\n            \n            scheduler.step()\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    save_model(model, tags, \"ner_manual_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model(model, tags, save_dir):\n    \"\"\"保存模型和配置\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # 保存模型权重\n    torch.save(model.state_dict(), os.path.join(save_dir, \"model_weights.pth\"))\n    \n    # 保存标签映射\n    label_config = {\n        'tags': tags,\n        'id2label': {i: tag for i, tag in enumerate(tags)},\n        'label2id': {tag: i for i, tag in enumerate(tags)}\n    }\n    with open(os.path.join(save_dir, \"label_config.pkl\"), \"wb\") as f:\n        pickle.dump(label_config, f)\n    \n    print(f\"模型已保存到: {save_dir}\")\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model(save_dir):\n    \"\"\"加载保存的模型\"\"\"\n    # 加载标签配置\n    with open(os.path.join(save_dir, \"label_config.pkl\"), \"rb\") as f:\n        label_config = pickle.load(f)\n    \n    tags = label_config['tags']\n    id2label = label_config['id2label']\n    label2id = label_config['label2id']\n    \n    # 创建模型\n    model = AutoModelForTokenClassification.from_pretrained(\n        'google-bert/bert-base-chinese',\n        num_labels=len(tags),\n        id2label=id2label,\n        label2id=label2id\n    )\n    \n    # 加载权重\n    model.load_state_dict(torch.load(os.path.join(save_dir, \"model_weights.pth\")))\n    \n    # 加载分词器\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    \n    return model, tokenizer, tags\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_entities(model, tokenizer, tags, text):\n    \"\"\"推理函数\"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    \n    # 处理输入文本\n    chars = list(text)\n    inputs = tokenizer(\n        chars,\n        return_tensors='pt',\n        is_split_into_words=True,\n        padding=True,\n        truncation=True,\n        max_length=512\n    )\n    \n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # 预测\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n    \n    # 解析结果\n    predicted_labels = [tags[pred.item()] for pred in predictions[0]]\n    \n    # 提取实体\n    entities = []\n    current_entity = None\n    \n    for i, (char, label) in enumerate(zip(chars, predicted_labels)):\n        if label.startswith('B-'):\n            if current_entity:\n                entities.append(current_entity)\n            current_entity = {\n                'text': char,\n                'label': label[2:],\n                'start': i,\n                'end': i + 1\n            }\n        elif label.startswith('I-') and current_entity and label[2:] == current_entity['label']:\n            current_entity['text'] += char\n            current_entity['end'] = i + 1\n        else:\n            if current_entity:\n                entities.append(current_entity)\n                current_entity = None\n    \n    if current_entity:\n        entities.append(current_entity)\n    \n    return entities","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    \"\"\"计算评估指标\"\"\"\n    seqeval = evaluate.load('seqeval')\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=2)\n    \n    # 获取标签映射\n    _, tags, _ = load_and_prepare_data()\n    \n    # 准备评估数据\n    true_predictions = [\n        [tags[p] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [tags[l] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    return seqeval.compute(predictions=true_predictions, references=true_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference_demo():\n    \"\"\"推理演示\"\"\"\n    try:\n        model, tokenizer, tags = load_model(\"ner_manual_model\")\n        model.eval()\n        \n        # 测试文本\n        test_texts = [\n            \"李四在北京大学学习人工智能\",\n            \"张三在腾讯公司工作\",\n            \"王二喜欢看动漫《剑来》\"\n        ]\n        \n        for text in test_texts:\n            print(f\"\\n输入文本: {text}\")\n            entities = predict_entities(model, tokenizer, tags, text)\n            print(\"识别的实体:\")\n            for entity in entities:\n                print(f\"  - {entity['text']} ({entity['label']}) [{entity['start']}:{entity['end']}]\")\n    \n    except FileNotFoundError:\n        print(\"未找到保存的模型，请先进行训练！\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds, tags, entity_index = load_and_prepare_data()\nds1 = process_entity_tags(ds, entity_index)\ntokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\nds2 = tokenize_data(ds1, tokenizer)\nmodel, optimizer = create_model_and_optimizer(tags)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 训练\nds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\ntrain_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\ntrain_manual(model, optimizer, train_dl, epochs=1, use_amp=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 模型推理演示\ninference_demo()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}