{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 数据集\n",
    "ds = load_dataset('nlhappy/CLUE-NER')\n",
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "            'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "def entity_tags_proc(item):\n",
    "    # item即是dataset中记录\n",
    "    text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "    tags = [0] * text_len    # 初始值为‘O’\n",
    "    # 遍历实体列表，所有实体类别标记填入tags\n",
    "    entites = item['ents']\n",
    "    for ent in entites:\n",
    "        indices = ent['indices']  # 实体索引\n",
    "        label = ent['label']   # 实体名\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "    return {'ent_tag': tags}\n",
    "\n",
    "# 使用自定义回调函数处理数据集记录\n",
    "ds1 = ds.map(entity_tags_proc)\n",
    "\n",
    "def data_input_proc(item):\n",
    "    # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "    batch_texts = [list(text) for text in item['text']]\n",
    "    # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "    input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n",
    "                            is_split_into_words=True, padding='max_length')\n",
    "    input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "    return input_data\n",
    "    \n",
    "\n",
    "ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n",
    "\n",
    "\n",
    "local_rank = torch.cuda.device_count()\n",
    "\n",
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                        num_labels=21,\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model.to(local_rank)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "    num_train_epochs = 3,    # 训练 epoch\n",
    "    save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "    per_device_train_batch_size=16,  # 训练批次\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to='tensorboard',  # 训练输出记录\n",
    "    eval_strategy=\"epoch\",\n",
    "    local_rank=local_rank,   # 当前进程 RANK\n",
    "    fp16=True,               # 使用混合精度\n",
    "    lr_scheduler_type='linear',  # 动态学习率\n",
    "    warmup_steps=100,        # 预热步数\n",
    "    ddp_find_unused_parameters=False  # 优化DDP性能\n",
    ")\n",
    "\n",
    "def compute_metric(result):\n",
    "    # result 是一个tuple (predicts, labels)\n",
    "    \n",
    "    # 获取评估对象\n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predicts,labels = result\n",
    "    predicts = np.argmax(predicts, axis=2)\n",
    "    \n",
    "    # 准备评估数据\n",
    "    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                    for ps,ls in zip(predicts,labels)]\n",
    "    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                    for ps,ls in zip(predicts,labels)]\n",
    "    results = seqeval.compute(predictions=predicts, references=labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds2['train'],\n",
    "    eval_dataset=ds2['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 数据集\n",
    "ds = load_dataset('nlhappy/CLUE-NER')\n",
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "            'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "def entity_tags_proc(item):\n",
    "    # item即是dataset中记录\n",
    "    text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "    tags = [0] * text_len    # 初始值为‘O’\n",
    "    # 遍历实体列表，所有实体类别标记填入tags\n",
    "    entites = item['ents']\n",
    "    for ent in entites:\n",
    "        indices = ent['indices']  # 实体索引\n",
    "        label = ent['label']   # 实体名\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "    return {'ent_tag': tags}\n",
    "\n",
    "# 使用自定义回调函数处理数据集记录\n",
    "ds1 = ds.map(entity_tags_proc)\n",
    "\n",
    "def data_input_proc(item):\n",
    "    # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "    batch_texts = [list(text) for text in item['text']]\n",
    "    # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "    input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n",
    "                            is_split_into_words=True, padding='max_length')\n",
    "    input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "    return input_data\n",
    "    \n",
    "\n",
    "ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n",
    "\n",
    "\n",
    "local_rank = torch.cuda.device_count()\n",
    "\n",
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                        num_labels=21,\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model.to(local_rank)\n",
    "# 损失函数及优化器\n",
    "criterion = nn.CrossEntropyLoss().to(rank)\n",
    "optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(10):\n",
    "    ddp_model.train()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(rank), labels.to(rank)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(ddp_model.parameters(), 1)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
