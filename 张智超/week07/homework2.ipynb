{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11445582,"sourceType":"datasetVersion","datasetId":7170435},{"sourceId":11449571,"sourceType":"datasetVersion","datasetId":7173485}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:16:00.767378Z","iopub.execute_input":"2025-04-17T12:16:00.767630Z","iopub.status.idle":"2025-04-17T12:16:01.116829Z","shell.execute_reply.started":"2025-04-17T12:16:00.767608Z","shell.execute_reply":"2025-04-17T12:16:01.116117Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/douban-comments/vocab.pkl\n/kaggle/input/douban-comments/comments_list.pkl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"#### 2. 加载处理后文本构建词典、定义模型、训练、评估、测试。","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:58:05.871081Z","iopub.execute_input":"2025-04-18T03:58:05.871350Z","iopub.status.idle":"2025-04-18T03:58:05.875411Z","shell.execute_reply.started":"2025-04-18T03:58:05.871327Z","shell.execute_reply":"2025-04-18T03:58:05.874633Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 加载数据\ncomments_list = pickle.load(open('/kaggle/input/douban-comments/comments_list.pkl', 'rb'))\nvocab = pickle.load(open('/kaggle/input/douban-comments/vocab.pkl', 'rb'))\nvocab_size = len(vocab)\nprint(len(comments_list))\nprint(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:58:14.320010Z","iopub.execute_input":"2025-04-18T03:58:14.320502Z","iopub.status.idle":"2025-04-18T03:58:25.166592Z","shell.execute_reply.started":"2025-04-18T03:58:14.320479Z","shell.execute_reply":"2025-04-18T03:58:25.165834Z"}},"outputs":[{"name":"stdout","text":"1220249\n278670\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def collate_fn(batch_data):\n    comments, labels = [], []\n    for comment, label in batch_data:\n        # 转为分词的索引\n        comments.append(torch.tensor([vocab.get(word, vocab['UNK']) for word in comment]))\n        labels.append(label)\n    comments = nn.utils.rnn.pad_sequence(comments, batch_first=True, padding_value=vocab['PAD'])\n    labels = torch.tensor(labels)\n    return comments, labels\n# 加载训练数据：使用自定义函数对齐批次的seq长度\ntrain_list, test_list = train_test_split(comments_list, test_size=0.2, random_state=42)\ntrain_dataLoader = DataLoader(train_list, batch_size=128, shuffle=True, collate_fn=collate_fn)\ntest_dataLoader = DataLoader(test_list, batch_size=128, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:58:29.640102Z","iopub.execute_input":"2025-04-18T03:58:29.640907Z","iopub.status.idle":"2025-04-18T03:58:30.093406Z","shell.execute_reply.started":"2025-04-18T03:58:29.640868Z","shell.execute_reply":"2025-04-18T03:58:30.092609Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 定义模型\nclass Comment_RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n    def forward(self, input_index):\n        embed = self.embedding(input_index)\n        out, (h, _) = self.rnn(embed)\n        out = self.fc(out[:, -1, :])\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:58:38.754533Z","iopub.execute_input":"2025-04-18T03:58:38.755392Z","iopub.status.idle":"2025-04-18T03:58:38.760232Z","shell.execute_reply.started":"2025-04-18T03:58:38.755357Z","shell.execute_reply":"2025-04-18T03:58:38.759585Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nembedding_dim = 100\nhidden_size = 128\nnum_layers = 2\nnum_classes = 2\nmodel = Comment_RNN(vocab_size, embedding_dim, hidden_size, num_layers, num_classes)\nmodel.to(device)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:58:41.588325Z","iopub.execute_input":"2025-04-18T03:58:41.589053Z","iopub.status.idle":"2025-04-18T03:58:44.750782Z","shell.execute_reply.started":"2025-04-18T03:58:41.589030Z","shell.execute_reply":"2025-04-18T03:58:44.750201Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 训练\nwriter = SummaryWriter(log_dir='/kaggle/working/runs/jieba')\nmodel.train()\nfor epoch in range(10):\n    for i, (comments, labels) in enumerate(train_dataLoader):\n        comments = comments.to(device)\n        labels = labels.to(device)\n        # 前向传播\n        outputs = model(comments)\n        loss = loss_fn(outputs, labels)\n        # 反向传播和优化\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        writer.add_scalar('train loss', loss.item(), len(train_dataLoader) * epoch + i)\n        if (i+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}], Step [{i+1}], Loss: {loss.item()}')\nwriter.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T03:58:51.913696Z","iopub.execute_input":"2025-04-18T03:58:51.914406Z","iopub.status.idle":"2025-04-18T04:18:20.544995Z","shell.execute_reply.started":"2025-04-18T03:58:51.914381Z","shell.execute_reply":"2025-04-18T04:18:20.544409Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch [1], Step [100], Loss: 0.5420422554016113\nEpoch [1], Step [200], Loss: 0.5845947861671448\nEpoch [1], Step [300], Loss: 0.5253560543060303\nEpoch [1], Step [400], Loss: 0.4294927418231964\nEpoch [1], Step [500], Loss: 0.48426496982574463\nEpoch [1], Step [600], Loss: 0.5418112874031067\nEpoch [1], Step [700], Loss: 0.46136951446533203\nEpoch [1], Step [800], Loss: 0.44212841987609863\nEpoch [1], Step [900], Loss: 0.4939717650413513\nEpoch [1], Step [1000], Loss: 0.4840162694454193\nEpoch [1], Step [1100], Loss: 0.5154366493225098\nEpoch [1], Step [1200], Loss: 0.464128315448761\nEpoch [1], Step [1300], Loss: 0.515199601650238\nEpoch [1], Step [1400], Loss: 0.47719088196754456\nEpoch [1], Step [1500], Loss: 0.5366746783256531\nEpoch [1], Step [1600], Loss: 0.49447524547576904\nEpoch [1], Step [1700], Loss: 0.677943766117096\nEpoch [1], Step [1800], Loss: 0.5544470548629761\nEpoch [1], Step [1900], Loss: 0.5833597183227539\nEpoch [1], Step [2000], Loss: 0.4743793308734894\nEpoch [1], Step [2100], Loss: 0.5456831455230713\nEpoch [1], Step [2200], Loss: 0.5320135951042175\nEpoch [1], Step [2300], Loss: 0.4271939694881439\nEpoch [1], Step [2400], Loss: 0.42046481370925903\nEpoch [1], Step [2500], Loss: 0.2871131896972656\nEpoch [1], Step [2600], Loss: 0.39829346537590027\nEpoch [1], Step [2700], Loss: 0.3167610168457031\nEpoch [1], Step [2800], Loss: 0.31535103917121887\nEpoch [1], Step [2900], Loss: 0.40920981764793396\nEpoch [1], Step [3000], Loss: 0.40872418880462646\nEpoch [1], Step [3100], Loss: 0.3612438440322876\nEpoch [1], Step [3200], Loss: 0.3353590667247772\nEpoch [1], Step [3300], Loss: 0.2793598771095276\nEpoch [1], Step [3400], Loss: 0.2547703683376312\nEpoch [1], Step [3500], Loss: 0.2560940384864807\nEpoch [1], Step [3600], Loss: 0.17872396111488342\nEpoch [1], Step [3700], Loss: 0.3798697888851166\nEpoch [1], Step [3800], Loss: 0.25692883133888245\nEpoch [1], Step [3900], Loss: 0.21121451258659363\nEpoch [1], Step [4000], Loss: 0.26956507563591003\nEpoch [1], Step [4100], Loss: 0.4153706729412079\nEpoch [1], Step [4200], Loss: 0.26017749309539795\nEpoch [1], Step [4300], Loss: 0.19302956759929657\nEpoch [1], Step [4400], Loss: 0.36341726779937744\nEpoch [1], Step [4500], Loss: 0.25712651014328003\nEpoch [1], Step [4600], Loss: 0.2413659691810608\nEpoch [1], Step [4700], Loss: 0.2528928518295288\nEpoch [1], Step [4800], Loss: 0.27623528242111206\nEpoch [1], Step [4900], Loss: 0.26050829887390137\nEpoch [1], Step [5000], Loss: 0.257253497838974\nEpoch [1], Step [5100], Loss: 0.21902886033058167\nEpoch [1], Step [5200], Loss: 0.18904782831668854\nEpoch [1], Step [5300], Loss: 0.2984464168548584\nEpoch [1], Step [5400], Loss: 0.3003526031970978\nEpoch [1], Step [5500], Loss: 0.3256339430809021\nEpoch [1], Step [5600], Loss: 0.2576996386051178\nEpoch [1], Step [5700], Loss: 0.23319797217845917\nEpoch [1], Step [5800], Loss: 0.26158589124679565\nEpoch [1], Step [5900], Loss: 0.2346711903810501\nEpoch [1], Step [6000], Loss: 0.1956709325313568\nEpoch [1], Step [6100], Loss: 0.2014564424753189\nEpoch [1], Step [6200], Loss: 0.2674766480922699\nEpoch [1], Step [6300], Loss: 0.30690646171569824\nEpoch [1], Step [6400], Loss: 0.22673754394054413\nEpoch [1], Step [6500], Loss: 0.23333904147148132\nEpoch [1], Step [6600], Loss: 0.18221397697925568\nEpoch [1], Step [6700], Loss: 0.2335754781961441\nEpoch [1], Step [6800], Loss: 0.27888041734695435\nEpoch [1], Step [6900], Loss: 0.19870559871196747\nEpoch [1], Step [7000], Loss: 0.32765376567840576\nEpoch [1], Step [7100], Loss: 0.19642315804958344\nEpoch [1], Step [7200], Loss: 0.1960970014333725\nEpoch [1], Step [7300], Loss: 0.18035346269607544\nEpoch [1], Step [7400], Loss: 0.2185353934764862\nEpoch [1], Step [7500], Loss: 0.2140287309885025\nEpoch [1], Step [7600], Loss: 0.21284601092338562\nEpoch [2], Step [100], Loss: 0.21893416345119476\nEpoch [2], Step [200], Loss: 0.24973493814468384\nEpoch [2], Step [300], Loss: 0.24419322609901428\nEpoch [2], Step [400], Loss: 0.19748638570308685\nEpoch [2], Step [500], Loss: 0.2773236930370331\nEpoch [2], Step [600], Loss: 0.22605079412460327\nEpoch [2], Step [700], Loss: 0.22388245165348053\nEpoch [2], Step [800], Loss: 0.18700368702411652\nEpoch [2], Step [900], Loss: 0.3329506814479828\nEpoch [2], Step [1000], Loss: 0.20165051519870758\nEpoch [2], Step [1100], Loss: 0.3067754805088043\nEpoch [2], Step [1200], Loss: 0.2579467296600342\nEpoch [2], Step [1300], Loss: 0.2829117476940155\nEpoch [2], Step [1400], Loss: 0.30260127782821655\nEpoch [2], Step [1500], Loss: 0.28444820642471313\nEpoch [2], Step [1600], Loss: 0.2945348024368286\nEpoch [2], Step [1700], Loss: 0.2544950246810913\nEpoch [2], Step [1800], Loss: 0.21157705783843994\nEpoch [2], Step [1900], Loss: 0.18884970247745514\nEpoch [2], Step [2000], Loss: 0.20255564153194427\nEpoch [2], Step [2100], Loss: 0.21070854365825653\nEpoch [2], Step [2200], Loss: 0.2474047839641571\nEpoch [2], Step [2300], Loss: 0.198502779006958\nEpoch [2], Step [2400], Loss: 0.1905384212732315\nEpoch [2], Step [2500], Loss: 0.19125697016716003\nEpoch [2], Step [2600], Loss: 0.20036032795906067\nEpoch [2], Step [2700], Loss: 0.1962711364030838\nEpoch [2], Step [2800], Loss: 0.30258089303970337\nEpoch [2], Step [2900], Loss: 0.2263776808977127\nEpoch [2], Step [3000], Loss: 0.23506516218185425\nEpoch [2], Step [3100], Loss: 0.1758616417646408\nEpoch [2], Step [3200], Loss: 0.1949882060289383\nEpoch [2], Step [3300], Loss: 0.23583610355854034\nEpoch [2], Step [3400], Loss: 0.2564055025577545\nEpoch [2], Step [3500], Loss: 0.16653130948543549\nEpoch [2], Step [3600], Loss: 0.23336222767829895\nEpoch [2], Step [3700], Loss: 0.19342589378356934\nEpoch [2], Step [3800], Loss: 0.22091801464557648\nEpoch [2], Step [3900], Loss: 0.17071768641471863\nEpoch [2], Step [4000], Loss: 0.1875065267086029\nEpoch [2], Step [4100], Loss: 0.2093438059091568\nEpoch [2], Step [4200], Loss: 0.27283477783203125\nEpoch [2], Step [4300], Loss: 0.15110082924365997\nEpoch [2], Step [4400], Loss: 0.1667824238538742\nEpoch [2], Step [4500], Loss: 0.2658589482307434\nEpoch [2], Step [4600], Loss: 0.22847880423069\nEpoch [2], Step [4700], Loss: 0.2460549920797348\nEpoch [2], Step [4800], Loss: 0.19195827841758728\nEpoch [2], Step [4900], Loss: 0.19246596097946167\nEpoch [2], Step [5000], Loss: 0.34987202286720276\nEpoch [2], Step [5100], Loss: 0.24750100076198578\nEpoch [2], Step [5200], Loss: 0.21566003561019897\nEpoch [2], Step [5300], Loss: 0.15575698018074036\nEpoch [2], Step [5400], Loss: 0.2107548713684082\nEpoch [2], Step [5500], Loss: 0.10769431293010712\nEpoch [2], Step [5600], Loss: 0.329436719417572\nEpoch [2], Step [5700], Loss: 0.2361656129360199\nEpoch [2], Step [5800], Loss: 0.22517412900924683\nEpoch [2], Step [5900], Loss: 0.22221072018146515\nEpoch [2], Step [6000], Loss: 0.2917724847793579\nEpoch [2], Step [6100], Loss: 0.19184617698192596\nEpoch [2], Step [6200], Loss: 0.20611564815044403\nEpoch [2], Step [6300], Loss: 0.19754475355148315\nEpoch [2], Step [6400], Loss: 0.22164438664913177\nEpoch [2], Step [6500], Loss: 0.18951216340065002\nEpoch [2], Step [6600], Loss: 0.1662774682044983\nEpoch [2], Step [6700], Loss: 0.17627283930778503\nEpoch [2], Step [6800], Loss: 0.3121216297149658\nEpoch [2], Step [6900], Loss: 0.1586792916059494\nEpoch [2], Step [7000], Loss: 0.1750517189502716\nEpoch [2], Step [7100], Loss: 0.32928115129470825\nEpoch [2], Step [7200], Loss: 0.2114182710647583\nEpoch [2], Step [7300], Loss: 0.16597580909729004\nEpoch [2], Step [7400], Loss: 0.17143163084983826\nEpoch [2], Step [7500], Loss: 0.19281578063964844\nEpoch [2], Step [7600], Loss: 0.24519075453281403\nEpoch [3], Step [100], Loss: 0.1611650586128235\nEpoch [3], Step [200], Loss: 0.0762314721941948\nEpoch [3], Step [300], Loss: 0.16025704145431519\nEpoch [3], Step [400], Loss: 0.23139861226081848\nEpoch [3], Step [500], Loss: 0.16359135508537292\nEpoch [3], Step [600], Loss: 0.19414040446281433\nEpoch [3], Step [700], Loss: 0.14551657438278198\nEpoch [3], Step [800], Loss: 0.12549751996994019\nEpoch [3], Step [900], Loss: 0.20447862148284912\nEpoch [3], Step [1000], Loss: 0.19598513841629028\nEpoch [3], Step [1100], Loss: 0.14255814254283905\nEpoch [3], Step [1200], Loss: 0.24830031394958496\nEpoch [3], Step [1300], Loss: 0.2811022400856018\nEpoch [3], Step [1400], Loss: 0.16044731438159943\nEpoch [3], Step [1500], Loss: 0.2262202799320221\nEpoch [3], Step [1600], Loss: 0.17527493834495544\nEpoch [3], Step [1700], Loss: 0.14307716488838196\nEpoch [3], Step [1800], Loss: 0.1634761542081833\nEpoch [3], Step [1900], Loss: 0.21012724936008453\nEpoch [3], Step [2000], Loss: 0.14357627928256989\nEpoch [3], Step [2100], Loss: 0.2391604334115982\nEpoch [3], Step [2200], Loss: 0.13271981477737427\nEpoch [3], Step [2300], Loss: 0.2196820080280304\nEpoch [3], Step [2400], Loss: 0.1688622385263443\nEpoch [3], Step [2500], Loss: 0.23243767023086548\nEpoch [3], Step [2600], Loss: 0.173984095454216\nEpoch [3], Step [2700], Loss: 0.16686628758907318\nEpoch [3], Step [2800], Loss: 0.12402619421482086\nEpoch [3], Step [2900], Loss: 0.22110332548618317\nEpoch [3], Step [3000], Loss: 0.2015572339296341\nEpoch [3], Step [3100], Loss: 0.212995246052742\nEpoch [3], Step [3200], Loss: 0.20435446500778198\nEpoch [3], Step [3300], Loss: 0.178089901804924\nEpoch [3], Step [3400], Loss: 0.18146724998950958\nEpoch [3], Step [3500], Loss: 0.22058436274528503\nEpoch [3], Step [3600], Loss: 0.1790829747915268\nEpoch [3], Step [3700], Loss: 0.23114623129367828\nEpoch [3], Step [3800], Loss: 0.12518425285816193\nEpoch [3], Step [3900], Loss: 0.2731592655181885\nEpoch [3], Step [4000], Loss: 0.2369592934846878\nEpoch [3], Step [4100], Loss: 0.24912232160568237\nEpoch [3], Step [4200], Loss: 0.20240387320518494\nEpoch [3], Step [4300], Loss: 0.28452441096305847\nEpoch [3], Step [4400], Loss: 0.19914551079273224\nEpoch [3], Step [4500], Loss: 0.21149909496307373\nEpoch [3], Step [4600], Loss: 0.23317644000053406\nEpoch [3], Step [4700], Loss: 0.12256254255771637\nEpoch [3], Step [4800], Loss: 0.1497936248779297\nEpoch [3], Step [4900], Loss: 0.15325452387332916\nEpoch [3], Step [5000], Loss: 0.15309865772724152\nEpoch [3], Step [5100], Loss: 0.2756185233592987\nEpoch [3], Step [5200], Loss: 0.1755620837211609\nEpoch [3], Step [5300], Loss: 0.23840086162090302\nEpoch [3], Step [5400], Loss: 0.15844093263149261\nEpoch [3], Step [5500], Loss: 0.16578152775764465\nEpoch [3], Step [5600], Loss: 0.16987670958042145\nEpoch [3], Step [5700], Loss: 0.29050618410110474\nEpoch [3], Step [5800], Loss: 0.11365122348070145\nEpoch [3], Step [5900], Loss: 0.16393409669399261\nEpoch [3], Step [6000], Loss: 0.16596829891204834\nEpoch [3], Step [6100], Loss: 0.15387170016765594\nEpoch [3], Step [6200], Loss: 0.1681172400712967\nEpoch [3], Step [6300], Loss: 0.2138916403055191\nEpoch [3], Step [6400], Loss: 0.09734656661748886\nEpoch [3], Step [6500], Loss: 0.27969324588775635\nEpoch [3], Step [6600], Loss: 0.18286646902561188\nEpoch [3], Step [6700], Loss: 0.1331394761800766\nEpoch [3], Step [6800], Loss: 0.1283699870109558\nEpoch [3], Step [6900], Loss: 0.21822598576545715\nEpoch [3], Step [7000], Loss: 0.2075887769460678\nEpoch [3], Step [7100], Loss: 0.12362103164196014\nEpoch [3], Step [7200], Loss: 0.1443958282470703\nEpoch [3], Step [7300], Loss: 0.2844356298446655\nEpoch [3], Step [7400], Loss: 0.19098913669586182\nEpoch [3], Step [7500], Loss: 0.1528852880001068\nEpoch [3], Step [7600], Loss: 0.25723135471343994\nEpoch [4], Step [100], Loss: 0.1390138864517212\nEpoch [4], Step [200], Loss: 0.13583026826381683\nEpoch [4], Step [300], Loss: 0.1299532800912857\nEpoch [4], Step [400], Loss: 0.15491032600402832\nEpoch [4], Step [500], Loss: 0.17849770188331604\nEpoch [4], Step [600], Loss: 0.0874612033367157\nEpoch [4], Step [700], Loss: 0.11778005212545395\nEpoch [4], Step [800], Loss: 0.21663054823875427\nEpoch [4], Step [900], Loss: 0.18670719861984253\nEpoch [4], Step [1000], Loss: 0.24818462133407593\nEpoch [4], Step [1100], Loss: 0.15504038333892822\nEpoch [4], Step [1200], Loss: 0.14461678266525269\nEpoch [4], Step [1300], Loss: 0.20909416675567627\nEpoch [4], Step [1400], Loss: 0.11631108075380325\nEpoch [4], Step [1500], Loss: 0.15052902698516846\nEpoch [4], Step [1600], Loss: 0.1336962878704071\nEpoch [4], Step [1700], Loss: 0.157737597823143\nEpoch [4], Step [1800], Loss: 0.16166704893112183\nEpoch [4], Step [1900], Loss: 0.14832112193107605\nEpoch [4], Step [2000], Loss: 0.15139541029930115\nEpoch [4], Step [2100], Loss: 0.08737511932849884\nEpoch [4], Step [2200], Loss: 0.15563082695007324\nEpoch [4], Step [2300], Loss: 0.17204636335372925\nEpoch [4], Step [2400], Loss: 0.1479419767856598\nEpoch [4], Step [2500], Loss: 0.15549172461032867\nEpoch [4], Step [2600], Loss: 0.2182314097881317\nEpoch [4], Step [2700], Loss: 0.1900419443845749\nEpoch [4], Step [2800], Loss: 0.15660682320594788\nEpoch [4], Step [2900], Loss: 0.13057681918144226\nEpoch [4], Step [3000], Loss: 0.1296280324459076\nEpoch [4], Step [3100], Loss: 0.1941467970609665\nEpoch [4], Step [3200], Loss: 0.1298247128725052\nEpoch [4], Step [3300], Loss: 0.10680628567934036\nEpoch [4], Step [3400], Loss: 0.10334836691617966\nEpoch [4], Step [3500], Loss: 0.12630583345890045\nEpoch [4], Step [3600], Loss: 0.15538744628429413\nEpoch [4], Step [3700], Loss: 0.2188982218503952\nEpoch [4], Step [3800], Loss: 0.20580364763736725\nEpoch [4], Step [3900], Loss: 0.2100904881954193\nEpoch [4], Step [4000], Loss: 0.18317000567913055\nEpoch [4], Step [4100], Loss: 0.20348258316516876\nEpoch [4], Step [4200], Loss: 0.14589709043502808\nEpoch [4], Step [4300], Loss: 0.1546238660812378\nEpoch [4], Step [4400], Loss: 0.14876894652843475\nEpoch [4], Step [4500], Loss: 0.15161468088626862\nEpoch [4], Step [4600], Loss: 0.18714700639247894\nEpoch [4], Step [4700], Loss: 0.1377740204334259\nEpoch [4], Step [4800], Loss: 0.16783465445041656\nEpoch [4], Step [4900], Loss: 0.1252124160528183\nEpoch [4], Step [5000], Loss: 0.07484443485736847\nEpoch [4], Step [5100], Loss: 0.1200668066740036\nEpoch [4], Step [5200], Loss: 0.11962087452411652\nEpoch [4], Step [5300], Loss: 0.12907114624977112\nEpoch [4], Step [5400], Loss: 0.10182168334722519\nEpoch [4], Step [5500], Loss: 0.1866445392370224\nEpoch [4], Step [5600], Loss: 0.14232085645198822\nEpoch [4], Step [5700], Loss: 0.10901422798633575\nEpoch [4], Step [5800], Loss: 0.2558152377605438\nEpoch [4], Step [5900], Loss: 0.185926154255867\nEpoch [4], Step [6000], Loss: 0.10253042727708817\nEpoch [4], Step [6100], Loss: 0.14343874156475067\nEpoch [4], Step [6200], Loss: 0.13118043541908264\nEpoch [4], Step [6300], Loss: 0.1753629446029663\nEpoch [4], Step [6400], Loss: 0.2310107797384262\nEpoch [4], Step [6500], Loss: 0.23328518867492676\nEpoch [4], Step [6600], Loss: 0.11600954830646515\nEpoch [4], Step [6700], Loss: 0.13787144422531128\nEpoch [4], Step [6800], Loss: 0.10570994019508362\nEpoch [4], Step [6900], Loss: 0.25542569160461426\nEpoch [4], Step [7000], Loss: 0.08189158141613007\nEpoch [4], Step [7100], Loss: 0.14248625934123993\nEpoch [4], Step [7200], Loss: 0.19628572463989258\nEpoch [4], Step [7300], Loss: 0.21182134747505188\nEpoch [4], Step [7400], Loss: 0.20194359123706818\nEpoch [4], Step [7500], Loss: 0.2116798460483551\nEpoch [4], Step [7600], Loss: 0.24973905086517334\nEpoch [5], Step [100], Loss: 0.09612368047237396\nEpoch [5], Step [200], Loss: 0.16219893097877502\nEpoch [5], Step [300], Loss: 0.0903778001666069\nEpoch [5], Step [400], Loss: 0.12446121126413345\nEpoch [5], Step [500], Loss: 0.1818854957818985\nEpoch [5], Step [600], Loss: 0.1958383470773697\nEpoch [5], Step [700], Loss: 0.09435532987117767\nEpoch [5], Step [800], Loss: 0.14158399403095245\nEpoch [5], Step [900], Loss: 0.10350441187620163\nEpoch [5], Step [1000], Loss: 0.19005794823169708\nEpoch [5], Step [1100], Loss: 0.11356139928102493\nEpoch [5], Step [1200], Loss: 0.1291067898273468\nEpoch [5], Step [1300], Loss: 0.17356465756893158\nEpoch [5], Step [1400], Loss: 0.103759765625\nEpoch [5], Step [1500], Loss: 0.13478952646255493\nEpoch [5], Step [1600], Loss: 0.11413191258907318\nEpoch [5], Step [1700], Loss: 0.17542286217212677\nEpoch [5], Step [1800], Loss: 0.11385554075241089\nEpoch [5], Step [1900], Loss: 0.07651196420192719\nEpoch [5], Step [2000], Loss: 0.12073047459125519\nEpoch [5], Step [2100], Loss: 0.1335439682006836\nEpoch [5], Step [2200], Loss: 0.21081066131591797\nEpoch [5], Step [2300], Loss: 0.10187681764364243\nEpoch [5], Step [2400], Loss: 0.13199077546596527\nEpoch [5], Step [2500], Loss: 0.11334191262722015\nEpoch [5], Step [2600], Loss: 0.12597589194774628\nEpoch [5], Step [2700], Loss: 0.09893272072076797\nEpoch [5], Step [2800], Loss: 0.08109971135854721\nEpoch [5], Step [2900], Loss: 0.14582939445972443\nEpoch [5], Step [3000], Loss: 0.12797409296035767\nEpoch [5], Step [3100], Loss: 0.18113850057125092\nEpoch [5], Step [3200], Loss: 0.07361625880002975\nEpoch [5], Step [3300], Loss: 0.12739650905132294\nEpoch [5], Step [3400], Loss: 0.10459233820438385\nEpoch [5], Step [3500], Loss: 0.19675220549106598\nEpoch [5], Step [3600], Loss: 0.20065994560718536\nEpoch [5], Step [3700], Loss: 0.14648036658763885\nEpoch [5], Step [3800], Loss: 0.1888505071401596\nEpoch [5], Step [3900], Loss: 0.1200542077422142\nEpoch [5], Step [4000], Loss: 0.15699462592601776\nEpoch [5], Step [4100], Loss: 0.0811193436384201\nEpoch [5], Step [4200], Loss: 0.13190115988254547\nEpoch [5], Step [4300], Loss: 0.16730515658855438\nEpoch [5], Step [4400], Loss: 0.08752157539129257\nEpoch [5], Step [4500], Loss: 0.17202942073345184\nEpoch [5], Step [4600], Loss: 0.14391222596168518\nEpoch [5], Step [4700], Loss: 0.18371571600437164\nEpoch [5], Step [4800], Loss: 0.15031705796718597\nEpoch [5], Step [4900], Loss: 0.1212240532040596\nEpoch [5], Step [5000], Loss: 0.04238910228013992\nEpoch [5], Step [5100], Loss: 0.127584308385849\nEpoch [5], Step [5200], Loss: 0.10102441906929016\nEpoch [5], Step [5300], Loss: 0.15263664722442627\nEpoch [5], Step [5400], Loss: 0.11952032893896103\nEpoch [5], Step [5500], Loss: 0.09578675031661987\nEpoch [5], Step [5600], Loss: 0.18785595893859863\nEpoch [5], Step [5700], Loss: 0.11868874728679657\nEpoch [5], Step [5800], Loss: 0.10071083158254623\nEpoch [5], Step [5900], Loss: 0.09073127061128616\nEpoch [5], Step [6000], Loss: 0.10280212759971619\nEpoch [5], Step [6100], Loss: 0.11955379694700241\nEpoch [5], Step [6200], Loss: 0.14685215055942535\nEpoch [5], Step [6300], Loss: 0.07292421162128448\nEpoch [5], Step [6400], Loss: 0.23935288190841675\nEpoch [5], Step [6500], Loss: 0.14197340607643127\nEpoch [5], Step [6600], Loss: 0.19941456615924835\nEpoch [5], Step [6700], Loss: 0.11268412321805954\nEpoch [5], Step [6800], Loss: 0.1418633908033371\nEpoch [5], Step [6900], Loss: 0.09638086706399918\nEpoch [5], Step [7000], Loss: 0.13670039176940918\nEpoch [5], Step [7100], Loss: 0.14524780213832855\nEpoch [5], Step [7200], Loss: 0.16192471981048584\nEpoch [5], Step [7300], Loss: 0.22094152867794037\nEpoch [5], Step [7400], Loss: 0.12478121370077133\nEpoch [5], Step [7500], Loss: 0.21961742639541626\nEpoch [5], Step [7600], Loss: 0.1162194088101387\nEpoch [6], Step [100], Loss: 0.12179462611675262\nEpoch [6], Step [200], Loss: 0.12290848791599274\nEpoch [6], Step [300], Loss: 0.11371661722660065\nEpoch [6], Step [400], Loss: 0.10463586449623108\nEpoch [6], Step [500], Loss: 0.05043487250804901\nEpoch [6], Step [600], Loss: 0.10551567375659943\nEpoch [6], Step [700], Loss: 0.09899851679801941\nEpoch [6], Step [800], Loss: 0.11127907782793045\nEpoch [6], Step [900], Loss: 0.16280785202980042\nEpoch [6], Step [1000], Loss: 0.10995826125144958\nEpoch [6], Step [1100], Loss: 0.08729042112827301\nEpoch [6], Step [1200], Loss: 0.07707197219133377\nEpoch [6], Step [1300], Loss: 0.06465748697519302\nEpoch [6], Step [1400], Loss: 0.06779330223798752\nEpoch [6], Step [1500], Loss: 0.0906304195523262\nEpoch [6], Step [1600], Loss: 0.09299419075250626\nEpoch [6], Step [1700], Loss: 0.06220276653766632\nEpoch [6], Step [1800], Loss: 0.11289539933204651\nEpoch [6], Step [1900], Loss: 0.08981654793024063\nEpoch [6], Step [2000], Loss: 0.050673142075538635\nEpoch [6], Step [2100], Loss: 0.09126410633325577\nEpoch [6], Step [2200], Loss: 0.12929925322532654\nEpoch [6], Step [2300], Loss: 0.08209209889173508\nEpoch [6], Step [2400], Loss: 0.11127200722694397\nEpoch [6], Step [2500], Loss: 0.07531970739364624\nEpoch [6], Step [2600], Loss: 0.12960736453533173\nEpoch [6], Step [2700], Loss: 0.08126218616962433\nEpoch [6], Step [2800], Loss: 0.10740465670824051\nEpoch [6], Step [2900], Loss: 0.10798896104097366\nEpoch [6], Step [3000], Loss: 0.16769354045391083\nEpoch [6], Step [3100], Loss: 0.160684734582901\nEpoch [6], Step [3200], Loss: 0.08289381116628647\nEpoch [6], Step [3300], Loss: 0.09978927671909332\nEpoch [6], Step [3400], Loss: 0.10654362291097641\nEpoch [6], Step [3500], Loss: 0.1389159858226776\nEpoch [6], Step [3600], Loss: 0.09370984137058258\nEpoch [6], Step [3700], Loss: 0.10685672610998154\nEpoch [6], Step [3800], Loss: 0.11382504552602768\nEpoch [6], Step [3900], Loss: 0.1278979778289795\nEpoch [6], Step [4000], Loss: 0.1442749798297882\nEpoch [6], Step [4100], Loss: 0.08728823065757751\nEpoch [6], Step [4200], Loss: 0.08569144457578659\nEpoch [6], Step [4300], Loss: 0.17182579636573792\nEpoch [6], Step [4400], Loss: 0.1395988017320633\nEpoch [6], Step [4500], Loss: 0.1603115051984787\nEpoch [6], Step [4600], Loss: 0.0941811054944992\nEpoch [6], Step [4700], Loss: 0.09659062325954437\nEpoch [6], Step [4800], Loss: 0.10306228697299957\nEpoch [6], Step [4900], Loss: 0.07316266000270844\nEpoch [6], Step [5000], Loss: 0.1380239874124527\nEpoch [6], Step [5100], Loss: 0.15790733695030212\nEpoch [6], Step [5200], Loss: 0.11917775869369507\nEpoch [6], Step [5300], Loss: 0.10896909981966019\nEpoch [6], Step [5400], Loss: 0.17847171425819397\nEpoch [6], Step [5500], Loss: 0.09066542983055115\nEpoch [6], Step [5600], Loss: 0.17312391102313995\nEpoch [6], Step [5700], Loss: 0.17170163989067078\nEpoch [6], Step [5800], Loss: 0.15597482025623322\nEpoch [6], Step [5900], Loss: 0.12066411226987839\nEpoch [6], Step [6000], Loss: 0.09982994943857193\nEpoch [6], Step [6100], Loss: 0.07160062342882156\nEpoch [6], Step [6200], Loss: 0.08154232800006866\nEpoch [6], Step [6300], Loss: 0.1743856817483902\nEpoch [6], Step [6400], Loss: 0.1232379823923111\nEpoch [6], Step [6500], Loss: 0.050266582518815994\nEpoch [6], Step [6600], Loss: 0.13364455103874207\nEpoch [6], Step [6700], Loss: 0.18794971704483032\nEpoch [6], Step [6800], Loss: 0.1328945755958557\nEpoch [6], Step [6900], Loss: 0.08670353144407272\nEpoch [6], Step [7000], Loss: 0.11493348330259323\nEpoch [6], Step [7100], Loss: 0.1077163890004158\nEpoch [6], Step [7200], Loss: 0.0790257677435875\nEpoch [6], Step [7300], Loss: 0.10743046551942825\nEpoch [6], Step [7400], Loss: 0.13625888526439667\nEpoch [6], Step [7500], Loss: 0.1301368921995163\nEpoch [6], Step [7600], Loss: 0.1222197562456131\nEpoch [7], Step [100], Loss: 0.12223722040653229\nEpoch [7], Step [200], Loss: 0.07332998514175415\nEpoch [7], Step [300], Loss: 0.14296238124370575\nEpoch [7], Step [400], Loss: 0.1397082358598709\nEpoch [7], Step [500], Loss: 0.1284656822681427\nEpoch [7], Step [600], Loss: 0.05930420383810997\nEpoch [7], Step [700], Loss: 0.09675556421279907\nEpoch [7], Step [800], Loss: 0.053913988173007965\nEpoch [7], Step [900], Loss: 0.09719377756118774\nEpoch [7], Step [1000], Loss: 0.054939113557338715\nEpoch [7], Step [1100], Loss: 0.0674659013748169\nEpoch [7], Step [1200], Loss: 0.06293594837188721\nEpoch [7], Step [1300], Loss: 0.1446942389011383\nEpoch [7], Step [1400], Loss: 0.08494306355714798\nEpoch [7], Step [1500], Loss: 0.08599063754081726\nEpoch [7], Step [1600], Loss: 0.10570061206817627\nEpoch [7], Step [1700], Loss: 0.10412616282701492\nEpoch [7], Step [1800], Loss: 0.07609567791223526\nEpoch [7], Step [1900], Loss: 0.07215180993080139\nEpoch [7], Step [2000], Loss: 0.07479757815599442\nEpoch [7], Step [2100], Loss: 0.04017137736082077\nEpoch [7], Step [2200], Loss: 0.030849311500787735\nEpoch [7], Step [2300], Loss: 0.13439078629016876\nEpoch [7], Step [2400], Loss: 0.1248287633061409\nEpoch [7], Step [2500], Loss: 0.06920719146728516\nEpoch [7], Step [2600], Loss: 0.09846964478492737\nEpoch [7], Step [2700], Loss: 0.1112348809838295\nEpoch [7], Step [2800], Loss: 0.10020793974399567\nEpoch [7], Step [2900], Loss: 0.07538481801748276\nEpoch [7], Step [3000], Loss: 0.08514495939016342\nEpoch [7], Step [3100], Loss: 0.09871841222047806\nEpoch [7], Step [3200], Loss: 0.04376765340566635\nEpoch [7], Step [3300], Loss: 0.03088412992656231\nEpoch [7], Step [3400], Loss: 0.1343154013156891\nEpoch [7], Step [3500], Loss: 0.12777669727802277\nEpoch [7], Step [3600], Loss: 0.10068223625421524\nEpoch [7], Step [3700], Loss: 0.09341008216142654\nEpoch [7], Step [3800], Loss: 0.1015339344739914\nEpoch [7], Step [3900], Loss: 0.06084441766142845\nEpoch [7], Step [4000], Loss: 0.10302910208702087\nEpoch [7], Step [4100], Loss: 0.09627391397953033\nEpoch [7], Step [4200], Loss: 0.03117356449365616\nEpoch [7], Step [4300], Loss: 0.13289794325828552\nEpoch [7], Step [4400], Loss: 0.08643237501382828\nEpoch [7], Step [4500], Loss: 0.036962442100048065\nEpoch [7], Step [4600], Loss: 0.12983189523220062\nEpoch [7], Step [4700], Loss: 0.09173840284347534\nEpoch [7], Step [4800], Loss: 0.06173053756356239\nEpoch [7], Step [4900], Loss: 0.08088518679141998\nEpoch [7], Step [5000], Loss: 0.04417691379785538\nEpoch [7], Step [5100], Loss: 0.10048437863588333\nEpoch [7], Step [5200], Loss: 0.08378399908542633\nEpoch [7], Step [5300], Loss: 0.10532183200120926\nEpoch [7], Step [5400], Loss: 0.09730974584817886\nEpoch [7], Step [5500], Loss: 0.1218150407075882\nEpoch [7], Step [5600], Loss: 0.06922542303800583\nEpoch [7], Step [5700], Loss: 0.06496775150299072\nEpoch [7], Step [5800], Loss: 0.0510634109377861\nEpoch [7], Step [5900], Loss: 0.05731084197759628\nEpoch [7], Step [6000], Loss: 0.17554982006549835\nEpoch [7], Step [6100], Loss: 0.05656774714589119\nEpoch [7], Step [6200], Loss: 0.06719564646482468\nEpoch [7], Step [6300], Loss: 0.09366961568593979\nEpoch [7], Step [6400], Loss: 0.03721357509493828\nEpoch [7], Step [6500], Loss: 0.05506272614002228\nEpoch [7], Step [6600], Loss: 0.08651091158390045\nEpoch [7], Step [6700], Loss: 0.079517662525177\nEpoch [7], Step [6800], Loss: 0.07553502172231674\nEpoch [7], Step [6900], Loss: 0.07467607408761978\nEpoch [7], Step [7000], Loss: 0.20876511931419373\nEpoch [7], Step [7100], Loss: 0.07738571614027023\nEpoch [7], Step [7200], Loss: 0.08392524719238281\nEpoch [7], Step [7300], Loss: 0.09823182225227356\nEpoch [7], Step [7400], Loss: 0.1394146978855133\nEpoch [7], Step [7500], Loss: 0.1278129518032074\nEpoch [7], Step [7600], Loss: 0.13949213922023773\nEpoch [8], Step [100], Loss: 0.05619637295603752\nEpoch [8], Step [200], Loss: 0.07450339943170547\nEpoch [8], Step [300], Loss: 0.08000554889440536\nEpoch [8], Step [400], Loss: 0.12669958174228668\nEpoch [8], Step [500], Loss: 0.08630315214395523\nEpoch [8], Step [600], Loss: 0.03923814743757248\nEpoch [8], Step [700], Loss: 0.0669376328587532\nEpoch [8], Step [800], Loss: 0.10410631448030472\nEpoch [8], Step [900], Loss: 0.0870431661605835\nEpoch [8], Step [1000], Loss: 0.027133820578455925\nEpoch [8], Step [1100], Loss: 0.06058916822075844\nEpoch [8], Step [1200], Loss: 0.10671292245388031\nEpoch [8], Step [1300], Loss: 0.07171310484409332\nEpoch [8], Step [1400], Loss: 0.06974076479673386\nEpoch [8], Step [1500], Loss: 0.10876596719026566\nEpoch [8], Step [1600], Loss: 0.0678221583366394\nEpoch [8], Step [1700], Loss: 0.09325672686100006\nEpoch [8], Step [1800], Loss: 0.0647890567779541\nEpoch [8], Step [1900], Loss: 0.06716536730527878\nEpoch [8], Step [2000], Loss: 0.09730466455221176\nEpoch [8], Step [2100], Loss: 0.06659979373216629\nEpoch [8], Step [2200], Loss: 0.06924594193696976\nEpoch [8], Step [2300], Loss: 0.19701707363128662\nEpoch [8], Step [2400], Loss: 0.03810729831457138\nEpoch [8], Step [2500], Loss: 0.046498045325279236\nEpoch [8], Step [2600], Loss: 0.0692649707198143\nEpoch [8], Step [2700], Loss: 0.0804542824625969\nEpoch [8], Step [2800], Loss: 0.11003290861845016\nEpoch [8], Step [2900], Loss: 0.08824018388986588\nEpoch [8], Step [3000], Loss: 0.02970850095152855\nEpoch [8], Step [3100], Loss: 0.05047585442662239\nEpoch [8], Step [3200], Loss: 0.032338179647922516\nEpoch [8], Step [3300], Loss: 0.06096067279577255\nEpoch [8], Step [3400], Loss: 0.08177930861711502\nEpoch [8], Step [3500], Loss: 0.13308492302894592\nEpoch [8], Step [3600], Loss: 0.07458148151636124\nEpoch [8], Step [3700], Loss: 0.06584656983613968\nEpoch [8], Step [3800], Loss: 0.07018839567899704\nEpoch [8], Step [3900], Loss: 0.06646524369716644\nEpoch [8], Step [4000], Loss: 0.14118672907352448\nEpoch [8], Step [4100], Loss: 0.09701749682426453\nEpoch [8], Step [4200], Loss: 0.06683452427387238\nEpoch [8], Step [4300], Loss: 0.15981590747833252\nEpoch [8], Step [4400], Loss: 0.06719691306352615\nEpoch [8], Step [4500], Loss: 0.11817046999931335\nEpoch [8], Step [4600], Loss: 0.05277075991034508\nEpoch [8], Step [4700], Loss: 0.14865334331989288\nEpoch [8], Step [4800], Loss: 0.03785179182887077\nEpoch [8], Step [4900], Loss: 0.041111912578344345\nEpoch [8], Step [5000], Loss: 0.03848231956362724\nEpoch [8], Step [5100], Loss: 0.12886235117912292\nEpoch [8], Step [5200], Loss: 0.1361386775970459\nEpoch [8], Step [5300], Loss: 0.0702570453286171\nEpoch [8], Step [5400], Loss: 0.08060004562139511\nEpoch [8], Step [5500], Loss: 0.11973898857831955\nEpoch [8], Step [5600], Loss: 0.0753091424703598\nEpoch [8], Step [5700], Loss: 0.05958615243434906\nEpoch [8], Step [5800], Loss: 0.12061775475740433\nEpoch [8], Step [5900], Loss: 0.04161544889211655\nEpoch [8], Step [6000], Loss: 0.07192307710647583\nEpoch [8], Step [6100], Loss: 0.1166326254606247\nEpoch [8], Step [6200], Loss: 0.04254000633955002\nEpoch [8], Step [6300], Loss: 0.04704448580741882\nEpoch [8], Step [6400], Loss: 0.0696486234664917\nEpoch [8], Step [6500], Loss: 0.10877455025911331\nEpoch [8], Step [6600], Loss: 0.08392028510570526\nEpoch [8], Step [6700], Loss: 0.055659763514995575\nEpoch [8], Step [6800], Loss: 0.08480550348758698\nEpoch [8], Step [6900], Loss: 0.07696735858917236\nEpoch [8], Step [7000], Loss: 0.06418966501951218\nEpoch [8], Step [7100], Loss: 0.07206787168979645\nEpoch [8], Step [7200], Loss: 0.0956479161977768\nEpoch [8], Step [7300], Loss: 0.07321283966302872\nEpoch [8], Step [7400], Loss: 0.07831356674432755\nEpoch [8], Step [7500], Loss: 0.057628657668828964\nEpoch [8], Step [7600], Loss: 0.11710444837808609\nEpoch [9], Step [100], Loss: 0.03187500312924385\nEpoch [9], Step [200], Loss: 0.04582041874527931\nEpoch [9], Step [300], Loss: 0.03880595043301582\nEpoch [9], Step [400], Loss: 0.08547958731651306\nEpoch [9], Step [500], Loss: 0.02571862004697323\nEpoch [9], Step [600], Loss: 0.023410892114043236\nEpoch [9], Step [700], Loss: 0.03307104483246803\nEpoch [9], Step [800], Loss: 0.03214573487639427\nEpoch [9], Step [900], Loss: 0.02493119053542614\nEpoch [9], Step [1000], Loss: 0.052385538816452026\nEpoch [9], Step [1100], Loss: 0.031740494072437286\nEpoch [9], Step [1200], Loss: 0.03379398211836815\nEpoch [9], Step [1300], Loss: 0.08241724222898483\nEpoch [9], Step [1400], Loss: 0.1351882368326187\nEpoch [9], Step [1500], Loss: 0.08336695283651352\nEpoch [9], Step [1600], Loss: 0.0839482992887497\nEpoch [9], Step [1700], Loss: 0.09523357450962067\nEpoch [9], Step [1800], Loss: 0.04356040433049202\nEpoch [9], Step [1900], Loss: 0.03601400554180145\nEpoch [9], Step [2000], Loss: 0.05741593986749649\nEpoch [9], Step [2100], Loss: 0.07183744013309479\nEpoch [9], Step [2200], Loss: 0.09570419788360596\nEpoch [9], Step [2300], Loss: 0.05320176109671593\nEpoch [9], Step [2400], Loss: 0.05028476566076279\nEpoch [9], Step [2500], Loss: 0.03129195794463158\nEpoch [9], Step [2600], Loss: 0.06954135000705719\nEpoch [9], Step [2700], Loss: 0.055891163647174835\nEpoch [9], Step [2800], Loss: 0.03698831424117088\nEpoch [9], Step [2900], Loss: 0.04947706684470177\nEpoch [9], Step [3000], Loss: 0.11085933446884155\nEpoch [9], Step [3100], Loss: 0.03224186599254608\nEpoch [9], Step [3200], Loss: 0.06228293851017952\nEpoch [9], Step [3300], Loss: 0.10696705430746078\nEpoch [9], Step [3400], Loss: 0.017019327729940414\nEpoch [9], Step [3500], Loss: 0.08046374469995499\nEpoch [9], Step [3600], Loss: 0.08434579521417618\nEpoch [9], Step [3700], Loss: 0.05834343656897545\nEpoch [9], Step [3800], Loss: 0.05082494392991066\nEpoch [9], Step [3900], Loss: 0.04511568695306778\nEpoch [9], Step [4000], Loss: 0.0550297349691391\nEpoch [9], Step [4100], Loss: 0.06448016315698624\nEpoch [9], Step [4200], Loss: 0.08693406730890274\nEpoch [9], Step [4300], Loss: 0.11267035454511642\nEpoch [9], Step [4400], Loss: 0.0838186964392662\nEpoch [9], Step [4500], Loss: 0.04625788331031799\nEpoch [9], Step [4600], Loss: 0.052630260586738586\nEpoch [9], Step [4700], Loss: 0.056871604174375534\nEpoch [9], Step [4800], Loss: 0.07989418506622314\nEpoch [9], Step [4900], Loss: 0.07928112149238586\nEpoch [9], Step [5000], Loss: 0.03797122836112976\nEpoch [9], Step [5100], Loss: 0.06128457933664322\nEpoch [9], Step [5200], Loss: 0.05149746686220169\nEpoch [9], Step [5300], Loss: 0.10899487882852554\nEpoch [9], Step [5400], Loss: 0.11606919765472412\nEpoch [9], Step [5500], Loss: 0.05130418390035629\nEpoch [9], Step [5600], Loss: 0.0392448827624321\nEpoch [9], Step [5700], Loss: 0.06452912092208862\nEpoch [9], Step [5800], Loss: 0.0401422381401062\nEpoch [9], Step [5900], Loss: 0.02034638077020645\nEpoch [9], Step [6000], Loss: 0.05526558682322502\nEpoch [9], Step [6100], Loss: 0.09997238218784332\nEpoch [9], Step [6200], Loss: 0.04041020944714546\nEpoch [9], Step [6300], Loss: 0.0625973492860794\nEpoch [9], Step [6400], Loss: 0.02345823496580124\nEpoch [9], Step [6500], Loss: 0.04552211984992027\nEpoch [9], Step [6600], Loss: 0.061755478382110596\nEpoch [9], Step [6700], Loss: 0.15309840440750122\nEpoch [9], Step [6800], Loss: 0.06682906299829483\nEpoch [9], Step [6900], Loss: 0.043444640934467316\nEpoch [9], Step [7000], Loss: 0.09777575731277466\nEpoch [9], Step [7100], Loss: 0.04416710138320923\nEpoch [9], Step [7200], Loss: 0.04255908727645874\nEpoch [9], Step [7300], Loss: 0.059373047202825546\nEpoch [9], Step [7400], Loss: 0.09384194761514664\nEpoch [9], Step [7500], Loss: 0.07103578746318817\nEpoch [9], Step [7600], Loss: 0.15864989161491394\nEpoch [10], Step [100], Loss: 0.0815487876534462\nEpoch [10], Step [200], Loss: 0.06122647598385811\nEpoch [10], Step [300], Loss: 0.03769339621067047\nEpoch [10], Step [400], Loss: 0.02815442718565464\nEpoch [10], Step [500], Loss: 0.041549600660800934\nEpoch [10], Step [600], Loss: 0.05472579970955849\nEpoch [10], Step [700], Loss: 0.029132328927516937\nEpoch [10], Step [800], Loss: 0.03603524714708328\nEpoch [10], Step [900], Loss: 0.09743957966566086\nEpoch [10], Step [1000], Loss: 0.09107843041419983\nEpoch [10], Step [1100], Loss: 0.09385161101818085\nEpoch [10], Step [1200], Loss: 0.03256863355636597\nEpoch [10], Step [1300], Loss: 0.021304359659552574\nEpoch [10], Step [1400], Loss: 0.06304078549146652\nEpoch [10], Step [1500], Loss: 0.07316156476736069\nEpoch [10], Step [1600], Loss: 0.03783062472939491\nEpoch [10], Step [1700], Loss: 0.03919731825590134\nEpoch [10], Step [1800], Loss: 0.04369005933403969\nEpoch [10], Step [1900], Loss: 0.03768513724207878\nEpoch [10], Step [2000], Loss: 0.010385683737695217\nEpoch [10], Step [2100], Loss: 0.04024437814950943\nEpoch [10], Step [2200], Loss: 0.11821316182613373\nEpoch [10], Step [2300], Loss: 0.05987513065338135\nEpoch [10], Step [2400], Loss: 0.0599525049328804\nEpoch [10], Step [2500], Loss: 0.040014009922742844\nEpoch [10], Step [2600], Loss: 0.013933327049016953\nEpoch [10], Step [2700], Loss: 0.028390515595674515\nEpoch [10], Step [2800], Loss: 0.1011921837925911\nEpoch [10], Step [2900], Loss: 0.05429540202021599\nEpoch [10], Step [3000], Loss: 0.018971750512719154\nEpoch [10], Step [3100], Loss: 0.08238214999437332\nEpoch [10], Step [3200], Loss: 0.02515498921275139\nEpoch [10], Step [3300], Loss: 0.09899192303419113\nEpoch [10], Step [3400], Loss: 0.09773823618888855\nEpoch [10], Step [3500], Loss: 0.08852596580982208\nEpoch [10], Step [3600], Loss: 0.0407387800514698\nEpoch [10], Step [3700], Loss: 0.04401611536741257\nEpoch [10], Step [3800], Loss: 0.06955517083406448\nEpoch [10], Step [3900], Loss: 0.03469850495457649\nEpoch [10], Step [4000], Loss: 0.05260545015335083\nEpoch [10], Step [4100], Loss: 0.05982588976621628\nEpoch [10], Step [4200], Loss: 0.04497155547142029\nEpoch [10], Step [4300], Loss: 0.02919531799852848\nEpoch [10], Step [4400], Loss: 0.047161247581243515\nEpoch [10], Step [4500], Loss: 0.062310751527547836\nEpoch [10], Step [4600], Loss: 0.09884839504957199\nEpoch [10], Step [4700], Loss: 0.04002920538187027\nEpoch [10], Step [4800], Loss: 0.04463963955640793\nEpoch [10], Step [4900], Loss: 0.055399198085069656\nEpoch [10], Step [5000], Loss: 0.06308084726333618\nEpoch [10], Step [5100], Loss: 0.008711696602404118\nEpoch [10], Step [5200], Loss: 0.06371032446622849\nEpoch [10], Step [5300], Loss: 0.046470120549201965\nEpoch [10], Step [5400], Loss: 0.04975943639874458\nEpoch [10], Step [5500], Loss: 0.058801617473363876\nEpoch [10], Step [5600], Loss: 0.022201502695679665\nEpoch [10], Step [5700], Loss: 0.05727187916636467\nEpoch [10], Step [5800], Loss: 0.1304149478673935\nEpoch [10], Step [5900], Loss: 0.059354789555072784\nEpoch [10], Step [6000], Loss: 0.024125056341290474\nEpoch [10], Step [6100], Loss: 0.10137810558080673\nEpoch [10], Step [6200], Loss: 0.04525397717952728\nEpoch [10], Step [6300], Loss: 0.09413058310747147\nEpoch [10], Step [6400], Loss: 0.042315587401390076\nEpoch [10], Step [6500], Loss: 0.08095918595790863\nEpoch [10], Step [6600], Loss: 0.09581990540027618\nEpoch [10], Step [6700], Loss: 0.08092113584280014\nEpoch [10], Step [6800], Loss: 0.011154962703585625\nEpoch [10], Step [6900], Loss: 0.04786020144820213\nEpoch [10], Step [7000], Loss: 0.07665544003248215\nEpoch [10], Step [7100], Loss: 0.0911746621131897\nEpoch [10], Step [7200], Loss: 0.053023532032966614\nEpoch [10], Step [7300], Loss: 0.0939248576760292\nEpoch [10], Step [7400], Loss: 0.03274228051304817\nEpoch [10], Step [7500], Loss: 0.07705746591091156\nEpoch [10], Step [7600], Loss: 0.07930167019367218\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 测试\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for comments, labels in test_dataLoader:\n        comments = comments.to(device)\n        labels = labels.to(device)\n        outputs = model(comments)\n        pred = torch.argmax(outputs, dim=1)\n        total += len(labels)\n        correct += (pred == labels).sum().item()\n    print('准确率：', correct * 100 / total)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:38:45.397340Z","iopub.execute_input":"2025-04-18T04:38:45.398021Z","iopub.status.idle":"2025-04-18T04:38:56.092599Z","shell.execute_reply.started":"2025-04-18T04:38:45.397996Z","shell.execute_reply":"2025-04-18T04:38:56.091964Z"}},"outputs":[{"name":"stdout","text":"准确率： 89.9020692481049\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import jieba\nstopwords = [line.strip() for line in open('/kaggle/input/stopwords/stopwords.txt', 'r', encoding='utf-8')] + ['PAD']\ndef comment_to_index(comment):\n    # 分词\n    words = jieba.lcut(comment)\n    words = [item for item in words if item not in stopwords]\n    # 转为索引\n    indices = [vocab.get(word, vocab['UNK']) for word in words]\n    return torch.tensor(indices).unsqueeze(0)\n\n# 预测\ncomment1 = comment_to_index('电影很好看，情节引人入胜，全员演技在线，强烈推荐！').to(device)\ncomment2 = comment_to_index('这个电影太烂了，不值得一看！').to(device)\npred1 = model(comment1)\npred2 = model(comment2)\npred1 = torch.argmax(pred1, dim=1).item()\npred2 = torch.argmax(pred2, dim=1).item()\nprint('评论1：', pred1)\nprint('评论2：', pred2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T04:39:01.905012Z","iopub.execute_input":"2025-04-18T04:39:01.905275Z","iopub.status.idle":"2025-04-18T04:39:02.853453Z","shell.execute_reply.started":"2025-04-18T04:39:01.905255Z","shell.execute_reply":"2025-04-18T04:39:02.852689Z"}},"outputs":[{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.652 seconds.\nPrefix dict has been built successfully.\n","output_type":"stream"},{"name":"stdout","text":"评论1： 1\n评论2： 0\n","output_type":"stream"}],"execution_count":9}]}