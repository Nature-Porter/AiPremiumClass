{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11806654,"sourceType":"datasetVersion","datasetId":7414854}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T08:36:44.916150Z","iopub.execute_input":"2025-05-14T08:36:44.916454Z","iopub.status.idle":"2025-05-14T08:36:45.273469Z","shell.execute_reply.started":"2025-05-14T08:36:44.916433Z","shell.execute_reply":"2025-05-14T08:36:45.272523Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/couplet-test-data/out.txt\n/kaggle/input/couplet-test-data/in.txt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom collections import OrderedDict\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\n# 位置编码\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_size, dropout, maxlen=5000):\n        super().__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros(maxlen, emb_size)\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        # 添加批次维度\n        pos_embedding = pos_embedding.unsqueeze(-2)\n        self.dropout = nn.Dropout(dropout)\n        # positional encoding注册为不需要作为模型参数的缓冲中\n        self.register_buffer('pos_embedding', pos_embedding)\n    def forward(self, token_embedding):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n# 将输入索引张量转换为token embedding张量\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size, emb_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n    def forward(self, tokens):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n    \nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, \n                 dim_feedforward=512, dropout=0.1):\n        super().__init__()\n        self.transformer = nn.Transformer(d_model=emb_size,\n                                          nhead=nhead,\n                                          num_encoder_layers=num_encoder_layers,\n                                          num_decoder_layers=num_decoder_layers,\n                                          dim_feedforward=dim_feedforward,\n                                          dropout=dropout,\n                                          batch_first=True)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_token_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_token_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n    def forward(self, src, tgt, src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask):\n        src_emb = self.positional_encoding(self.src_token_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_token_emb(tgt))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n                                src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n    def encode(self, src, src_mask):\n        return self.transformer.encoder(self.positional_encoding(self.src_token_emb(src)), src_mask)\n    def decode(self, tgt, memory, tgt_mask):\n        return self.transformer.decoder(self.positional_encoding(self.tgt_token_emb(tgt)), memory, tgt_mask)\n\n# 自定义数据集\nclass Seq2SeqDataset(Dataset):\n    def __init__(self, encode_datas, decode_datas):\n        super().__init__()\n        self.encode_datas = encode_datas\n        self.decode_datas = decode_datas\n        self.encode_vocab = self.build_vocab(encode_datas, fill_mask = ['PAD', 'EOS', 'UNK'])\n        self.decode_vocab = self.build_vocab(decode_datas, fill_mask = ['PAD', 'BOS', 'EOS', 'UNK'])\n    def __getitem__(self, index):\n        enc = list(self.encode_datas[index]) + ['EOS']\n        dec = ['BOS'] + list(self.decode_datas[index]) + ['EOS']\n        e = [self.encode_vocab.get(tk, self.encode_vocab['UNK']) for tk in enc]\n        d = [self.decode_vocab.get(tk, self.decode_vocab['UNK']) for tk in dec]\n        return e,d\n    def __len__(self):\n        return len(self.encode_datas)\n    # 构建词汇表\n    def build_vocab(self, datas, fill_mask):\n        vocab = OrderedDict({msk: idx for idx, msk in enumerate(fill_mask)})\n        for item in datas:\n            for token in list(item):\n                vocab[token] = vocab.get(token, len(vocab))\n        return vocab\n\ndef read_couplet(path):\n    datas = []\n    with open(path, encoding='utf-8') as f:\n        lines = f.readlines()\n        for l in lines:\n            datas.append(l.strip().split())\n    return datas\n\ndef build_dataloader(dataset, batch_size, shuffle = False):\n    def collate_batch(batch):\n        encode_list, decode_list = [], []\n        for encode, decode in batch:\n            encode_list.append(torch.tensor(encode, dtype=torch.int64))\n            decode_list.append(torch.tensor(decode, dtype=torch.int64))\n        encode_list = pad_sequence(encode_list, batch_first=True, padding_value=0)\n        decode_list = pad_sequence(decode_list, batch_first=True, padding_value=0)\n        return encode_list, decode_list\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_batch)\ndef generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[1]\n    tgt_seq_len = tgt.shape[1]\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len, src_seq_len)).type(torch.bool)\n    src_padding_mask = src == 0\n    tgt_padding_mask = tgt == 0\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n# 训练模型\ndef train_epoch(epoch, train_dl, model, loss_fn, optimizer):\n    model.train()\n    losses = 0\n    train_bar = tqdm(train_dl)\n    for src, tgt in train_bar:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n        tgt_input = tgt[:, :-1]\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = src_mask.to(DEVICE), tgt_mask.to(DEVICE), src_padding_mask.to(DEVICE), tgt_padding_mask.to(DEVICE)\n        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n        optimizer.zero_grad()\n        tgt_output = tgt[:, 1:]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))\n        loss.backward()\n        optimizer.step()\n        losses += loss.item()\n        train_bar.set_description(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n    avg_loss = losses/len(train_dl)\n    print(f'epoch={epoch + 1}, avg_loss={avg_loss}')\n    return losses/len(train_dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T13:37:27.986486Z","iopub.execute_input":"2025-05-14T13:37:27.986701Z","iopub.status.idle":"2025-05-14T13:37:31.188638Z","shell.execute_reply.started":"2025-05-14T13:37:27.986679Z","shell.execute_reply":"2025-05-14T13:37:31.187720Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"encode_datas = read_couplet('/kaggle/input/couplet-test-data/in.txt')\ndecode_datas = read_couplet('/kaggle/input/couplet-test-data/out.txt')\ndataset = Seq2SeqDataset(encode_datas, decode_datas)\n# 初始化超参数\nLR = 0.0001\nSRC_VOCAB_SIZE = len(dataset.encode_vocab)\nTGT_VOCAB_SIZE = len(dataset.decode_vocab)\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 2048\nBATCH_SIZE = 64\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\nNUM_EPOCHS = 60\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# 模型初始化\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\ntransformer = transformer.to(DEVICE)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(transformer.parameters(), lr=LR)\n# 训练模型\ntrain_dl = build_dataloader(dataset, BATCH_SIZE)\nfor epoch in range(NUM_EPOCHS):\n    train_epoch(epoch, train_dl, transformer, loss_fn, optimizer)\ntorch.save(transformer.state_dict(), 'model_new.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T13:37:38.496720Z","iopub.execute_input":"2025-05-14T13:37:38.497019Z","iopub.status.idle":"2025-05-14T13:44:16.272143Z","shell.execute_reply.started":"2025-05-14T13:37:38.496993Z","shell.execute_reply":"2025-05-14T13:44:16.271396Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/63 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\nEpoch 1, Loss: 2.8482983112335205: 100%|██████████| 63/63 [00:06<00:00, 10.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=1, avg_loss=3.1825558533744207\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2, Loss: 2.74883770942688: 100%|██████████| 63/63 [00:05<00:00, 11.34it/s]  \n","output_type":"stream"},{"name":"stdout","text":"epoch=2, avg_loss=2.6722925466204446\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3, Loss: 2.684967041015625: 100%|██████████| 63/63 [00:05<00:00, 11.17it/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch=3, avg_loss=2.6173970207335455\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4, Loss: 2.6365303993225098: 100%|██████████| 63/63 [00:05<00:00, 11.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=4, avg_loss=2.5777441547030495\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5, Loss: 2.602055311203003: 100%|██████████| 63/63 [00:05<00:00, 10.96it/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch=5, avg_loss=2.5468029294695174\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6, Loss: 2.5356009006500244: 100%|██████████| 63/63 [00:05<00:00, 10.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=6, avg_loss=2.512404835413373\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7, Loss: 2.4306538105010986: 100%|██████████| 63/63 [00:05<00:00, 10.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=7, avg_loss=2.4639988815973677\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8, Loss: 2.3530731201171875: 100%|██████████| 63/63 [00:05<00:00, 10.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=8, avg_loss=2.412877457482474\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9, Loss: 2.242774724960327: 100%|██████████| 63/63 [00:06<00:00, 10.47it/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch=9, avg_loss=2.3617190709189764\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10, Loss: 2.1533074378967285: 100%|██████████| 63/63 [00:06<00:00, 10.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=10, avg_loss=2.315885753858657\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11, Loss: 2.074043035507202: 100%|██████████| 63/63 [00:06<00:00, 10.13it/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch=11, avg_loss=2.258560712375338\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12, Loss: 1.9774694442749023: 100%|██████████| 63/63 [00:06<00:00, 10.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=12, avg_loss=2.2029240017845515\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13, Loss: 1.8835729360580444: 100%|██████████| 63/63 [00:06<00:00,  9.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=13, avg_loss=2.14374479982588\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14, Loss: 1.7768335342407227: 100%|██████████| 63/63 [00:06<00:00,  9.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=14, avg_loss=2.092683220666552\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15, Loss: 1.7136436700820923: 100%|██████████| 63/63 [00:06<00:00,  9.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=15, avg_loss=2.0452617387922984\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16, Loss: 1.6420565843582153: 100%|██████████| 63/63 [00:06<00:00,  9.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=16, avg_loss=2.002429349081857\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17, Loss: 1.5596915483474731: 100%|██████████| 63/63 [00:06<00:00,  9.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=17, avg_loss=1.9445921144788227\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18, Loss: 1.4509209394454956: 100%|██████████| 63/63 [00:07<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=18, avg_loss=1.8783104608929346\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19, Loss: 1.358810544013977: 100%|██████████| 63/63 [00:07<00:00,  8.84it/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch=19, avg_loss=1.821823307446071\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20, Loss: 1.2800512313842773: 100%|██████████| 63/63 [00:07<00:00,  9.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=20, avg_loss=1.7703121824870034\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21, Loss: 1.1821765899658203: 100%|██████████| 63/63 [00:06<00:00,  9.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=21, avg_loss=1.7197475509038047\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22, Loss: 1.1165297031402588: 100%|██████████| 63/63 [00:06<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=22, avg_loss=1.6713562238784063\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23, Loss: 1.0431289672851562: 100%|██████████| 63/63 [00:06<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=23, avg_loss=1.6218054237819852\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24, Loss: 0.9686458110809326: 100%|██████████| 63/63 [00:06<00:00,  9.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=24, avg_loss=1.5664033303185114\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25, Loss: 0.8697195053100586: 100%|██████████| 63/63 [00:06<00:00,  9.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=25, avg_loss=1.5096670502708072\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26, Loss: 0.832181990146637: 100%|██████████| 63/63 [00:06<00:00,  9.45it/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch=26, avg_loss=1.4528756264656308\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27, Loss: 0.7546163201332092: 100%|██████████| 63/63 [00:06<00:00,  9.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=27, avg_loss=1.3982609027907962\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28, Loss: 0.6864866614341736: 100%|██████████| 63/63 [00:06<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=28, avg_loss=1.3500013587966797\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29, Loss: 0.6386058330535889: 100%|██████████| 63/63 [00:06<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=29, avg_loss=1.300939505062406\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30, Loss: 0.5676818490028381: 100%|██████████| 63/63 [00:06<00:00,  9.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=30, avg_loss=1.2572637030056544\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31, Loss: 0.5394834280014038: 100%|██████████| 63/63 [00:06<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=31, avg_loss=1.2087750624096583\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32, Loss: 0.5003144145011902: 100%|██████████| 63/63 [00:06<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=32, avg_loss=1.1575375123629494\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33, Loss: 0.43488609790802: 100%|██████████| 63/63 [00:06<00:00,  9.27it/s]  \n","output_type":"stream"},{"name":"stdout","text":"epoch=33, avg_loss=1.1044778937385196\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34, Loss: 0.40678152441978455: 100%|██████████| 63/63 [00:06<00:00,  9.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=34, avg_loss=1.0411704300888\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35, Loss: 0.37820684909820557: 100%|██████████| 63/63 [00:06<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=35, avg_loss=0.9972410921066527\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36, Loss: 0.3285517394542694: 100%|██████████| 63/63 [00:06<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=36, avg_loss=0.9385514841193244\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37, Loss: 0.2885262668132782: 100%|██████████| 63/63 [00:06<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=37, avg_loss=0.8842848960369353\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38, Loss: 0.26354658603668213: 100%|██████████| 63/63 [00:06<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=38, avg_loss=0.8313889550784278\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39, Loss: 0.24906182289123535: 100%|██████████| 63/63 [00:06<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=39, avg_loss=0.7905436148719182\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40, Loss: 0.22239446640014648: 100%|██████████| 63/63 [00:06<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=40, avg_loss=0.7387698141355363\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41, Loss: 0.20700079202651978: 100%|██████████| 63/63 [00:06<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=41, avg_loss=0.6988023141073803\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42, Loss: 0.19877149164676666: 100%|██████████| 63/63 [00:06<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=42, avg_loss=0.6596921500232484\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43, Loss: 0.1709120124578476: 100%|██████████| 63/63 [00:06<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=43, avg_loss=0.6134581066786297\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44, Loss: 0.15418429672718048: 100%|██████████| 63/63 [00:06<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=44, avg_loss=0.5807452686722316\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45, Loss: 0.1533908247947693: 100%|██████████| 63/63 [00:06<00:00,  9.38it/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch=45, avg_loss=0.5405890061741784\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46, Loss: 0.15430134534835815: 100%|██████████| 63/63 [00:06<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=46, avg_loss=0.510881804757648\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47, Loss: 0.12811340391635895: 100%|██████████| 63/63 [00:06<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=47, avg_loss=0.47812619781683363\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48, Loss: 0.12954308092594147: 100%|██████████| 63/63 [00:06<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=48, avg_loss=0.44675822745239924\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49, Loss: 0.11758356541395187: 100%|██████████| 63/63 [00:06<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=49, avg_loss=0.41497572345866096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50, Loss: 0.10317942500114441: 100%|██████████| 63/63 [00:06<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=50, avg_loss=0.3851585473333086\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51, Loss: 0.11148757487535477: 100%|██████████| 63/63 [00:06<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=51, avg_loss=0.3624423752937998\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52, Loss: 0.09324676543474197: 100%|██████████| 63/63 [00:06<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=52, avg_loss=0.33284033231792\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53, Loss: 0.09522426873445511: 100%|██████████| 63/63 [00:06<00:00,  9.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=53, avg_loss=0.30486582089511177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54, Loss: 0.08078301697969437: 100%|██████████| 63/63 [00:06<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=54, avg_loss=0.2804211563770733\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55, Loss: 0.07565359026193619: 100%|██████████| 63/63 [00:06<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=55, avg_loss=0.26027760465466787\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56, Loss: 0.07522259652614594: 100%|██████████| 63/63 [00:06<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=56, avg_loss=0.24225775779239714\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57, Loss: 0.059968963265419006: 100%|██████████| 63/63 [00:06<00:00,  9.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=57, avg_loss=0.2243859630728525\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58, Loss: 0.0752275139093399: 100%|██████████| 63/63 [00:06<00:00,  9.43it/s] \n","output_type":"stream"},{"name":"stdout","text":"epoch=58, avg_loss=0.20570194650264012\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59, Loss: 0.06385817378759384: 100%|██████████| 63/63 [00:06<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=59, avg_loss=0.19258426780265475\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60, Loss: 0.05539719760417938: 100%|██████████| 63/63 [00:06<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"epoch=60, avg_loss=0.18152227997779846\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 推理\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src, src_mask = src.to(DEVICE), src_mask.to(DEVICE)\n    memory = model.encode(src, src_mask)\n    # decode的第一个token：BOS\n    ys = torch.ones(1,1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.shape[1])).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n        ys = torch.cat([ys, torch.ones(1,1).fill_(next_word).to(DEVICE)], dim=1)\n        if next_word == dataset.decode_vocab['EOS']:\n            break\n    return ys\ndef translate(model, src_sentence):\n    model.eval()\n    src = torch.tensor([dataset.encode_vocab[tk] for tk in list(src_sentence) + ['EOS']]).reshape(1, -1)\n    num_tokens = src.shape[1]\n    src_mask = torch.zeros(num_tokens, num_tokens).type(torch.bool)\n    tgt_tokens = greedy_decode(model, src, src_mask, num_tokens, dataset.decode_vocab['BOS']).reshape(-1)\n    return ''.join([decode_vocab_rev[tk] for tk in list(tgt_tokens.cpu().numpy())][1:-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T13:59:04.048928Z","iopub.execute_input":"2025-05-14T13:59:04.049256Z","iopub.status.idle":"2025-05-14T13:59:04.056633Z","shell.execute_reply.started":"2025-05-14T13:59:04.049228Z","shell.execute_reply":"2025-05-14T13:59:04.055698Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"decode_vocab_rev = {v:k for k, v in dataset.decode_vocab.items()}\n# transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n#                                      NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n# transformer = transformer.to(DEVICE)\n# transformer.load_state_dict(torch.load(\"kaggle_output/model_new.pth\"))\nsen_enc = '陆才吟岁，心定方知时日快'\nsen_dec = translate(transformer, sen_enc)\nprint(sen_enc)\nprint(sen_dec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T13:59:10.206674Z","iopub.execute_input":"2025-05-14T13:59:10.206952Z","iopub.status.idle":"2025-05-14T13:59:10.267035Z","shell.execute_reply.started":"2025-05-14T13:59:10.206929Z","shell.execute_reply":"2025-05-14T13:59:10.266266Z"}},"outputs":[{"name":"stdout","text":"陆才吟岁，心定方知时日快\n果真情人常是，应当达民族\n","output_type":"stream"}],"execution_count":18}]}