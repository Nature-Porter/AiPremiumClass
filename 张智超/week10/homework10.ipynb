{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":318737,"sourceType":"datasetVersion","datasetId":134082}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:06:11.631808Z","iopub.execute_input":"2025-05-22T05:06:11.632597Z","iopub.status.idle":"2025-05-22T05:06:11.897408Z","shell.execute_reply.started":"2025-05-22T05:06:11.632564Z","shell.execute_reply":"2025-05-22T05:06:11.896743Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:06:14.935301Z","iopub.execute_input":"2025-05-22T05:06:14.935632Z","iopub.status.idle":"2025-05-22T05:06:38.763431Z","shell.execute_reply.started":"2025-05-22T05:06:14.935615Z","shell.execute_reply":"2025-05-22T05:06:38.762635Z"}},"outputs":[{"name":"stderr","text":"2025-05-22 05:06:27.745402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747890387.925915      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747890387.980998      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 处理数据\ndef handleComments(path):\n    df = pd.read_excel(path)\n    # print(df.columns)\n    content_name = '评价内容(content)'\n    score_name = '评分（总分5分）(score)'\n    df = df[df[content_name].notna() & (df[content_name] != '此用户未填写评价内容') & (df[content_name] != '您没有填写内容，默认好评')]\n    comments_list = df[[content_name, score_name]].values.tolist()\n    # print(len(data_list))\n    return comments_list\ndef build_collate_fn(tokenizer):\n    def collate_fn(batch):\n        comments, scores = zip(*batch)\n        token_encodings = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n        scores = torch.tensor(scores) - 1\n        return token_encodings['input_ids'], token_encodings['attention_mask'], scores\n    return collate_fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:06:43.492829Z","iopub.execute_input":"2025-05-22T05:06:43.493931Z","iopub.status.idle":"2025-05-22T05:06:43.502966Z","shell.execute_reply.started":"2025-05-22T05:06:43.493894Z","shell.execute_reply":"2025-05-22T05:06:43.502046Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"comments_list = handleComments('/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx')\ntrain_list, test_list = train_test_split(comments_list, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:06:47.164613Z","iopub.execute_input":"2025-05-22T05:06:47.164889Z","iopub.status.idle":"2025-05-22T05:06:57.202798Z","shell.execute_reply.started":"2025-05-22T05:06:47.164866Z","shell.execute_reply":"2025-05-22T05:06:57.202234Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMODEL_NAME = 'google-bert/bert-base-chinese'\nEPOCHS = 3\nLR = 1e-5\nBATCH_SIZE = 32 # 批次太大，可能会导致：OutOfMemoryError: CUDA out of memory\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntrain_dataLoader = DataLoader(train_list, batch_size=BATCH_SIZE, shuffle=True, collate_fn=build_collate_fn(tokenizer))\ntest_dataLoader = DataLoader(test_list, batch_size=BATCH_SIZE, collate_fn=build_collate_fn(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:07:01.519017Z","iopub.execute_input":"2025-05-22T05:07:01.519949Z","iopub.status.idle":"2025-05-22T05:07:04.438213Z","shell.execute_reply.started":"2025-05-22T05:07:01.519924Z","shell.execute_reply":"2025-05-22T05:07:04.437286Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3554906324f640e98b19bfddccf15989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce5d2c91b484e0c9296bf819ef5f7c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05ab01525d094f8880a5bccb6438ad9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79a05378b4974f72a66a7e63abe6c063"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# 加载模型进行训练\ndef train(trainable):\n    log_dir = '/kaggle/working/runs/trainable_true' if trainable else '/kaggle/working/runs/trainable_false'\n    writer = SummaryWriter(log_dir=log_dir)\n    # 加载模型\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5)\n    # False:冻结bert, True:不冻结bert\n    model.bert.trainable = trainable\n    model.to(DEVICE)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n    # 进行训练\n    for epoch in range(EPOCHS):\n        train_bar = tqdm(train_dataLoader)\n        model.train()\n        for i, (input_ids, attention_mask, scores) in enumerate(train_bar):\n            input_ids, attention_mask, scores = input_ids.to(DEVICE), attention_mask.to(DEVICE), scores.to(DEVICE)\n            logits = model(input_ids=input_ids, attention_mask=attention_mask)['logits']\n            loss = loss_fn(logits, scores)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            writer.add_scalar(\"loss\", loss.item(), epoch * len(train_dataLoader) + i)\n            train_bar.set_description(f\"Epoch={epoch + 1}, Loss={loss.item()}\")\n        model.eval()\n        with torch.no_grad():\n            total = 0\n            correct = 0\n            for input_ids, attention_mask, scores in test_dataLoader:\n                input_ids, attention_mask, scores = input_ids.to(DEVICE), attention_mask.to(DEVICE), scores.to(DEVICE)\n                logits = model(input_ids=input_ids, attention_mask=attention_mask)['logits']\n                pred = torch.argmax(logits, dim=1)\n                total += len(scores)\n                correct += (pred == scores).sum().item()\n            accuracy = 100 * correct / total\n            print(f\"Epoch {epoch + 1}, Accuracy: {accuracy}, Total: {total}, Correct: {correct}\")\n    writer.close()\n    # 保存模型\n    save_name = 'model_true.pth' if trainable else 'model_false.pth'\n    torch.save(model.state_dict(), save_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:07:09.095329Z","iopub.execute_input":"2025-05-22T05:07:09.096078Z","iopub.status.idle":"2025-05-22T05:07:09.104307Z","shell.execute_reply.started":"2025-05-22T05:07:09.096051Z","shell.execute_reply":"2025-05-22T05:07:09.103613Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def predict(comments, trainable):\n    model_path = '/kaggle/working/model_true.pth' if trainable else '/kaggle/working/model_false.pth'\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5)\n    model.load_state_dict(torch.load(model_path))\n    model.to(DEVICE)\n    model.eval()\n    token_encodings = tokenizer(comments, padding=True, truncation=True, return_tensors=\"pt\")\n    input_ids, attention_mask = token_encodings['input_ids'].to(DEVICE), token_encodings['attention_mask'].to(DEVICE)\n    logits = model(input_ids=input_ids, attention_mask=attention_mask)['logits']\n    pred = torch.argmax(logits, dim=1) + 1\n    return pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:07:25.352884Z","iopub.execute_input":"2025-05-22T05:07:25.353188Z","iopub.status.idle":"2025-05-22T05:07:25.358476Z","shell.execute_reply.started":"2025-05-22T05:07:25.353165Z","shell.execute_reply":"2025-05-22T05:07:25.357629Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# 冻结bert\ntrain(False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:07:29.056247Z","iopub.execute_input":"2025-05-22T05:07:29.056760Z","iopub.status.idle":"2025-05-22T05:35:37.019902Z","shell.execute_reply.started":"2025-05-22T05:07:29.056739Z","shell.execute_reply":"2025-05-22T05:35:37.019102Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e4b14bd561455198ee1d8705756d95"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch=1, Loss=0.08166500926017761: 100%|██████████| 1067/1067 [08:44<00:00,  2.03it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Accuracy: 94.13005272407733, Total: 8535, Correct: 8034\n","output_type":"stream"},{"name":"stderr","text":"Epoch=2, Loss=0.14433038234710693: 100%|██████████| 1067/1067 [08:41<00:00,  2.04it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Accuracy: 94.13005272407733, Total: 8535, Correct: 8034\n","output_type":"stream"},{"name":"stderr","text":"Epoch=3, Loss=0.08118396997451782: 100%|██████████| 1067/1067 [08:41<00:00,  2.05it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Accuracy: 93.90743995313416, Total: 8535, Correct: 8015\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(predict(['这款手机真的太让我惊喜了！外观设计简约又高级，拿在手里质感十足。',\n              '对这个手机真的太失望了！刚用没多久就频繁出现卡顿现象，打开几个应用就卡得不行'], False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:47:17.803178Z","iopub.execute_input":"2025-05-22T05:47:17.803708Z","iopub.status.idle":"2025-05-22T05:47:18.992255Z","shell.execute_reply.started":"2025-05-22T05:47:17.803685Z","shell.execute_reply":"2025-05-22T05:47:18.991625Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"tensor([5, 1], device='cuda:0')\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 不冻结bert\ntrain(True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T05:47:55.986647Z","iopub.execute_input":"2025-05-22T05:47:55.987346Z","iopub.status.idle":"2025-05-22T06:16:01.820472Z","shell.execute_reply.started":"2025-05-22T05:47:55.987321Z","shell.execute_reply":"2025-05-22T06:16:01.819666Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch=1, Loss=0.2219211608171463: 100%|██████████| 1067/1067 [08:41<00:00,  2.05it/s]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Accuracy: 94.27065026362038, Total: 8535, Correct: 8046\n","output_type":"stream"},{"name":"stderr","text":"Epoch=2, Loss=0.3838520348072052: 100%|██████████| 1067/1067 [08:45<00:00,  2.03it/s]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Accuracy: 94.11833626244874, Total: 8535, Correct: 8033\n","output_type":"stream"},{"name":"stderr","text":"Epoch=3, Loss=0.12285474687814713: 100%|██████████| 1067/1067 [08:43<00:00,  2.04it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Accuracy: 94.17691857059168, Total: 8535, Correct: 8038\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(predict(['这款手机真的太让我惊喜了！外观设计简约又高级，拿在手里质感十足。',\n              '对这个手机真的太失望了！刚用没多久就频繁出现卡顿现象，打开几个应用就卡得不行'], True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T06:21:22.217543Z","iopub.execute_input":"2025-05-22T06:21:22.218226Z","iopub.status.idle":"2025-05-22T06:21:23.370532Z","shell.execute_reply.started":"2025-05-22T06:21:22.218200Z","shell.execute_reply":"2025-05-22T06:21:23.369794Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"tensor([5, 1], device='cuda:0')\n","output_type":"stream"}],"execution_count":12}]}