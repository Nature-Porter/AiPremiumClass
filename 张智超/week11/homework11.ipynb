{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-06T07:57:37.718615Z",
     "iopub.status.busy": "2025-06-06T07:57:37.717914Z",
     "iopub.status.idle": "2025-06-06T07:57:37.722855Z",
     "shell.execute_reply": "2025-06-06T07:57:37.721944Z",
     "shell.execute_reply.started": "2025-06-06T07:57:37.718593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:32:23.491797Z",
     "iopub.status.busy": "2025-06-06T07:32:23.491188Z",
     "iopub.status.idle": "2025-06-06T07:32:23.496094Z",
     "shell.execute_reply": "2025-06-06T07:32:23.495276Z",
     "shell.execute_reply.started": "2025-06-06T07:32:23.491772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:32:26.811803Z",
     "iopub.status.busy": "2025-06-06T07:32:26.811082Z",
     "iopub.status.idle": "2025-06-06T07:32:29.203832Z",
     "shell.execute_reply": "2025-06-06T07:32:29.203047Z",
     "shell.execute_reply.started": "2025-06-06T07:32:26.811775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49db41c8b00946a09b86f319ff9fb965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/697 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67075253f73942a3a63dc93b382b1693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-42717a92413393f9.parquet:   0%|          | 0.00/13.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a752e1e39d4255b409fe8384e728dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-8899cab5fdab45bc.parquet:   0%|          | 0.00/946k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692f113ca32047d6b1bb448e439187f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/45001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6618d9f34c648e49b47be531bfaf30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 数据预处理\n",
    "ds = load_dataset(\"doushabao4766/msra_ner_k_V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:32:34.046585Z",
     "iopub.status.busy": "2025-06-06T07:32:34.046051Z",
     "iopub.status.idle": "2025-06-06T07:32:37.466146Z",
     "shell.execute_reply": "2025-06-06T07:32:37.465293Z",
     "shell.execute_reply.started": "2025-06-06T07:32:34.046552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef4d132ecb740f4afe8384fd1f6d048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/45001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcc6987c45d4bb48425b7377c28480e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n",
       "        num_rows: 3442\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对ds中的数据进行过滤:过滤掉tokens为空的数据\n",
    "def data_filter(item):\n",
    "    return len(item['tokens']) > 0\n",
    "ds['train'] = ds['train'].filter(data_filter)\n",
    "ds['test'] = ds['test'].filter(data_filter)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:32:41.371956Z",
     "iopub.status.busy": "2025-06-06T07:32:41.371192Z",
     "iopub.status.idle": "2025-06-06T07:32:41.376376Z",
     "shell.execute_reply": "2025-06-06T07:32:41.375292Z",
     "shell.execute_reply.started": "2025-06-06T07:32:41.371930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tags = ds['train'].features['ner_tags'].feature.names\n",
    "entites = ['O', 'PER', 'ORG', 'LOC']\n",
    "entity_index = {e:i for i,e in enumerate(entites)}\n",
    "model_name = 'google-bert/bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:32:45.136282Z",
     "iopub.status.busy": "2025-06-06T07:32:45.136004Z",
     "iopub.status.idle": "2025-06-06T07:32:45.767015Z",
     "shell.execute_reply": "2025-06-06T07:32:45.766483Z",
     "shell.execute_reply.started": "2025-06-06T07:32:45.136263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a77f9f507cd4871ab89cf7c8340a74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a832cb2839464aab6ae654531342dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2241377ec36c4903b59e7bc681f1c2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c1e3752a0f4f55a7e86df40ef922df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_length = tokenizer.model_max_length # 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:32:50.607481Z",
     "iopub.status.busy": "2025-06-06T07:32:50.606894Z",
     "iopub.status.idle": "2025-06-06T07:32:50.613221Z",
     "shell.execute_reply": "2025-06-06T07:32:50.612314Z",
     "shell.execute_reply.started": "2025-06-06T07:32:50.607459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def data_input_proc(item):\n",
    "    input_data_list = []\n",
    "    # 对tokens进行分词,而不是将tokens合并成句子再分词,因为合并成句子再分词会导致input_ids的长度和ner_tags的长度不一致\n",
    "    for one in item['tokens']:\n",
    "        # 截取最大长度否则在模型训练时会提示:The size of tensor a (706) must match the size of tensor b (512) at non-singleton dimension 1\n",
    "        if len(one) > max_length:\n",
    "            one = one[:512]\n",
    "        input_data_one = tokenizer(one, truncation=True, add_special_tokens=False, max_length=512)\n",
    "        adjust_input_data = {key: [i for idarr in value for i in idarr] for key,value in input_data_one.items()}\n",
    "        input_data_list.append(adjust_input_data)\n",
    "    input_data = {}\n",
    "    for input_data_one in input_data_list:\n",
    "        for k,v in input_data_one.items():\n",
    "            input_data.setdefault(k, []).append(v)\n",
    "    # 对ner_tags的长度也进行截取和input_data长度一致\n",
    "    ner_tags = [n[:512] for n in item['ner_tags']]\n",
    "    # DataCollatorForTokenClassification中需要有labels这个标签\n",
    "    input_data['labels'] = ner_tags\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:32:54.841108Z",
     "iopub.status.busy": "2025-06-06T07:32:54.840843Z",
     "iopub.status.idle": "2025-06-06T07:33:27.672867Z",
     "shell.execute_reply": "2025-06-06T07:33:27.672292Z",
     "shell.execute_reply.started": "2025-06-06T07:32:54.841089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7a73280f3d4cbf9e3596cb5d6a6e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66723e4f1d045919a25a136cf0075d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3442 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(data_input_proc, batched=True)\n",
    "# subds = ds['train'].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:33:31.821645Z",
     "iopub.status.busy": "2025-06-06T07:33:31.820945Z",
     "iopub.status.idle": "2025-06-06T07:33:31.825910Z",
     "shell.execute_reply": "2025-06-06T07:33:31.825077Z",
     "shell.execute_reply.started": "2025-06-06T07:33:31.821622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds.set_format(type=\"torch\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T08:31:23.694772Z",
     "iopub.status.busy": "2025-06-06T08:31:23.694174Z",
     "iopub.status.idle": "2025-06-06T08:31:23.726817Z",
     "shell.execute_reply": "2025-06-06T08:31:23.726087Z",
     "shell.execute_reply.started": "2025-06-06T08:31:23.694751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='ner_train',\n",
    "    num_train_epochs=3,\n",
    "    save_safetensors=False,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to='tensorboard',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T08:31:27.314810Z",
     "iopub.status.busy": "2025-06-06T08:31:27.314228Z",
     "iopub.status.idle": "2025-06-06T08:31:27.499862Z",
     "shell.execute_reply": "2025-06-06T08:31:27.499140Z",
     "shell.execute_reply.started": "2025-06-06T08:31:27.314787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {i:tag for i, tag in enumerate(tags)}\n",
    "label2id = {tag:i for i, tag in enumerate(tags)}\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=7, id2label=id2label, label2id=label2id)\n",
    "# label_pad_token_id默认为-100\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T08:31:31.921048Z",
     "iopub.status.busy": "2025-06-06T08:31:31.920340Z",
     "iopub.status.idle": "2025-06-06T08:31:31.926075Z",
     "shell.execute_reply": "2025-06-06T08:31:31.925192Z",
     "shell.execute_reply.started": "2025-06-06T08:31:31.921024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# result是EvalPrediction类型的一个简单数据类,包含两个主要属性:predictions,label_ids,一般都是numpy数组\n",
    "def compute_metrics(result):\n",
    "    predicts,labels = result\n",
    "    # predicts.shape = (样本数量, padding后的sequence_length, num_labels)\n",
    "    # labels.shape = (样本数量, padding后的sequence_length)\n",
    "    # 获取评估对象\n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predicts = np.argmax(predicts, axis=2)\n",
    "    # 准备评估数据\n",
    "    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    labels = [[tags[l] for l in ls if l != -100]\n",
    "                 for ls in labels]\n",
    "    results = seqeval.compute(predictions=predicts, references=labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T08:31:36.475302Z",
     "iopub.status.busy": "2025-06-06T08:31:36.474678Z",
     "iopub.status.idle": "2025-06-06T08:31:36.648448Z",
     "shell.execute_reply": "2025-06-06T08:31:36.647653Z",
     "shell.execute_reply.started": "2025-06-06T08:31:36.475280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model, \n",
    "    train_args,\n",
    "    train_dataset=ds['train'],\n",
    "    eval_dataset=ds['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T08:31:39.323411Z",
     "iopub.status.busy": "2025-06-06T08:31:39.322817Z",
     "iopub.status.idle": "2025-06-06T09:08:04.998571Z",
     "shell.execute_reply": "2025-06-06T09:08:04.997861Z",
     "shell.execute_reply.started": "2025-06-06T08:31:39.323390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2112' max='2112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2112/2112 36:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Org</th>\n",
       "      <th>Per</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.030972</td>\n",
       "      <td>{'precision': 0.9194050501556554, 'recall': 0.9319775596072931, 'f1': 0.9256486157060768, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.7833787465940054, 'recall': 0.8712121212121212, 'f1': 0.8249641319942611, 'number': 1320}</td>\n",
       "      <td>{'precision': 0.92488860598345, 'recall': 0.9667332002661344, 'f1': 0.9453480806766428, 'number': 1503}</td>\n",
       "      <td>0.887184</td>\n",
       "      <td>0.927048</td>\n",
       "      <td>0.906678</td>\n",
       "      <td>0.990814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.027452</td>\n",
       "      <td>{'precision': 0.9533005617977528, 'recall': 0.9519635343618513, 'f1': 0.9526315789473685, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.8420684835779175, 'recall': 0.9128787878787878, 'f1': 0.8760450745183569, 'number': 1320}</td>\n",
       "      <td>{'precision': 0.9534120734908137, 'recall': 0.9667332002661344, 'f1': 0.9600264288074001, 'number': 1503}</td>\n",
       "      <td>0.925900</td>\n",
       "      <td>0.946784</td>\n",
       "      <td>0.936226</td>\n",
       "      <td>0.992511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.028544</td>\n",
       "      <td>{'precision': 0.9590540063536886, 'recall': 0.9526647966339411, 'f1': 0.95584872471416, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.888807607900512, 'recall': 0.9204545454545454, 'f1': 0.9043542984741347, 'number': 1320}</td>\n",
       "      <td>{'precision': 0.9530733641771315, 'recall': 0.959414504324684, 'f1': 0.9562334217506632, 'number': 1503}</td>\n",
       "      <td>0.940662</td>\n",
       "      <td>0.946960</td>\n",
       "      <td>0.943800</td>\n",
       "      <td>0.993486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9194050501556554, 'recall': 0.9319775596072931, 'f1': 0.9256486157060768, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7833787465940054, 'recall': 0.8712121212121212, 'f1': 0.8249641319942611, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.92488860598345, 'recall': 0.9667332002661344, 'f1': 0.9453480806766428, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9533005617977528, 'recall': 0.9519635343618513, 'f1': 0.9526315789473685, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8420684835779175, 'recall': 0.9128787878787878, 'f1': 0.8760450745183569, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9534120734908137, 'recall': 0.9667332002661344, 'f1': 0.9600264288074001, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9590540063536886, 'recall': 0.9526647966339411, 'f1': 0.95584872471416, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.888807607900512, 'recall': 0.9204545454545454, 'f1': 0.9043542984741347, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9530733641771315, 'recall': 0.959414504324684, 'f1': 0.9562334217506632, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2112, training_loss=0.02307896087454124, metrics={'train_runtime': 2184.6791, 'train_samples_per_second': 61.794, 'train_steps_per_second': 0.967, 'total_flos': 1.1824996653769728e+16, 'train_loss': 0.02307896087454124, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T09:19:51.316389Z",
     "iopub.status.busy": "2025-06-06T09:19:51.316106Z",
     "iopub.status.idle": "2025-06-06T09:19:51.335897Z",
     "shell.execute_reply": "2025-06-06T09:19:51.335282Z",
     "shell.execute_reply.started": "2025-06-06T09:19:51.316368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': 0.9998104, 'index': 10, 'word': '中', 'start': 9, 'end': 10}, {'entity': 'B-LOC', 'score': 0.99977416, 'index': 11, 'word': '美', 'start': 10, 'end': 11}]\n"
     ]
    }
   ],
   "source": [
    "# 进行命名实体识别\n",
    "ner = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "seq = '双方确定了今后发展中美关系的指导方针。'\n",
    "ner_result = ner(seq)\n",
    "print(ner_result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
