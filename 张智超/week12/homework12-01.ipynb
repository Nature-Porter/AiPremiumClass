{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1、手动实现动态学习率、混合精度、DDP训练","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport torch\nfrom transformers import get_linear_schedule_with_warmup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:45:22.346302Z","iopub.execute_input":"2025-06-14T10:45:22.346579Z","iopub.status.idle":"2025-06-14T10:45:22.376138Z","shell.execute_reply.started":"2025-06-14T10:45:22.346557Z","shell.execute_reply":"2025-06-14T10:45:22.375328Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = 'google-bert/bert-base-chinese'\nEPOCHES = 3\n# 数据预处理\nds = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n# 对ds中的数据进行过滤:过滤掉tokens为空的数据\ndef data_filter(item):\n    return len(item['tokens']) > 0\nds['train'] = ds['train'].filter(data_filter)\nds['test'] = ds['test'].filter(data_filter)\ntags = ds['train'].features['ner_tags'].feature.names\nentites = ['O', 'PER', 'ORG', 'LOC']\nentity_index = {e:i for i,e in enumerate(entites)}\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmax_length = tokenizer.model_max_length # 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:45:24.987307Z","iopub.execute_input":"2025-06-14T10:45:24.987882Z","iopub.status.idle":"2025-06-14T10:45:25.932059Z","shell.execute_reply.started":"2025-06-14T10:45:24.987850Z","shell.execute_reply":"2025-06-14T10:45:25.931343Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def data_input_proc(item):\n    input_data_list = []\n    # 对tokens进行分词,而不是将tokens合并成句子再分词,因为合并成句子再分词会导致input_ids的长度和ner_tags的长度不一致\n    # is_split_into_words=True已经分词不需要再分词，https://hf.cloudwisdom.top/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__\n    input_data = tokenizer(item['tokens'], \n                           truncation=True, \n                           add_special_tokens=False, \n                           max_length=512, \n                           is_split_into_words=True,\n                           padding='max_length')\n    # 对ner_tags的长度也进行截取和input_data长度一致\n    ner_tags = [n[:512] for n in item['ner_tags']]\n    ner_tags = [n+[0]*(512-len(n)) for n in ner_tags]\n    # DataCollatorForTokenClassification中需要有labels这个标签\n    input_data['labels'] = ner_tags\n    return input_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:45:30.302496Z","iopub.execute_input":"2025-06-14T10:45:30.303017Z","iopub.status.idle":"2025-06-14T10:45:30.307776Z","shell.execute_reply.started":"2025-06-14T10:45:30.302993Z","shell.execute_reply":"2025-06-14T10:45:30.307008Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"ds = ds.map(data_input_proc, batched=True)\nds.set_format(type=\"torch\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:45:33.682519Z","iopub.execute_input":"2025-06-14T10:45:33.683217Z","iopub.status.idle":"2025-06-14T10:45:35.626472Z","shell.execute_reply.started":"2025-06-14T10:45:33.683193Z","shell.execute_reply":"2025-06-14T10:45:35.625608Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3442 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c07cce83996c49e7a1e8835eecd12088"}},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"def gen_dl_model():\n    train_dl = DataLoader(ds['train'], shuffle=True, batch_size=16)\n    id2label = {i:tag for i, tag in enumerate(tags)}\n    label2id = {tag:i for i, tag in enumerate(tags)}\n    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=7, id2label=id2label, label2id=label2id)\n    return train_dl, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:45:43.080193Z","iopub.execute_input":"2025-06-14T10:45:43.080448Z","iopub.status.idle":"2025-06-14T10:45:43.084958Z","shell.execute_reply.started":"2025-06-14T10:45:43.080429Z","shell.execute_reply":"2025-06-14T10:45:43.084302Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# 模型训练\ntrain_dl, model = gen_dl_model()\nmodel.to(DEVICE)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\nfor epoch in range(EPOCHES):\n    model.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        optimizer.zero_grad()\n        outputs = model(**items)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        tpbar.set_description(f'epoch={epoch+1},loss={loss.item()}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:58:52.445887Z","iopub.execute_input":"2025-06-14T10:58:52.446438Z","iopub.status.idle":"2025-06-14T10:58:59.609699Z","shell.execute_reply.started":"2025-06-14T10:58:52.446412Z","shell.execute_reply":"2025-06-14T10:58:59.608815Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nepoch=1,loss=0.07638794183731079:   0%|          | 3/2813 [00:06<1:45:42,  2.26s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/173860007.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch={epoch+1},loss={loss.item()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":46},{"cell_type":"code","source":"# 动态学习率\ntrain_dl, model = gen_dl_model()\nmodel.to(DEVICE)\nmodel_named_params = list(model.named_parameters())\nbert_params, classifier_params = [], []\nfor name, params in model_named_params:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\nparam_groups = [\n    {'params': bert_params, 'lr':1e-5},\n    {'params': classifier_params, 'lr':1e-3}\n]\noptimizer = optim.AdamW(param_groups)\n# 学习率调度器\ntrain_steps = len(train_dl) * EPOCHES\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=train_steps\n)\nfor epoch in range(EPOCHES):\n    model.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        optimizer.zero_grad()\n        outputs = model(**items)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        tpbar.set_description(f'epoch={epoch+1},' + \n                             f'loss={loss.item()},' + \n                             f'bert_lr={scheduler.get_lr()[0]},' + \n                             f'classifier_lr={scheduler.get_lr()[1]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:51:47.356026Z","iopub.execute_input":"2025-06-14T10:51:47.356299Z","iopub.status.idle":"2025-06-14T10:52:20.407880Z","shell.execute_reply.started":"2025-06-14T10:51:47.356277Z","shell.execute_reply":"2025-06-14T10:52:20.406919Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nepoch=1,loss=1.4927350282669067,bert_lr=1.7000000000000002e-06,classifier_lr=0.00017:   1%|          | 17/2813 [00:32<1:29:46,  1.93s/it]               \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1431334654.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         tpbar.set_description(f'epoch={epoch+1},' + \n\u001b[0;32m---> 35\u001b[0;31m                              \u001b[0;34mf'loss={loss.item()},'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                              \u001b[0;34mf'bert_lr={scheduler.get_lr()[0]},'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                              f'classifier_lr={scheduler.get_lr()[1]}')\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"# 混合精度训练\ntrain_dl, model = gen_dl_model()\nmodel.to(DEVICE)\n# 梯度计算缩放器\nscaler = torch.GradScaler()\n\nmodel_named_params = list(model.named_parameters())\nbert_params, classifier_params = [], []\nfor name, params in model_named_params:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\nparam_groups = [\n    {'params': bert_params, 'lr':1e-5},\n    {'params': classifier_params, 'lr':1e-3}\n]\noptimizer = optim.AdamW(param_groups)\n# 学习率调度器\ntrain_steps = len(train_dl) * EPOCHES\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=train_steps\n)\nfor epoch in range(EPOCHES):\n    model.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        optimizer.zero_grad()\n        with torch.autocast(device_type = 'cuda'):\n            outputs = model(**items)\n        loss = outputs.loss\n        # 缩放loss后，反向传播\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        tpbar.set_description(f'epoch={epoch+1},' + \n                             f'loss={loss.item()},' + \n                             f'bert_lr={scheduler.get_lr()[0]},' + \n                             f'classifier_lr={scheduler.get_lr()[1]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T11:01:47.531198Z","iopub.execute_input":"2025-06-14T11:01:47.531956Z","iopub.status.idle":"2025-06-14T11:02:06.979817Z","shell.execute_reply.started":"2025-06-14T11:01:47.531924Z","shell.execute_reply":"2025-06-14T11:02:06.978898Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nepoch=1,loss=0.09300730377435684,bert_lr=4.1e-06,classifier_lr=0.00041:   1%|▏         | 41/2813 [00:19<21:34,  2.14it/s]                             \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/286022457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 缩放loss后，反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":48},{"cell_type":"markdown","source":"分布式训练","metadata":{}},{"cell_type":"code","source":"%%writefile ddp_simple.py\n\nimport os\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom torch.utils.data import DataLoader, DistributedSampler\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport torch\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# 对ds中的数据进行过滤:过滤掉tokens为空的数据\ndef data_filter(item):\n    return len(item['tokens']) > 0\n\ndef data_input_proc_fn(tokenizer):\n    def data_input_proc(item):\n        input_data_list = []\n        # 对tokens进行分词,而不是将tokens合并成句子再分词,因为合并成句子再分词会导致input_ids的长度和ner_tags的长度不一致\n        # is_split_into_words=True已经分词不需要再分词，https://hf.cloudwisdom.top/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__\n        input_data = tokenizer(item['tokens'], \n                               truncation=True, \n                               add_special_tokens=False, \n                               max_length=512, \n                               is_split_into_words=True,\n                               padding='max_length')\n        # 对ner_tags的长度也进行截取和input_data长度一致\n        ner_tags = [n[:512] for n in item['ner_tags']]\n        ner_tags = [n+[0]*(512-len(n)) for n in ner_tags]\n        # DataCollatorForTokenClassification中需要有labels这个标签\n        input_data['labels'] = ner_tags\n        return input_data\n    return data_input_proc\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group('nccl', rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n# 模型训练\ndef train(rank, world_size):\n    setup(rank, world_size)\n    \n    model_name = 'google-bert/bert-base-chinese'\n    EPOCHES = 3\n    # 数据预处理\n    ds = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n    ds['train'] = ds['train'].filter(data_filter)\n    ds['test'] = ds['test'].filter(data_filter)\n    tags = ds['train'].features['ner_tags'].feature.names\n    entites = ['O', 'PER', 'ORG', 'LOC']\n    entity_index = {e:i for i,e in enumerate(entites)}\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    max_length = tokenizer.model_max_length # 512\n    ds = ds.map(data_input_proc_fn(tokenizer), batched=True)\n    ds.set_format(type=\"torch\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n    \n    # 构建模型\n    id2label = {i:tag for i, tag in enumerate(tags)}\n    label2id = {tag:i for i, tag in enumerate(tags)}\n    model_tmp = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=7, id2label=id2label, label2id=label2id)\n    model_tmp.to(rank)\n    model = DDP(model_tmp, device_ids=[rank])\n    # 分布式训练采样器\n    sampler = DistributedSampler(ds['train'], num_replicas=world_size, rank=rank)\n    train_dl = DataLoader(ds['train'], sampler=sampler, batch_size=16)\n\n    # 动态学习率\n    model_named_params = list(model.named_parameters())\n    bert_params, classifier_params = [], []\n    for name, params in model_named_params:\n        if 'bert' in name:\n            bert_params.append(params)\n        else:\n            classifier_params.append(params)\n    param_groups = [\n        {'params': bert_params, 'lr':1e-5},\n        {'params': classifier_params, 'lr':1e-3}\n    ]\n    optimizer = optim.AdamW(param_groups)\n    # 学习率调度器\n    train_steps = len(train_dl) * EPOCHES\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=100,\n        num_training_steps=train_steps\n    )\n\n    # 梯度计算缩放器(混合精度)\n    scaler = torch.GradScaler()\n    \n    for epoch in range(EPOCHES):\n        model.train()\n        tpbar = tqdm(train_dl)\n        for items in tpbar:\n            items = {k:v.to(rank) for k,v in items.items()}\n            optimizer.zero_grad()\n            with torch.autocast(device_type = 'cuda'):\n                outputs = model(**items)\n            loss = outputs.loss\n            # 缩放loss后，反向传播\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            tpbar.set_description(f'rank={rank},' + \n                                 f'epoch={epoch+1},' + \n                                 f'loss={loss.item()},' + \n                                 f'bert_lr={scheduler.get_lr()[0]},' + \n                                 f'classifier_lr={scheduler.get_lr()[1]}')\n    cleanup()\n\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T11:40:51.174437Z","iopub.execute_input":"2025-06-14T11:40:51.174826Z","iopub.status.idle":"2025-06-14T11:40:51.183817Z","shell.execute_reply.started":"2025-06-14T11:40:51.174795Z","shell.execute_reply":"2025-06-14T11:40:51.183001Z"}},"outputs":[{"name":"stdout","text":"Overwriting ddp_simple.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!python ddp_simple.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T11:40:56.474153Z","iopub.execute_input":"2025-06-14T11:40:56.474525Z","iopub.status.idle":"2025-06-14T12:13:28.160427Z","shell.execute_reply.started":"2025-06-14T11:40:56.474500Z","shell.execute_reply":"2025-06-14T12:13:28.159590Z"}},"outputs":[{"name":"stdout","text":"2025-06-14 11:41:03.050989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749901263.076372     249 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749901263.084284     249 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 11:41:13.539285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749901273.566147     264 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749901273.574525     264 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 11:41:13.639698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749901273.665679     263 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749901273.674106     263 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[W614 11:41:17.839946672 socket.cpp:759] [c10d] The client socket has failed to connect to [localhost]:12355 (errno: 99 - Cannot assign requested address).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nrank=0,epoch=1,loss=0.00232483446598053,bert_lr=6.8284396991021605e-06,classifie\nrank=1,epoch=1,loss=0.010465458035469055,bert_lr=6.8284396991021605e-06,classifi\nrank=0,epoch=2,loss=0.0004858212196268141,bert_lr=3.4142198495510802e-06,classif\nrank=1,epoch=2,loss=0.0031414709519594908,bert_lr=3.4142198495510802e-06,classif\nrank=0,epoch=3,loss=0.0004249613848514855,bert_lr=0.0,classifier_lr=0.0: 100%|█|\nrank=1,epoch=3,loss=0.005716114304959774,bert_lr=0.0,classifier_lr=0.0: 100%|█| \n","output_type":"stream"}],"execution_count":6}]}