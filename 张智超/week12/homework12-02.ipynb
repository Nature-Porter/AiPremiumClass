{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"2、实现分布式DDP模型训练，存盘后加载实现推理。","metadata":{}},{"cell_type":"code","source":"%%writefile ddp_ner.py\n\nimport os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nimport torch\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\n\n# 对ds中的数据进行过滤:过滤掉tokens为空的数据\ndef data_filter(item):\n    return len(item['tokens']) > 0\n\ndef data_input_proc_fn(tokenizer):\n    def data_input_proc(item):\n        input_data_list = []\n        # 对tokens进行分词,而不是将tokens合并成句子再分词,因为合并成句子再分词会导致input_ids的长度和ner_tags的长度不一致\n        # is_split_into_words=True已经分词不需要再分词，https://hf.cloudwisdom.top/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__\n        input_data = tokenizer(item['tokens'], \n                               truncation=True, \n                               add_special_tokens=False, \n                               max_length=512, \n                               is_split_into_words=True,\n                               padding='max_length')\n        # 对ner_tags的长度也进行截取和input_data长度一致\n        ner_tags = [n[:512] for n in item['ner_tags']]\n        # DataCollatorForTokenClassification中需要有labels这个标签\n        input_data['labels'] = ner_tags\n        return input_data\n    return data_input_proc\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group('nccl', rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n# 模型训练\ndef train(rank, world_size):\n    setup(rank, world_size)\n    \n    model_name = 'google-bert/bert-base-chinese'\n    EPOCHES = 1\n    # 数据预处理\n    ds = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n    ds['train'] = ds['train'].filter(data_filter)\n    ds['test'] = ds['test'].filter(data_filter)\n    tags = ds['train'].features['ner_tags'].feature.names\n    entites = ['O', 'PER', 'ORG', 'LOC']\n    entity_index = {e:i for i,e in enumerate(entites)}\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    max_length = tokenizer.model_max_length # 512\n    ds = ds.map(data_input_proc_fn(tokenizer), batched=True)\n    ds.set_format(type=\"torch\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n    \n    # 构建模型\n    id2label = {i:tag for i, tag in enumerate(tags)}\n    label2id = {tag:i for i, tag in enumerate(tags)}\n    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=7, id2label=id2label, label2id=label2id)\n    # model.to(rank)\n\n    # label_pad_token_id默认为-100\n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n\n    train_args = TrainingArguments(\n        output_dir='ner_train',\n        num_train_epochs=EPOCHES,\n        save_safetensors=True,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        report_to='tensorboard',\n        eval_strategy='epoch',\n        learning_rate=1e-4,\n        local_rank=rank, # 当前进程rank\n        fp16=True, # 使用混合精度\n        lr_scheduler_type='linear', # 动态学习率\n        warmup_steps=100, # 预热步数\n        ddp_find_unused_parameters=False # 优化DDP性能\n    )\n\n    def compute_metrics(result):\n        predicts,labels = result\n        # predicts.shape = (样本数量, padding后的sequence_length, num_labels)\n        # labels.shape = (样本数量, padding后的sequence_length)\n        # 获取评估对象\n        seqeval = evaluate.load('seqeval')\n        predicts = np.argmax(predicts, axis=2)\n        # 准备评估数据\n        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        labels = [[tags[l] for l in ls if l != -100]\n                     for ls in labels]\n        results = seqeval.compute(predictions=predicts, references=labels)\n        return results\n    \n    trainer = Trainer(\n        model, \n        train_args,\n        train_dataset=ds['train'],\n        eval_dataset=ds['test'],\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n    )\n    trainer.train()\n    cleanup()\n\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:10:26.356501Z","iopub.execute_input":"2025-06-14T15:10:26.356948Z","iopub.status.idle":"2025-06-14T15:10:26.364472Z","shell.execute_reply.started":"2025-06-14T15:10:26.356918Z","shell.execute_reply":"2025-06-14T15:10:26.363778Z"}},"outputs":[{"name":"stdout","text":"Overwriting ddp_ner.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!python ddp_ner.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T15:10:32.182352Z","iopub.execute_input":"2025-06-14T15:10:32.182792Z","iopub.status.idle":"2025-06-14T16:38:28.167416Z","shell.execute_reply.started":"2025-06-14T15:10:32.182769Z","shell.execute_reply":"2025-06-14T16:38:28.166619Z"}},"outputs":[{"name":"stdout","text":"2025-06-14 15:10:37.644228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749913837.668023    6759 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749913837.675568    6759 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 15:10:46.982746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749913847.005157    6773 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-06-14 15:10:47.010584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nE0000 00:00:1749913847.011954    6773 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749913847.032230    6774 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749913847.039980    6774 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[W614 15:10:50.697459672 socket.cpp:759] [c10d] The client socket has failed to connect to [localhost]:12355 (errno: 99 - Cannot assign requested address).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n  0%|                                                  | 0/1407 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.1123, 'grad_norm': 57148.234375, 'learning_rate': 6.947207345065035e-05, 'epoch': 0.36}\n{'loss': 0.1125, 'grad_norm': 68992.578125, 'learning_rate': 6.947207345065035e-05, 'epoch': 0.36}\n 36%|██████████████▏                         | 500/1407 [30:13<54:43,  3.62s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.0349, 'grad_norm': 56731.0078125, 'learning_rate': 3.121652639632747e-05, 'epoch': 0.71}\n{'loss': 0.0341, 'grad_norm': 80056.15625, 'learning_rate': 3.121652639632747e-05, 'epoch': 0.71}\n 71%|██████████████████████████▎          | 1000/1407 [1:00:35<24:48,  3.66s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n100%|█████████████████████████████████████| 1407/1407 [1:25:18<00:00,  2.89s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n\n  0%|                                                   | 0/108 [00:00<?, ?it/s]\u001b[A\n  0%|                                                   | 0/108 [00:00<?, ?it/s]\u001b[A\n  2%|▊                                          | 2/108 [00:01<01:00,  1.74it/s]\u001b[A\n  2%|▊                                          | 2/108 [00:01<01:01,  1.71it/s]\u001b[A\n  3%|█▏                                         | 3/108 [00:02<01:25,  1.23it/s]\u001b[A\n  3%|█▏                                         | 3/108 [00:02<01:29,  1.17it/s]\u001b[A\n  4%|█▌                                         | 4/108 [00:03<01:38,  1.06it/s]\u001b[A\n  4%|█▌                                         | 4/108 [00:03<01:41,  1.02it/s]\u001b[A\n  5%|█▉                                         | 5/108 [00:04<01:45,  1.02s/it]\u001b[A\n  5%|█▉                                         | 5/108 [00:04<01:48,  1.05s/it]\u001b[A\n  6%|██▍                                        | 6/108 [00:05<01:49,  1.07s/it]\u001b[A\n  6%|██▍                                        | 6/108 [00:05<01:52,  1.10s/it]\u001b[A\n  6%|██▊                                        | 7/108 [00:06<01:52,  1.11s/it]\u001b[A\n  6%|██▊                                        | 7/108 [00:07<01:53,  1.13s/it]\u001b[A\n  7%|███▏                                       | 8/108 [00:08<01:51,  1.12s/it]\u001b[A\n  7%|███▏                                       | 8/108 [00:08<01:57,  1.18s/it]\u001b[A\n  8%|███▌                                       | 9/108 [00:09<01:52,  1.14s/it]\u001b[A\n  8%|███▌                                       | 9/108 [00:09<01:56,  1.18s/it]\u001b[A\n  9%|███▉                                      | 10/108 [00:10<01:52,  1.15s/it]\u001b[A\n  9%|███▉                                      | 10/108 [00:10<01:55,  1.18s/it]\u001b[A\n 10%|████▎                                     | 11/108 [00:11<01:52,  1.16s/it]\u001b[A\n 10%|████▎                                     | 11/108 [00:12<01:55,  1.19s/it]\u001b[A\n 11%|████▋                                     | 12/108 [00:12<01:51,  1.17s/it]\u001b[A\n 11%|████▋                                     | 12/108 [00:12<01:47,  1.12s/it]\u001b[A\n 12%|█████                                     | 13/108 [00:13<01:50,  1.16s/it]\u001b[A\n 12%|█████                                     | 13/108 [00:14<01:54,  1.20s/it]\u001b[A\n 13%|█████▍                                    | 14/108 [00:15<01:50,  1.17s/it]\u001b[A\n 13%|█████▍                                    | 14/108 [00:15<01:55,  1.23s/it]\u001b[A\n 14%|█████▊                                    | 15/108 [00:16<01:49,  1.18s/it]\u001b[A\n 14%|█████▊                                    | 15/108 [00:16<01:52,  1.21s/it]\u001b[A\n 15%|██████▏                                   | 16/108 [00:17<01:48,  1.18s/it]\u001b[A\n 15%|██████▏                                   | 16/108 [00:18<01:50,  1.21s/it]\u001b[A\n 16%|██████▌                                   | 17/108 [00:18<01:47,  1.18s/it]\u001b[A\n 16%|██████▌                                   | 17/108 [00:19<01:49,  1.20s/it]\u001b[A\n 17%|███████                                   | 18/108 [00:19<01:45,  1.18s/it]\u001b[A\n 17%|███████                                   | 18/108 [00:20<01:46,  1.19s/it]\u001b[A\n 18%|███████▍                                  | 19/108 [00:21<01:45,  1.18s/it]\u001b[A\n 18%|███████▍                                  | 19/108 [00:21<01:45,  1.19s/it]\u001b[A\n 19%|███████▊                                  | 20/108 [00:22<01:43,  1.18s/it]\u001b[A\n 19%|███████▊                                  | 20/108 [00:22<01:44,  1.19s/it]\u001b[A\n 19%|████████▏                                 | 21/108 [00:23<01:42,  1.18s/it]\u001b[A\n 19%|████████▏                                 | 21/108 [00:23<01:43,  1.19s/it]\u001b[A\n 20%|████████▌                                 | 22/108 [00:24<01:41,  1.18s/it]\u001b[A\n 20%|████████▌                                 | 22/108 [00:25<01:41,  1.18s/it]\u001b[A\n 21%|████████▉                                 | 23/108 [00:25<01:40,  1.18s/it]\u001b[A\n 21%|████████▉                                 | 23/108 [00:26<01:40,  1.18s/it]\u001b[A\n 22%|█████████▎                                | 24/108 [00:26<01:39,  1.18s/it]\u001b[A\n 22%|█████████▎                                | 24/108 [00:27<01:39,  1.18s/it]\u001b[A\n 23%|█████████▋                                | 25/108 [00:28<01:37,  1.18s/it]\u001b[A\n 23%|█████████▋                                | 25/108 [00:28<01:37,  1.18s/it]\u001b[A\n 24%|██████████                                | 26/108 [00:29<01:36,  1.18s/it]\u001b[A\n 24%|██████████                                | 26/108 [00:29<01:36,  1.18s/it]\u001b[A\n 25%|██████████▌                               | 27/108 [00:30<01:35,  1.18s/it]\u001b[A\n 25%|██████████▌                               | 27/108 [00:31<01:35,  1.18s/it]\u001b[A\n 26%|██████████▉                               | 28/108 [00:31<01:34,  1.18s/it]\u001b[A\n 26%|██████████▉                               | 28/108 [00:32<01:34,  1.18s/it]\u001b[A\n 27%|███████████▎                              | 29/108 [00:32<01:33,  1.18s/it]\u001b[A\n 27%|███████████▎                              | 29/108 [00:33<01:33,  1.18s/it]\u001b[A\n 28%|███████████▋                              | 30/108 [00:34<01:32,  1.18s/it]\u001b[A\n 28%|███████████▋                              | 30/108 [00:34<01:32,  1.18s/it]\u001b[A\n 29%|████████████                              | 31/108 [00:35<01:30,  1.18s/it]\u001b[A\n 29%|████████████                              | 31/108 [00:35<01:30,  1.18s/it]\u001b[A\n 30%|████████████▍                             | 32/108 [00:36<01:30,  1.19s/it]\u001b[A\n 30%|████████████▍                             | 32/108 [00:36<01:29,  1.18s/it]\u001b[A\n 31%|████████████▊                             | 33/108 [00:37<01:28,  1.18s/it]\u001b[A\n 31%|████████████▊                             | 33/108 [00:38<01:28,  1.18s/it]\u001b[A\n 31%|█████████████▏                            | 34/108 [00:38<01:27,  1.18s/it]\u001b[A\n 31%|█████████████▏                            | 34/108 [00:39<01:27,  1.18s/it]\u001b[A\n 32%|█████████████▌                            | 35/108 [00:39<01:26,  1.18s/it]\u001b[A\n 32%|█████████████▌                            | 35/108 [00:40<01:26,  1.18s/it]\u001b[A\n 33%|██████████████                            | 36/108 [00:41<01:24,  1.18s/it]\u001b[A\n 33%|██████████████                            | 36/108 [00:41<01:24,  1.18s/it]\u001b[A\n 34%|██████████████▍                           | 37/108 [00:42<01:23,  1.18s/it]\u001b[A\n 34%|██████████████▍                           | 37/108 [00:42<01:23,  1.18s/it]\u001b[A\n 35%|██████████████▊                           | 38/108 [00:43<01:22,  1.17s/it]\u001b[A\n 35%|██████████████▊                           | 38/108 [00:43<01:22,  1.18s/it]\u001b[A\n 36%|███████████████▏                          | 39/108 [00:44<01:20,  1.17s/it]\u001b[A\n 36%|███████████████▏                          | 39/108 [00:45<01:21,  1.17s/it]\u001b[A\n 37%|███████████████▌                          | 40/108 [00:45<01:19,  1.17s/it]\u001b[A\n 37%|███████████████▌                          | 40/108 [00:46<01:19,  1.17s/it]\u001b[A\n 38%|███████████████▉                          | 41/108 [00:47<01:18,  1.17s/it]\u001b[A\n 38%|███████████████▉                          | 41/108 [00:47<01:18,  1.17s/it]\u001b[A\n 39%|████████████████▎                         | 42/108 [00:48<01:17,  1.17s/it]\u001b[A\n 39%|████████████████▎                         | 42/108 [00:48<01:17,  1.17s/it]\u001b[A\n 40%|████████████████▋                         | 43/108 [00:49<01:16,  1.17s/it]\u001b[A\n 40%|████████████████▋                         | 43/108 [00:49<01:15,  1.17s/it]\u001b[A\n 41%|█████████████████                         | 44/108 [00:50<01:15,  1.17s/it]\u001b[A\n 41%|█████████████████                         | 44/108 [00:51<01:14,  1.17s/it]\u001b[A\n 42%|█████████████████▌                        | 45/108 [00:51<01:13,  1.17s/it]\u001b[A\n 42%|█████████████████▌                        | 45/108 [00:52<01:13,  1.17s/it]\u001b[A\n 43%|█████████████████▉                        | 46/108 [00:52<01:12,  1.17s/it]\u001b[A\n 43%|█████████████████▉                        | 46/108 [00:53<01:12,  1.17s/it]\u001b[A\n 44%|██████████████████▎                       | 47/108 [00:54<01:11,  1.17s/it]\u001b[A\n 44%|██████████████████▎                       | 47/108 [00:54<01:11,  1.17s/it]\u001b[A\n 44%|██████████████████▋                       | 48/108 [00:55<01:10,  1.17s/it]\u001b[A\n 44%|██████████████████▋                       | 48/108 [00:55<01:10,  1.17s/it]\u001b[A\n 45%|███████████████████                       | 49/108 [00:56<01:08,  1.17s/it]\u001b[A\n 45%|███████████████████                       | 49/108 [00:56<01:08,  1.17s/it]\u001b[A\n 46%|███████████████████▍                      | 50/108 [00:57<01:08,  1.17s/it]\u001b[A\n 46%|███████████████████▍                      | 50/108 [00:58<01:07,  1.17s/it]\u001b[A\n 47%|███████████████████▊                      | 51/108 [00:58<01:06,  1.17s/it]\u001b[A\n 47%|███████████████████▊                      | 51/108 [00:59<01:06,  1.17s/it]\u001b[A\n 48%|████████████████████▏                     | 52/108 [00:59<01:05,  1.17s/it]\u001b[A\n 48%|████████████████████▏                     | 52/108 [01:00<01:05,  1.17s/it]\u001b[A\n 49%|████████████████████▌                     | 53/108 [01:01<01:04,  1.17s/it]\u001b[A\n 49%|████████████████████▌                     | 53/108 [01:01<01:04,  1.17s/it]\u001b[A\n 50%|█████████████████████                     | 54/108 [01:02<01:02,  1.16s/it]\u001b[A\n 50%|█████████████████████                     | 54/108 [01:02<01:03,  1.17s/it]\u001b[A\n 51%|█████████████████████▍                    | 55/108 [01:03<01:01,  1.17s/it]\u001b[A\n 51%|█████████████████████▍                    | 55/108 [01:03<01:01,  1.16s/it]\u001b[A\n 52%|█████████████████████▊                    | 56/108 [01:04<01:00,  1.17s/it]\u001b[A\n 52%|█████████████████████▊                    | 56/108 [01:05<01:00,  1.16s/it]\u001b[A\n 53%|██████████████████████▏                   | 57/108 [01:05<00:59,  1.16s/it]\u001b[A\n 53%|██████████████████████▏                   | 57/108 [01:06<00:59,  1.17s/it]\u001b[A\n 54%|██████████████████████▌                   | 58/108 [01:06<00:58,  1.16s/it]\u001b[A\n 54%|██████████████████████▌                   | 58/108 [01:07<00:58,  1.16s/it]\u001b[A\n 55%|██████████████████████▉                   | 59/108 [01:08<00:57,  1.16s/it]\u001b[A\n 55%|██████████████████████▉                   | 59/108 [01:08<00:56,  1.16s/it]\u001b[A\n 56%|███████████████████████▎                  | 60/108 [01:09<00:55,  1.16s/it]\u001b[A\n 56%|███████████████████████▎                  | 60/108 [01:09<00:55,  1.16s/it]\u001b[A\n 56%|███████████████████████▋                  | 61/108 [01:10<00:54,  1.16s/it]\u001b[A\n 56%|███████████████████████▋                  | 61/108 [01:10<00:54,  1.16s/it]\u001b[A\n 57%|████████████████████████                  | 62/108 [01:11<00:53,  1.16s/it]\u001b[A\n 57%|████████████████████████                  | 62/108 [01:11<00:53,  1.16s/it]\u001b[A\n 58%|████████████████████████▌                 | 63/108 [01:12<00:52,  1.16s/it]\u001b[A\n 58%|████████████████████████▌                 | 63/108 [01:13<00:52,  1.16s/it]\u001b[A\n 59%|████████████████████████▉                 | 64/108 [01:13<00:51,  1.17s/it]\u001b[A\n 59%|████████████████████████▉                 | 64/108 [01:14<00:51,  1.16s/it]\u001b[A\n 60%|█████████████████████████▎                | 65/108 [01:15<00:50,  1.17s/it]\u001b[A\n 60%|█████████████████████████▎                | 65/108 [01:15<00:50,  1.17s/it]\u001b[A\n 61%|█████████████████████████▋                | 66/108 [01:16<00:49,  1.17s/it]\u001b[A\n 61%|█████████████████████████▋                | 66/108 [01:16<00:49,  1.17s/it]\u001b[A\n 62%|██████████████████████████                | 67/108 [01:17<00:47,  1.17s/it]\u001b[A\n 62%|██████████████████████████                | 67/108 [01:17<00:47,  1.17s/it]\u001b[A\n 63%|██████████████████████████▍               | 68/108 [01:18<00:46,  1.17s/it]\u001b[A\n 63%|██████████████████████████▍               | 68/108 [01:19<00:46,  1.17s/it]\u001b[A\n 64%|██████████████████████████▊               | 69/108 [01:19<00:45,  1.17s/it]\u001b[A\n 64%|██████████████████████████▊               | 69/108 [01:20<00:45,  1.17s/it]\u001b[A\n 65%|███████████████████████████▏              | 70/108 [01:20<00:44,  1.17s/it]\u001b[A\n 65%|███████████████████████████▏              | 70/108 [01:21<00:44,  1.17s/it]\u001b[A\n 66%|███████████████████████████▌              | 71/108 [01:22<00:43,  1.17s/it]\u001b[A\n 66%|███████████████████████████▌              | 71/108 [01:22<00:43,  1.17s/it]\u001b[A\n 67%|████████████████████████████              | 72/108 [01:23<00:42,  1.17s/it]\u001b[A\n 67%|████████████████████████████              | 72/108 [01:23<00:42,  1.17s/it]\u001b[A\n 68%|████████████████████████████▍             | 73/108 [01:24<00:41,  1.17s/it]\u001b[A\n 68%|████████████████████████████▍             | 73/108 [01:24<00:40,  1.17s/it]\u001b[A\n 69%|████████████████████████████▊             | 74/108 [01:25<00:40,  1.18s/it]\u001b[A\n 69%|████████████████████████████▊             | 74/108 [01:26<00:39,  1.18s/it]\u001b[A\n 69%|█████████████████████████████▏            | 75/108 [01:26<00:38,  1.18s/it]\u001b[A\n 69%|█████████████████████████████▏            | 75/108 [01:27<00:38,  1.18s/it]\u001b[A\n 70%|█████████████████████████████▌            | 76/108 [01:27<00:37,  1.18s/it]\u001b[A\n 70%|█████████████████████████████▌            | 76/108 [01:28<00:37,  1.18s/it]\u001b[A\n 71%|█████████████████████████████▉            | 77/108 [01:29<00:36,  1.18s/it]\u001b[A\n 71%|█████████████████████████████▉            | 77/108 [01:29<00:36,  1.18s/it]\u001b[A\n 72%|██████████████████████████████▎           | 78/108 [01:30<00:35,  1.18s/it]\u001b[A\n 72%|██████████████████████████████▎           | 78/108 [01:30<00:35,  1.18s/it]\u001b[A\n 73%|██████████████████████████████▋           | 79/108 [01:31<00:34,  1.18s/it]\u001b[A\n 73%|██████████████████████████████▋           | 79/108 [01:31<00:34,  1.18s/it]\u001b[A\n 74%|███████████████████████████████           | 80/108 [01:32<00:33,  1.18s/it]\u001b[A\n 74%|███████████████████████████████           | 80/108 [01:33<00:33,  1.18s/it]\u001b[A\n 75%|███████████████████████████████▌          | 81/108 [01:33<00:31,  1.18s/it]\u001b[A\n 75%|███████████████████████████████▌          | 81/108 [01:34<00:31,  1.18s/it]\u001b[A\n 76%|███████████████████████████████▉          | 82/108 [01:35<00:30,  1.18s/it]\u001b[A\n 76%|███████████████████████████████▉          | 82/108 [01:35<00:30,  1.17s/it]\u001b[A\n 77%|████████████████████████████████▎         | 83/108 [01:36<00:29,  1.18s/it]\u001b[A\n 77%|████████████████████████████████▎         | 83/108 [01:36<00:29,  1.17s/it]\u001b[A\n 78%|████████████████████████████████▋         | 84/108 [01:37<00:28,  1.18s/it]\u001b[A\n 78%|████████████████████████████████▋         | 84/108 [01:37<00:28,  1.18s/it]\u001b[A\n 79%|█████████████████████████████████         | 85/108 [01:38<00:27,  1.18s/it]\u001b[A\n 79%|█████████████████████████████████         | 85/108 [01:38<00:26,  1.17s/it]\u001b[A\n 80%|█████████████████████████████████▍        | 86/108 [01:39<00:25,  1.18s/it]\u001b[A\n 80%|█████████████████████████████████▍        | 86/108 [01:40<00:25,  1.18s/it]\u001b[A\n 81%|█████████████████████████████████▊        | 87/108 [01:40<00:24,  1.18s/it]\u001b[A\n 81%|█████████████████████████████████▊        | 87/108 [01:41<00:24,  1.17s/it]\u001b[A\n 81%|██████████████████████████████████▏       | 88/108 [01:42<00:23,  1.18s/it]\u001b[A\n 81%|██████████████████████████████████▏       | 88/108 [01:42<00:23,  1.17s/it]\u001b[A\n 82%|██████████████████████████████████▌       | 89/108 [01:43<00:22,  1.18s/it]\u001b[A\n 82%|██████████████████████████████████▌       | 89/108 [01:43<00:22,  1.17s/it]\u001b[A\n 83%|███████████████████████████████████       | 90/108 [01:44<00:21,  1.18s/it]\u001b[A\n 83%|███████████████████████████████████       | 90/108 [01:44<00:21,  1.18s/it]\u001b[A\n 84%|███████████████████████████████████▍      | 91/108 [01:45<00:20,  1.18s/it]\u001b[A\n 84%|███████████████████████████████████▍      | 91/108 [01:46<00:20,  1.18s/it]\u001b[A\n 85%|███████████████████████████████████▊      | 92/108 [01:46<00:18,  1.18s/it]\u001b[A\n 85%|███████████████████████████████████▊      | 92/108 [01:47<00:18,  1.18s/it]\u001b[A\n 86%|████████████████████████████████████▏     | 93/108 [01:48<00:17,  1.18s/it]\u001b[A\n 86%|████████████████████████████████████▏     | 93/108 [01:48<00:17,  1.18s/it]\u001b[A\n 87%|████████████████████████████████████▌     | 94/108 [01:49<00:16,  1.18s/it]\u001b[A\n 87%|████████████████████████████████████▌     | 94/108 [01:49<00:16,  1.18s/it]\u001b[A\n 88%|████████████████████████████████████▉     | 95/108 [01:50<00:15,  1.18s/it]\u001b[A\n 88%|████████████████████████████████████▉     | 95/108 [01:50<00:15,  1.18s/it]\u001b[A\n 89%|█████████████████████████████████████▎    | 96/108 [01:51<00:14,  1.18s/it]\u001b[A\n 89%|█████████████████████████████████████▎    | 96/108 [01:51<00:14,  1.19s/it]\u001b[A\n 90%|█████████████████████████████████████▋    | 97/108 [01:52<00:12,  1.18s/it]\u001b[A\n 90%|█████████████████████████████████████▋    | 97/108 [01:53<00:13,  1.19s/it]\u001b[A\n 91%|██████████████████████████████████████    | 98/108 [01:53<00:11,  1.18s/it]\u001b[A\n 91%|██████████████████████████████████████    | 98/108 [01:54<00:11,  1.19s/it]\u001b[A\n 92%|██████████████████████████████████████▌   | 99/108 [01:55<00:10,  1.18s/it]\u001b[A\n 92%|██████████████████████████████████████▌   | 99/108 [01:55<00:10,  1.19s/it]\u001b[A\n 93%|█████████████████████████████████████▉   | 100/108 [01:56<00:09,  1.18s/it]\u001b[A\n 93%|█████████████████████████████████████▉   | 100/108 [01:56<00:09,  1.19s/it]\u001b[A\n 94%|██████████████████████████████████████▎  | 101/108 [01:57<00:08,  1.18s/it]\u001b[A\n 94%|██████████████████████████████████████▎  | 101/108 [01:57<00:08,  1.19s/it]\u001b[A\n 94%|██████████████████████████████████████▋  | 102/108 [01:58<00:07,  1.18s/it]\u001b[A\n 94%|██████████████████████████████████████▋  | 102/108 [01:59<00:07,  1.19s/it]\u001b[A\n 95%|███████████████████████████████████████  | 103/108 [01:59<00:05,  1.18s/it]\u001b[A\n 95%|███████████████████████████████████████  | 103/108 [02:00<00:05,  1.19s/it]\u001b[A\n 96%|███████████████████████████████████████▍ | 104/108 [02:01<00:04,  1.18s/it]\u001b[A\n 96%|███████████████████████████████████████▍ | 104/108 [02:01<00:04,  1.19s/it]\u001b[A\n 97%|███████████████████████████████████████▊ | 105/108 [02:02<00:03,  1.18s/it]\u001b[A\n 97%|███████████████████████████████████████▊ | 105/108 [02:02<00:03,  1.18s/it]\u001b[A\n 98%|████████████████████████████████████████▏| 106/108 [02:03<00:02,  1.19s/it]\u001b[A\n 98%|████████████████████████████████████████▏| 106/108 [02:03<00:02,  1.19s/it]\u001b[A\n 99%|████████████████████████████████████████▌| 107/108 [02:04<00:01,  1.19s/it]\u001b[A\n 99%|████████████████████████████████████████▌| 107/108 [02:05<00:01,  1.19s/it]\u001b[A\n100%|█████████████████████████████████████████| 108/108 [02:05<00:00,  1.04s/it]\u001b[A\n100%|█████████████████████████████████████████| 108/108 [02:05<00:00,  1.06it/s]\u001b[ATrainer is attempting to log a value of \"{'precision': 0.9481902058197303, 'recall': 0.9368863955119214, 'f1': 0.9425044091710758, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8677867056245434, 'recall': 0.9, 'f1': 0.8835998512458163, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9448818897637795, 'recall': 0.9580838323353293, 'f1': 0.9514370664023785, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.025961654260754585, 'eval_LOC': {'precision': 0.9481902058197303, 'recall': 0.9368863955119214, 'f1': 0.9425044091710758, 'number': 2852}, 'eval_ORG': {'precision': 0.8677867056245434, 'recall': 0.9, 'f1': 0.8835998512458163, 'number': 1320}, 'eval_PER': {'precision': 0.9448818897637795, 'recall': 0.9580838323353293, 'f1': 0.9514370664023785, 'number': 1503}, 'eval_overall_precision': 0.9280336193311154, 'eval_overall_recall': 0.933920704845815, 'eval_overall_f1': 0.9309678552608467, 'eval_overall_accuracy': 0.9926113003641489, 'eval_runtime': 128.9099, 'eval_samples_per_second': 26.701, 'eval_steps_per_second': 0.838, 'epoch': 1.0}\n100%|█████████████████████████████████████| 1407/1407 [1:27:29<00:00,  2.89s/it]\n100%|█████████████████████████████████████████| 108/108 [02:07<00:00,  1.04s/it]\u001b[A\n{'train_runtime': 5249.9069, 'train_samples_per_second': 8.572, 'train_steps_per_second': 0.268, 'train_loss': 0.05940031513900052, 'epoch': 1.0}\n100%|█████████████████████████████████████| 1407/1407 [1:27:29<00:00,  3.73s/it]\nTrainer is attempting to log a value of \"{'precision': 0.9516584333098095, 'recall': 0.9456521739130435, 'f1': 0.9486457966936335, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8788332086761406, 'recall': 0.8901515151515151, 'f1': 0.8844561535566428, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9542743538767395, 'recall': 0.9580838323353293, 'f1': 0.9561752988047809, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.025363432243466377, 'eval_LOC': {'precision': 0.9516584333098095, 'recall': 0.9456521739130435, 'f1': 0.9486457966936335, 'number': 2852}, 'eval_ORG': {'precision': 0.8788332086761406, 'recall': 0.8901515151515151, 'f1': 0.8844561535566428, 'number': 1320}, 'eval_PER': {'precision': 0.9542743538767395, 'recall': 0.9580838323353293, 'f1': 0.9561752988047809, 'number': 1503}, 'eval_overall_precision': 0.9352112676056338, 'eval_overall_recall': 0.9360352422907489, 'eval_overall_f1': 0.9356230735358871, 'eval_overall_accuracy': 0.9927992482086221, 'eval_runtime': 130.162, 'eval_samples_per_second': 26.444, 'eval_steps_per_second': 0.83, 'epoch': 1.0}\n100%|█████████████████████████████████████| 1407/1407 [1:27:31<00:00,  2.89s/it]\n100%|█████████████████████████████████████████| 108/108 [02:08<00:00,  1.06it/s]\u001b[A\n{'train_runtime': 5251.2596, 'train_samples_per_second': 8.569, 'train_steps_per_second': 0.268, 'train_loss': 0.0590813846222119, 'epoch': 1.0}\n100%|█████████████████████████████████████| 1407/1407 [1:27:31<00:00,  3.73s/it]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from transformers import pipeline\n# 进行命名实体识别\nner = pipeline('token-classification', '/kaggle/working/ner_train/checkpoint-1407')\nseq = '双方确定了今后发展中美关系的指导方针。'\nner_result = ner(seq)\nprint(ner_result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T16:39:09.424931Z","iopub.execute_input":"2025-06-14T16:39:09.425527Z","iopub.status.idle":"2025-06-14T16:39:18.965209Z","shell.execute_reply.started":"2025-06-14T16:39:09.425492Z","shell.execute_reply":"2025-06-14T16:39:18.964571Z"}},"outputs":[{"name":"stderr","text":"2025-06-14 16:39:12.934549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749919152.957840      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749919152.964877      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"[{'entity': 'B-LOC', 'score': 0.9991559, 'index': 10, 'word': '中', 'start': 9, 'end': 10}, {'entity': 'B-LOC', 'score': 0.9991559, 'index': 11, 'word': '美', 'start': 10, 'end': 11}]\n","output_type":"stream"}],"execution_count":15}]}