{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T08:15:09.077760Z",
     "iopub.status.busy": "2025-05-25T08:15:09.077223Z",
     "iopub.status.idle": "2025-05-25T08:15:09.085407Z",
     "shell.execute_reply": "2025-05-25T08:15:09.084790Z",
     "shell.execute_reply.started": "2025-05-25T08:15:09.077732Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:17.376952Z",
     "iopub.status.busy": "2025-05-25T07:46:17.376496Z",
     "iopub.status.idle": "2025-05-25T07:46:19.805470Z",
     "shell.execute_reply": "2025-05-25T07:46:19.804886Z",
     "shell.execute_reply.started": "2025-05-25T07:46:17.376933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f20845792e4dc5a66dad1d48053ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573b74f1bf59479e9eca3c6e1b42bf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:19.806441Z",
     "iopub.status.busy": "2025-05-25T07:46:19.806127Z",
     "iopub.status.idle": "2025-05-25T07:46:19.811801Z",
     "shell.execute_reply": "2025-05-25T07:46:19.811296Z",
     "shell.execute_reply.started": "2025-05-25T07:46:19.806422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:19.813322Z",
     "iopub.status.busy": "2025-05-25T07:46:19.813071Z",
     "iopub.status.idle": "2025-05-25T07:46:20.655115Z",
     "shell.execute_reply": "2025-05-25T07:46:20.654409Z",
     "shell.execute_reply.started": "2025-05-25T07:46:19.813306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9936bb35ee4b38b09ad0d3b3e27081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2652a5af574d05913c8e8ad74e055d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762e38fc87654244856be618e99d21d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:20.656579Z",
     "iopub.status.busy": "2025-05-25T07:46:20.655872Z",
     "iopub.status.idle": "2025-05-25T07:46:20.806211Z",
     "shell.execute_reply": "2025-05-25T07:46:20.805413Z",
     "shell.execute_reply.started": "2025-05-25T07:46:20.656559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[[ 0.3466,  0.0680,  0.1839,  0.3548,  0.1633,  0.2896,  0.4516],\n",
      "         [ 0.1477, -0.2928,  0.4195, -0.0412, -0.1842,  0.3515,  0.5917],\n",
      "         [-0.0289,  0.1072,  0.5445,  0.1000,  0.0958,  0.8696,  0.1344],\n",
      "         [-0.1187, -0.1498,  0.2413, -0.3251, -0.0966,  0.6368, -0.3206],\n",
      "         [-0.1577, -0.0087,  0.7181, -0.0674,  0.0150,  0.5987, -0.4756],\n",
      "         [-0.2419, -0.3130, -0.3713, -0.5937,  0.0140,  0.5136,  0.1689],\n",
      "         [-0.0920, -0.1692,  0.4886, -0.0962,  0.2686,  0.3336, -0.0947],\n",
      "         [ 0.5450,  0.1952,  0.2713, -0.2293,  0.3201,  0.4604,  0.3622]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "message= \"命名实体识别\"\n",
    "label = torch.tensor([0,1,0,2,5,4])\n",
    "\n",
    "model_input = tokenizer([message], return_tensors='pt')\n",
    "result = model(**model_input)\n",
    "\n",
    "print(result.loss)\n",
    "print(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:20.807311Z",
     "iopub.status.busy": "2025-05-25T07:46:20.807053Z",
     "iopub.status.idle": "2025-05-25T07:46:22.326490Z",
     "shell.execute_reply": "2025-05-25T07:46:22.325892Z",
     "shell.execute_reply.started": "2025-05-25T07:46:20.807293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0990978bdd104d78b0e926977ae5903f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3def6a1ccd7b4d8283f3542fa303d956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b595406b734b698977797f6f48cec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-a33d0e4276aef9b4.parquet:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00db90689565495282231e8420240a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-07f476b71c5edde6.parquet:   0%|          | 0.00/178k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286b0fdd3898418e8ac9e80a6637e0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9aca8dd978446f99b029106f64ba58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载hf中dataset\n",
    "ds = load_dataset('nlhappy/CLUE-NER')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实体映射数据集词典准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:22.327501Z",
     "iopub.status.busy": "2025-05-25T07:46:22.327196Z",
     "iopub.status.idle": "2025-05-25T07:46:22.332054Z",
     "shell.execute_reply": "2025-05-25T07:46:22.331469Z",
     "shell.execute_reply.started": "2025-05-25T07:46:22.327483Z"
    }
   },
   "outputs": [],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "           'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:22.333611Z",
     "iopub.status.busy": "2025-05-25T07:46:22.332733Z",
     "iopub.status.idle": "2025-05-25T07:46:22.430372Z",
     "shell.execute_reply": "2025-05-25T07:46:22.429703Z",
     "shell.execute_reply.started": "2025-05-25T07:46:22.333582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'government': 1,\n",
       " 'position': 2,\n",
       " 'company': 3,\n",
       " 'movie': 4,\n",
       " 'address': 5,\n",
       " 'book': 6,\n",
       " 'scene': 7,\n",
       " 'organization': 8,\n",
       " 'game': 9,\n",
       " 'name': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:22.431504Z",
     "iopub.status.busy": "2025-05-25T07:46:22.431077Z",
     "iopub.status.idle": "2025-05-25T07:46:22.442581Z",
     "shell.execute_reply": "2025-05-25T07:46:22.442023Z",
     "shell.execute_reply.started": "2025-05-25T07:46:22.431481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'B-POSITION',\n",
       " 'I-POSITION',\n",
       " 'B-COMPANY',\n",
       " 'I-COMPANY',\n",
       " 'B-MOVIE',\n",
       " 'I-MOVIE',\n",
       " 'B-ADDRESS',\n",
       " 'I-ADDRESS',\n",
       " 'B-BOOK',\n",
       " 'I-BOOK',\n",
       " 'B-SCENE',\n",
       " 'I-SCENE',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-GAME',\n",
       " 'I-GAME',\n",
       " 'B-NAME',\n",
       " 'I-NAME']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:22.444968Z",
     "iopub.status.busy": "2025-05-25T07:46:22.444461Z",
     "iopub.status.idle": "2025-05-25T07:46:23.889323Z",
     "shell.execute_reply": "2025-05-25T07:46:23.888630Z",
     "shell.execute_reply.started": "2025-05-25T07:46:22.444952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebf79e311504f88851b0af8b0934724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac0289c818f4457b72b15f697984330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def entity_tags_proc(item):\n",
    "    # item即是dataset中记录\n",
    "    text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "    tags = [0] * text_len    # 初始值为‘O’\n",
    "    # 遍历实体列表，所有实体类别标记填入tags\n",
    "    entites = item['ents']\n",
    "    for ent in entites:\n",
    "        indices = ent['indices']  # 实体索引\n",
    "        label = ent['label']   # 实体名\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "    return {'ent_tag': tags}\n",
    "\n",
    "# 使用自定义回调函数处理数据集记录\n",
    "ds1 = ds.map(entity_tags_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:23.890297Z",
     "iopub.status.busy": "2025-05-25T07:46:23.890051Z",
     "iopub.status.idle": "2025-05-25T07:46:23.894974Z",
     "shell.execute_reply": "2025-05-25T07:46:23.894149Z",
     "shell.execute_reply.started": "2025-05-25T07:46:23.890270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，\n",
      "[5, 6, 6, 6, 0, 0, 0, 0, 0, 19, 20, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "for row in ds1['train']:\n",
    "    print(row['text'])\n",
    "    print(row['ent_tag'])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:23.895990Z",
     "iopub.status.busy": "2025-05-25T07:46:23.895736Z",
     "iopub.status.idle": "2025-05-25T07:46:23.916991Z",
     "shell.execute_reply": "2025-05-25T07:46:23.916418Z",
     "shell.execute_reply.started": "2025-05-25T07:46:23.895968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文bert分词在日期时间和英文转换token过程中，出现合并。影响ner标注准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:23.918017Z",
     "iopub.status.busy": "2025-05-25T07:46:23.917739Z",
     "iopub.status.idle": "2025-05-25T07:46:23.927151Z",
     "shell.execute_reply": "2025-05-25T07:46:23.926488Z",
     "shell.execute_reply.started": "2025-05-25T07:46:23.918001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8202, 2399, 123, 3299, 10253]\n",
      "2000 年 2 月 add\n"
     ]
    }
   ],
   "source": [
    "token_index = tokenizer.encode('2000年2月add', add_special_tokens=False)\n",
    "print(token_index)\n",
    "tokens = tokenizer.decode(token_index)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:23.927998Z",
     "iopub.status.busy": "2025-05-25T07:46:23.927810Z",
     "iopub.status.idle": "2025-05-25T07:46:23.939401Z",
     "shell.execute_reply": "2025-05-25T07:46:23.938642Z",
     "shell.execute_reply.started": "2025-05-25T07:46:23.927984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8202, 2399, 123, 3299, 10253]], 'token_type_ids': [[0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tokenizer(['2000年2月add'], add_special_tokens=False, truncation=True)\n",
    "print(input_data)\n",
    "\n",
    "input_data.word_ids(0) # 返回批次中指定token对应原始文本的索引映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:23.940565Z",
     "iopub.status.busy": "2025-05-25T07:46:23.940347Z",
     "iopub.status.idle": "2025-05-25T07:46:25.374301Z",
     "shell.execute_reply": "2025-05-25T07:46:25.373604Z",
     "shell.execute_reply.started": "2025-05-25T07:46:23.940543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab9958349734858959b0e18f313cc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ab8e36907c4e6d94cb6ea41abb65a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始文本转换模型需要token_idx,生成和token_idx对齐label\n",
    "def data_input_proc(item):\n",
    "    # 输入文本转换模型输入token索引\n",
    "    input_data = tokenizer(item['text'], truncation=True, add_special_tokens=False, max_length=512)\n",
    "    adjust_labels = []  # 所有修正后label索引列表\n",
    "    # 上一步骤生成ner_tag中索引和token对齐\n",
    "    for k in range(len(input_data['input_ids'])):\n",
    "        # 每条记录token对应word_ids\n",
    "        word_ids = input_data.word_ids(k)\n",
    "        # 批次ner_tag长度和token长度对齐\n",
    "        tags = item['ent_tag'][k]\n",
    "        \n",
    "        adjusted_label_ids = []\n",
    "        i, prev_wid = -1,-1\n",
    "        for wid in word_ids:\n",
    "            if (wid != prev_wid):   #  word_ids [1,1,1,2,3,4,5] -> [0,1,2,3,4,5,6]\n",
    "                i += 1 # token对应检索位置+1\n",
    "                prev_wid = wid\n",
    "            adjusted_label_ids.append(tags[i])\n",
    "        adjust_labels.append(adjusted_label_ids)                \n",
    "    # 修正后label添加到input_data\n",
    "    input_data['labels'] = adjust_labels\n",
    "    return input_data\n",
    "    \n",
    "\n",
    "ds2 = ds1.map(data_input_proc, batched=True)  # batched 每次传入自定义方法样本数量多个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:25.375612Z",
     "iopub.status.busy": "2025-05-25T07:46:25.375059Z",
     "iopub.status.idle": "2025-05-25T07:46:25.379571Z",
     "shell.execute_reply": "2025-05-25T07:46:25.378869Z",
     "shell.execute_reply.started": "2025-05-25T07:46:25.375585Z"
    }
   },
   "outputs": [],
   "source": [
    "# 记录转换为pytorch\n",
    "ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "# ds_new = ds2.with_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:25.380432Z",
     "iopub.status.busy": "2025-05-25T07:46:25.380235Z",
     "iopub.status.idle": "2025-05-25T07:46:25.424821Z",
     "shell.execute_reply": "2025-05-25T07:46:25.424082Z",
     "shell.execute_reply.started": "2025-05-25T07:46:25.380418Z"
    }
   },
   "outputs": [],
   "source": [
    "# for item in ds2['train']:\n",
    "#     print(item)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "### TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:25.426007Z",
     "iopub.status.busy": "2025-05-25T07:46:25.425711Z",
     "iopub.status.idle": "2025-05-25T07:46:25.464473Z",
     "shell.execute_reply": "2025-05-25T07:46:25.463977Z",
     "shell.execute_reply.started": "2025-05-25T07:46:25.425984Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "    num_train_epochs = 3,    # 训练 epoch\n",
    "    save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "    per_device_train_batch_size=32,  # 训练批次\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to='tensorboard',  # 训练输出记录\n",
    "    eval_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-25T07:46:25.465355Z",
     "iopub.status.busy": "2025-05-25T07:46:25.465123Z",
     "iopub.status.idle": "2025-05-25T07:46:25.590947Z",
     "shell.execute_reply": "2025-05-25T07:46:25.590450Z",
     "shell.execute_reply.started": "2025-05-25T07:46:25.465330Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                        num_labels=21,\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T08:30:01.851683Z",
     "iopub.status.busy": "2025-05-25T08:30:01.851176Z",
     "iopub.status.idle": "2025-05-25T08:30:01.856752Z",
     "shell.execute_reply": "2025-05-25T08:30:01.855998Z",
     "shell.execute_reply.started": "2025-05-25T08:30:01.851661Z"
    }
   },
   "outputs": [],
   "source": [
    "# metric 方法\n",
    "def compute_metric(result):\n",
    "    # result 是一个tuple (predicts, labels)\n",
    "    \n",
    "    # 获取评估对象\n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predicts,labels = result\n",
    "    predicts = np.argmax(prdicts, axis=2)\n",
    "    \n",
    "    # 准备评估数据\n",
    "    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    results = seqeval.compute(predictions=predicts, references=labels)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T08:30:19.058354Z",
     "iopub.status.busy": "2025-05-25T08:30:19.058065Z",
     "iopub.status.idle": "2025-05-25T08:30:19.082537Z",
     "shell.execute_reply": "2025-05-25T08:30:19.081782Z",
     "shell.execute_reply.started": "2025-05-25T08:30:19.058336Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds2['train'],\n",
    "    eval_dataset=ds2['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T08:30:25.933488Z",
     "iopub.status.busy": "2025-05-25T08:30:25.933226Z",
     "iopub.status.idle": "2025-05-25T08:33:51.068401Z",
     "shell.execute_reply": "2025-05-25T08:33:51.067509Z",
     "shell.execute_reply.started": "2025-05-25T08:30:25.933468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [504/504 03:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.175500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=504, training_loss=0.17525818352661435, metrics={'train_runtime': 204.5277, 'train_samples_per_second': 157.651, 'train_steps_per_second': 2.464, 'total_flos': 820469833815600.0, 'train_loss': 0.17525818352661435, 'epoch': 3.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:55:07.657027Z",
     "iopub.status.busy": "2025-05-25T07:55:07.656319Z",
     "iopub.status.idle": "2025-05-25T07:55:13.025352Z",
     "shell.execute_reply": "2025-05-25T07:55:13.024775Z",
     "shell.execute_reply.started": "2025-05-25T07:55:07.656997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.predict(ds2['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T08:21:27.274888Z",
     "iopub.status.busy": "2025-05-25T08:21:27.274620Z",
     "iopub.status.idle": "2025-05-25T08:21:27.281685Z",
     "shell.execute_reply": "2025-05-25T08:21:27.281116Z",
     "shell.execute_reply.started": "2025-05-25T08:21:27.274867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据北京市消防局的说法，此次火灾主要原因是责任单位违规燃放礼花弹，燃放期间民警多次劝阻未果。\n",
      "tensor([0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0])\n",
      "[   0    0    1    2    2    2    2    2    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    3    4    0    0    0\n",
      "    0    0    0    0 -100 -100 -100 -100]\n"
     ]
    }
   ],
   "source": [
    "print(ds1['validation'][10]['text'])\n",
    "print(ds2['validation'][10]['labels'])\n",
    "print(result.label_ids[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-25T08:24:37.844890Z",
     "iopub.status.busy": "2025-05-25T08:24:37.844364Z",
     "iopub.status.idle": "2025-05-25T08:24:37.851464Z",
     "shell.execute_reply": "2025-05-25T08:24:37.850779Z",
     "shell.execute_reply.started": "2025-05-25T08:24:37.844869Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'B-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-POSITION',\n",
       " 'I-POSITION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " [tags[p] for p,l in zip(result.label_ids[10],ds2['validation'][10]['labels'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T08:25:04.724510Z",
     "iopub.status.busy": "2025-05-25T08:25:04.723891Z",
     "iopub.status.idle": "2025-05-25T08:25:04.731215Z",
     "shell.execute_reply": "2025-05-25T08:25:04.730611Z",
     "shell.execute_reply.started": "2025-05-25T08:25:04.724487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'B-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-POSITION',\n",
       " 'I-POSITION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tags[l] for p,l in zip(result.label_ids[10],ds2['validation'][10]['labels'])]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
