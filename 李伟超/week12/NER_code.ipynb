{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3355a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ner_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ner_ddp.py\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "def extract_entities(sentence, model, tokenizer, id2label):\n",
    "    tokens = list(sentence)\n",
    "\n",
    "    # 分词 + 放入模型设备\n",
    "    inputs = tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # 模型预测\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "    # 标签解码\n",
    "    labels = [id2label[idx] for idx in predictions]\n",
    "\n",
    "    # BIO 解码成实体块\n",
    "    entities = []\n",
    "    entity = None\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if entity:\n",
    "                entities.append(entity)\n",
    "            entity = {\"entity\": label[2:], \"content\": token}\n",
    "        elif label.startswith(\"I-\") and entity and label[2:] == entity[\"entity\"]:\n",
    "            entity[\"content\"] += token\n",
    "        else:\n",
    "            if entity:\n",
    "                entities.append(entity)\n",
    "                entity = None\n",
    "    if entity:\n",
    "        entities.append(entity)\n",
    "\n",
    "    return entities\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 数据集\n",
    "    dataset = load_dataset('doushabao4766/msra_ner_k_V3')\n",
    "    label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if label[word_idx] != -100 else -100)\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        'google-bert/bert-base-chinese', \n",
    "        num_labels=len(label_list),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    model.to(local_rank)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./ner-model\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        # learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        report_to='tensorboard',  # 训练输出记录\n",
    "        num_train_epochs=3,\n",
    "        # weight_decay=0.01,\n",
    "        # save_total_limit=2,\n",
    "        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "        local_rank=local_rank,   # 当前进程 RANK\n",
    "        fp16=True,               # 使用混合精度\n",
    "        lr_scheduler_type='linear',  # 动态学习率\n",
    "        warmup_steps=100,        # 预热步数\n",
    "        ddp_find_unused_parameters=False  # 优化DDP性能\n",
    "    )\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "        true_predictions = [\n",
    "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        return {\n",
    "            \"f1\": seqeval.metrics.f1_score(true_labels, true_predictions),\n",
    "            \"precision\": seqeval.metrics.precision_score(true_labels, true_predictions),\n",
    "            \"recall\": seqeval.metrics.recall_score(true_labels, true_predictions),\n",
    "        }\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        # tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69bc236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\liwei\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ner_ddp.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
