{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom transformers import AutoTokenizer,AutoModelForTokenClassification,DataCollatorForTokenClassification\nfrom transformers import Trainer,TrainingArguments\nfrom datasets import load_dataset\nimport evaluate\nimport numpy as np\nimport seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:39:48.318727Z","iopub.execute_input":"2025-06-12T06:39:48.319626Z","iopub.status.idle":"2025-06-12T06:39:48.648210Z","shell.execute_reply.started":"2025-06-12T06:39:48.319590Z","shell.execute_reply":"2025-06-12T06:39:48.647494Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### 1、load_dataset下载、读取数据集；\n### 2、使用AutoTokenizer、AutoModelForToeknClassification加载分词器和生成模型框架","metadata":{}},{"cell_type":"code","source":"ds = load_dataset('doushabao4766/msra_ner_k_V3')\nprint(ds['train']['id'][16])\nprint(ds['train']['tokens'][16])\nprint(ds['train']['ner_tags'][16])\nprint(ds['train']['tokens'][16][10:13])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:39:51.355683Z","iopub.execute_input":"2025-06-12T06:39:51.356182Z","iopub.status.idle":"2025-06-12T06:39:59.654355Z","shell.execute_reply.started":"2025-06-12T06:39:51.356154Z","shell.execute_reply":"2025-06-12T06:39:59.653466Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90bb007feb00435798a4d66450f72eb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-42717a92413393f9.parquet:   0%|          | 0.00/13.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b6e610f37784cd9a14407eb36a1cb05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-8899cab5fdab45bc.parquet:   0%|          | 0.00/946k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ba68ea4f3643beb935f9ff91fe4e20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bf7797e4e224f9a8aa1b1e80d89f878"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b33b91f4175744ecb4e5c51f36a79778"}},"metadata":{}},{"name":"stdout","text":"16\n['去', '年', '，', '我', '们', '又', '被', '评', '为', '“', '北', '京', '市', '首', '届', '家', '庭', '藏', '书', '状', '元', '明', '星', '户', '”', '。']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n['北', '京', '市']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:40:00.664814Z","iopub.execute_input":"2025-06-12T06:40:00.665176Z","iopub.status.idle":"2025-06-12T06:40:01.366189Z","shell.execute_reply.started":"2025-06-12T06:40:00.665151Z","shell.execute_reply":"2025-06-12T06:40:01.365532Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae5f64f74ba4419b97ff35f29daf0259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2254d4f903f48819b50817f06f77683"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8315406f70fb4150b6008c41a8a05045"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c8a78c2de14d98816db4acc8c76fb1"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"tag_name = ds['train'].features['ner_tags'].feature.names\nprint(f'tag_name: {tag_name}')\ntag_id = set()\nfor id in ds['train']['ner_tags']:\n    tag_id.update(id)\ntag_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:40:04.371147Z","iopub.execute_input":"2025-06-12T06:40:04.371887Z","iopub.status.idle":"2025-06-12T06:40:05.849424Z","shell.execute_reply.started":"2025-06-12T06:40:04.371861Z","shell.execute_reply":"2025-06-12T06:40:05.848788Z"}},"outputs":[{"name":"stdout","text":"tag_name: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{0, 1, 2, 3, 4, 5, 6}"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### 3、根据transformers中dataset中features属性进行查看实体标签然后构造实体映射字典entite_index和实体词典集合tags","metadata":{}},{"cell_type":"code","source":"entites = ['O'] + list({'PER', 'LOC', 'ORG'})\ntags = ['O']\nfor ent in entites[1:]:\n    tags.append('B-' + ent.upper())\n    tags.append('I-' + ent.upper())\nentite_index = {entite:i for i,entite in enumerate(entites)}\nentite_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:40:08.559798Z","iopub.execute_input":"2025-06-12T06:40:08.560505Z","iopub.status.idle":"2025-06-12T06:40:08.566161Z","shell.execute_reply.started":"2025-06-12T06:40:08.560480Z","shell.execute_reply":"2025-06-12T06:40:08.565590Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'O': 0, 'LOC': 1, 'PER': 2, 'ORG': 3}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"tags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T05:34:12.287298Z","iopub.execute_input":"2025-06-12T05:34:12.287887Z","iopub.status.idle":"2025-06-12T05:34:12.292500Z","shell.execute_reply.started":"2025-06-12T05:34:12.287864Z","shell.execute_reply":"2025-06-12T05:34:12.291806Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['O', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-PER', 'I-PER']"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"def data_input_proc(item):\n    input_data = tokenizer(item['tokens'],\n                           padding='max_length',\n                           truncation = True,\n                           max_length=512,\n                           add_special_tokens=False,\n                           is_split_into_words=True)  #就是中文用list变为单个汉字(这个已经是单个字了)防止合并\n    # labels = [l[:512]for l in item['ner_tags']]\n    input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ner_tags']]\n    # input_data['labels'] = labels\n    return input_data\nds1 = ds.map(data_input_proc,batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:40:13.529316Z","iopub.execute_input":"2025-06-12T06:40:13.529636Z","iopub.status.idle":"2025-06-12T06:40:38.979807Z","shell.execute_reply.started":"2025-06-12T06:40:13.529613Z","shell.execute_reply":"2025-06-12T06:40:38.979062Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5078083bb3cb4a8eb4ce7d20dd10fdb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a32d28d2622b4a9c8953c134d02ae407"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"len(ds1['train']['input_ids'][30])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:54:54.944857Z","iopub.execute_input":"2025-06-12T06:54:54.945189Z","iopub.status.idle":"2025-06-12T06:54:55.124677Z","shell.execute_reply.started":"2025-06-12T06:54:54.945164Z","shell.execute_reply":"2025-06-12T06:54:55.123785Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"print(f'orginal: {ds1}')\nds1.set_format('torch',columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:14:39.835199Z","iopub.execute_input":"2025-06-12T07:14:39.835777Z","iopub.status.idle":"2025-06-12T07:14:39.841379Z","shell.execute_reply.started":"2025-06-12T07:14:39.835753Z","shell.execute_reply":"2025-06-12T07:14:39.840672Z"}},"outputs":[{"name":"stdout","text":"orginal: DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3443\n    })\n})\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"for it in ds1['train']:\n    print(it)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T03:41:08.326883Z","iopub.execute_input":"2025-06-12T03:41:08.327168Z","iopub.status.idle":"2025-06-12T03:41:08.340192Z","shell.execute_reply.started":"2025-06-12T03:41:08.327149Z","shell.execute_reply":"2025-06-12T03:41:08.339399Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n        1159, 8013,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"id2lbl = {i:k for i,k in enumerate(tags)}\nlbl2id = {k:i for i,k in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                        num_labels=len(tags),\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:40:48.847710Z","iopub.execute_input":"2025-06-12T06:40:48.848014Z","iopub.status.idle":"2025-06-12T06:40:51.682529Z","shell.execute_reply.started":"2025-06-12T06:40:48.847992Z","shell.execute_reply":"2025-06-12T06:40:51.681995Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c1be689f794c53981d10bb8a7563cc"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### 模型训练Trainer","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(output_dir = 'ner_train',     #训练保存的文件及模型\n                         num_train_epochs=3,           #训练的epoch数量\n                         save_safetensors=False,       #是否保存为safetensor\n                         per_device_train_batch_size=32, #训练的batch_size\n                         per_device_eval_batch_size=32,  #评估的batch size\n                         report_to='tensorboard',        # 训练记录保存方式默认是all保存到wandb中\n                         eval_strategy='epoch'          #每个epoch结束后评估一次\n                        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T03:41:23.283556Z","iopub.execute_input":"2025-06-12T03:41:23.283851Z","iopub.status.idle":"2025-06-12T03:41:23.319361Z","shell.execute_reply.started":"2025-06-12T03:41:23.283831Z","shell.execute_reply":"2025-06-12T03:41:23.318791Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"#可以看到我们没有做dataloader，所以在trainer中需要用DataCollatorForTokenClassification方式制造数据\n\n#将数据元素列表转换为批次矩阵的对象，做padding、在序列首尾添加特殊标记（如 [CLS]、[SEP]），并同步调整标签\n# 生成注意力掩码标签(attention mask) 、转换为张量\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer,\n                                                   padding=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T01:37:09.731379Z","iopub.execute_input":"2025-06-12T01:37:09.731980Z","iopub.status.idle":"2025-06-12T01:37:09.736062Z","shell.execute_reply.started":"2025-06-12T01:37:09.731958Z","shell.execute_reply":"2025-06-12T01:37:09.735201Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def compute_matric(result):\n     # result 是一个tuple (predicts, labels)\n\n    #获取评估对象( 初始化评估器,Hugging Face)\n    seqeval = evaluate.load('seqeval')\n    predicts,labels = result\n    predicts = np.argmax(predicts,axis=-1)\n\n    #准备评估数据\n    predicts = [[tags[p] for p,l in zip(ps,ls) if p != -100]for ps,ls in zip(predicts,labels)]\n    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]for ps,ls in zip(predicts,labels)]\n\n    #计算序列标注的评估指标\n    results = seqeval.compute(predicts=predicts,references=labels)\n    \n    # predictions：模型预测的标签序列（列表的列表）。\n    # references：真实标签序列（列表的列表）。\n    \n    return results\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T01:37:12.256982Z","iopub.execute_input":"2025-06-12T01:37:12.257737Z","iopub.status.idle":"2025-06-12T01:37:12.262954Z","shell.execute_reply.started":"2025-06-12T01:37:12.257714Z","shell.execute_reply":"2025-06-12T01:37:12.262152Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trainer = Trainer(model,\n                  args,\n                  train_dataset=ds1['train'],\n                  eval_dataset=ds1['test'],\n                  data_collator=data_collator,\n                  compute_metrics=compute_matric)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T01:37:15.293201Z","iopub.execute_input":"2025-06-12T01:37:15.293522Z","iopub.status.idle":"2025-06-12T01:37:16.039513Z","shell.execute_reply.started":"2025-06-12T01:37:15.293502Z","shell.execute_reply":"2025-06-12T01:37:16.038691Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"t.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 混合精度训练，所以要制造dataloder详细看步骤trainer也有单独的处理方式","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:41:03.141087Z","iopub.execute_input":"2025-06-12T06:41:03.141506Z","iopub.status.idle":"2025-06-12T06:41:03.146423Z","shell.execute_reply.started":"2025-06-12T06:41:03.141477Z","shell.execute_reply":"2025-06-12T06:41:03.145493Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"[name for name, params in model.named_parameters()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T05:36:38.781566Z","iopub.execute_input":"2025-06-12T05:36:38.781837Z","iopub.status.idle":"2025-06-12T05:36:38.788730Z","shell.execute_reply.started":"2025-06-12T05:36:38.781818Z","shell.execute_reply":"2025-06-12T05:36:38.787982Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['bert.embeddings.word_embeddings.weight',\n 'bert.embeddings.position_embeddings.weight',\n 'bert.embeddings.token_type_embeddings.weight',\n 'bert.embeddings.LayerNorm.weight',\n 'bert.embeddings.LayerNorm.bias',\n 'bert.encoder.layer.0.attention.self.query.weight',\n 'bert.encoder.layer.0.attention.self.query.bias',\n 'bert.encoder.layer.0.attention.self.key.weight',\n 'bert.encoder.layer.0.attention.self.key.bias',\n 'bert.encoder.layer.0.attention.self.value.weight',\n 'bert.encoder.layer.0.attention.self.value.bias',\n 'bert.encoder.layer.0.attention.output.dense.weight',\n 'bert.encoder.layer.0.attention.output.dense.bias',\n 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.0.intermediate.dense.weight',\n 'bert.encoder.layer.0.intermediate.dense.bias',\n 'bert.encoder.layer.0.output.dense.weight',\n 'bert.encoder.layer.0.output.dense.bias',\n 'bert.encoder.layer.0.output.LayerNorm.weight',\n 'bert.encoder.layer.0.output.LayerNorm.bias',\n 'bert.encoder.layer.1.attention.self.query.weight',\n 'bert.encoder.layer.1.attention.self.query.bias',\n 'bert.encoder.layer.1.attention.self.key.weight',\n 'bert.encoder.layer.1.attention.self.key.bias',\n 'bert.encoder.layer.1.attention.self.value.weight',\n 'bert.encoder.layer.1.attention.self.value.bias',\n 'bert.encoder.layer.1.attention.output.dense.weight',\n 'bert.encoder.layer.1.attention.output.dense.bias',\n 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.1.intermediate.dense.weight',\n 'bert.encoder.layer.1.intermediate.dense.bias',\n 'bert.encoder.layer.1.output.dense.weight',\n 'bert.encoder.layer.1.output.dense.bias',\n 'bert.encoder.layer.1.output.LayerNorm.weight',\n 'bert.encoder.layer.1.output.LayerNorm.bias',\n 'bert.encoder.layer.2.attention.self.query.weight',\n 'bert.encoder.layer.2.attention.self.query.bias',\n 'bert.encoder.layer.2.attention.self.key.weight',\n 'bert.encoder.layer.2.attention.self.key.bias',\n 'bert.encoder.layer.2.attention.self.value.weight',\n 'bert.encoder.layer.2.attention.self.value.bias',\n 'bert.encoder.layer.2.attention.output.dense.weight',\n 'bert.encoder.layer.2.attention.output.dense.bias',\n 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.2.intermediate.dense.weight',\n 'bert.encoder.layer.2.intermediate.dense.bias',\n 'bert.encoder.layer.2.output.dense.weight',\n 'bert.encoder.layer.2.output.dense.bias',\n 'bert.encoder.layer.2.output.LayerNorm.weight',\n 'bert.encoder.layer.2.output.LayerNorm.bias',\n 'bert.encoder.layer.3.attention.self.query.weight',\n 'bert.encoder.layer.3.attention.self.query.bias',\n 'bert.encoder.layer.3.attention.self.key.weight',\n 'bert.encoder.layer.3.attention.self.key.bias',\n 'bert.encoder.layer.3.attention.self.value.weight',\n 'bert.encoder.layer.3.attention.self.value.bias',\n 'bert.encoder.layer.3.attention.output.dense.weight',\n 'bert.encoder.layer.3.attention.output.dense.bias',\n 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.3.intermediate.dense.weight',\n 'bert.encoder.layer.3.intermediate.dense.bias',\n 'bert.encoder.layer.3.output.dense.weight',\n 'bert.encoder.layer.3.output.dense.bias',\n 'bert.encoder.layer.3.output.LayerNorm.weight',\n 'bert.encoder.layer.3.output.LayerNorm.bias',\n 'bert.encoder.layer.4.attention.self.query.weight',\n 'bert.encoder.layer.4.attention.self.query.bias',\n 'bert.encoder.layer.4.attention.self.key.weight',\n 'bert.encoder.layer.4.attention.self.key.bias',\n 'bert.encoder.layer.4.attention.self.value.weight',\n 'bert.encoder.layer.4.attention.self.value.bias',\n 'bert.encoder.layer.4.attention.output.dense.weight',\n 'bert.encoder.layer.4.attention.output.dense.bias',\n 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.4.intermediate.dense.weight',\n 'bert.encoder.layer.4.intermediate.dense.bias',\n 'bert.encoder.layer.4.output.dense.weight',\n 'bert.encoder.layer.4.output.dense.bias',\n 'bert.encoder.layer.4.output.LayerNorm.weight',\n 'bert.encoder.layer.4.output.LayerNorm.bias',\n 'bert.encoder.layer.5.attention.self.query.weight',\n 'bert.encoder.layer.5.attention.self.query.bias',\n 'bert.encoder.layer.5.attention.self.key.weight',\n 'bert.encoder.layer.5.attention.self.key.bias',\n 'bert.encoder.layer.5.attention.self.value.weight',\n 'bert.encoder.layer.5.attention.self.value.bias',\n 'bert.encoder.layer.5.attention.output.dense.weight',\n 'bert.encoder.layer.5.attention.output.dense.bias',\n 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.5.intermediate.dense.weight',\n 'bert.encoder.layer.5.intermediate.dense.bias',\n 'bert.encoder.layer.5.output.dense.weight',\n 'bert.encoder.layer.5.output.dense.bias',\n 'bert.encoder.layer.5.output.LayerNorm.weight',\n 'bert.encoder.layer.5.output.LayerNorm.bias',\n 'bert.encoder.layer.6.attention.self.query.weight',\n 'bert.encoder.layer.6.attention.self.query.bias',\n 'bert.encoder.layer.6.attention.self.key.weight',\n 'bert.encoder.layer.6.attention.self.key.bias',\n 'bert.encoder.layer.6.attention.self.value.weight',\n 'bert.encoder.layer.6.attention.self.value.bias',\n 'bert.encoder.layer.6.attention.output.dense.weight',\n 'bert.encoder.layer.6.attention.output.dense.bias',\n 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.6.intermediate.dense.weight',\n 'bert.encoder.layer.6.intermediate.dense.bias',\n 'bert.encoder.layer.6.output.dense.weight',\n 'bert.encoder.layer.6.output.dense.bias',\n 'bert.encoder.layer.6.output.LayerNorm.weight',\n 'bert.encoder.layer.6.output.LayerNorm.bias',\n 'bert.encoder.layer.7.attention.self.query.weight',\n 'bert.encoder.layer.7.attention.self.query.bias',\n 'bert.encoder.layer.7.attention.self.key.weight',\n 'bert.encoder.layer.7.attention.self.key.bias',\n 'bert.encoder.layer.7.attention.self.value.weight',\n 'bert.encoder.layer.7.attention.self.value.bias',\n 'bert.encoder.layer.7.attention.output.dense.weight',\n 'bert.encoder.layer.7.attention.output.dense.bias',\n 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.7.intermediate.dense.weight',\n 'bert.encoder.layer.7.intermediate.dense.bias',\n 'bert.encoder.layer.7.output.dense.weight',\n 'bert.encoder.layer.7.output.dense.bias',\n 'bert.encoder.layer.7.output.LayerNorm.weight',\n 'bert.encoder.layer.7.output.LayerNorm.bias',\n 'bert.encoder.layer.8.attention.self.query.weight',\n 'bert.encoder.layer.8.attention.self.query.bias',\n 'bert.encoder.layer.8.attention.self.key.weight',\n 'bert.encoder.layer.8.attention.self.key.bias',\n 'bert.encoder.layer.8.attention.self.value.weight',\n 'bert.encoder.layer.8.attention.self.value.bias',\n 'bert.encoder.layer.8.attention.output.dense.weight',\n 'bert.encoder.layer.8.attention.output.dense.bias',\n 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.8.intermediate.dense.weight',\n 'bert.encoder.layer.8.intermediate.dense.bias',\n 'bert.encoder.layer.8.output.dense.weight',\n 'bert.encoder.layer.8.output.dense.bias',\n 'bert.encoder.layer.8.output.LayerNorm.weight',\n 'bert.encoder.layer.8.output.LayerNorm.bias',\n 'bert.encoder.layer.9.attention.self.query.weight',\n 'bert.encoder.layer.9.attention.self.query.bias',\n 'bert.encoder.layer.9.attention.self.key.weight',\n 'bert.encoder.layer.9.attention.self.key.bias',\n 'bert.encoder.layer.9.attention.self.value.weight',\n 'bert.encoder.layer.9.attention.self.value.bias',\n 'bert.encoder.layer.9.attention.output.dense.weight',\n 'bert.encoder.layer.9.attention.output.dense.bias',\n 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.9.intermediate.dense.weight',\n 'bert.encoder.layer.9.intermediate.dense.bias',\n 'bert.encoder.layer.9.output.dense.weight',\n 'bert.encoder.layer.9.output.dense.bias',\n 'bert.encoder.layer.9.output.LayerNorm.weight',\n 'bert.encoder.layer.9.output.LayerNorm.bias',\n 'bert.encoder.layer.10.attention.self.query.weight',\n 'bert.encoder.layer.10.attention.self.query.bias',\n 'bert.encoder.layer.10.attention.self.key.weight',\n 'bert.encoder.layer.10.attention.self.key.bias',\n 'bert.encoder.layer.10.attention.self.value.weight',\n 'bert.encoder.layer.10.attention.self.value.bias',\n 'bert.encoder.layer.10.attention.output.dense.weight',\n 'bert.encoder.layer.10.attention.output.dense.bias',\n 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.10.intermediate.dense.weight',\n 'bert.encoder.layer.10.intermediate.dense.bias',\n 'bert.encoder.layer.10.output.dense.weight',\n 'bert.encoder.layer.10.output.dense.bias',\n 'bert.encoder.layer.10.output.LayerNorm.weight',\n 'bert.encoder.layer.10.output.LayerNorm.bias',\n 'bert.encoder.layer.11.attention.self.query.weight',\n 'bert.encoder.layer.11.attention.self.query.bias',\n 'bert.encoder.layer.11.attention.self.key.weight',\n 'bert.encoder.layer.11.attention.self.key.bias',\n 'bert.encoder.layer.11.attention.self.value.weight',\n 'bert.encoder.layer.11.attention.self.value.bias',\n 'bert.encoder.layer.11.attention.output.dense.weight',\n 'bert.encoder.layer.11.attention.output.dense.bias',\n 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.11.intermediate.dense.weight',\n 'bert.encoder.layer.11.intermediate.dense.bias',\n 'bert.encoder.layer.11.output.dense.weight',\n 'bert.encoder.layer.11.output.dense.bias',\n 'bert.encoder.layer.11.output.LayerNorm.weight',\n 'bert.encoder.layer.11.output.LayerNorm.bias',\n 'classifier.weight',\n 'classifier.bias']"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"#将数据通过dataloader封装打包\ntrain_dl = DataLoader(ds1['train'],shuffle=True,batch_size=16)\n#创建模型，传入参数做分类记得要放id和label转换的接口\nid2lbl = {i:k for i,k in enumerate(tags)}\nlbl2id = {k:i for i,k in enumerate(tags)}\nmodel2 = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                        num_labels=len(tags),\n                                                        id2label = id2lbl,\n                                                        label2id = lbl2id)\nmodel2.to('cuda')\n#模型参数分组\nparam_optimizer = list(model2.named_parameters())\nbert_params,classifier_params = [],[]\n#获取模型名称和参数，然后分别进行存储后指定学习率进行差分学习设置学习率\nfor name,params in param_optimizer:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n#AdamW有一股权重参数可以进行学习学习的参数以前都是直接用model.parameters这里可以用列表字典形式进行传入\noptimizer = optim.AdamW(param_groups)\n#学习率调度器(动态学习),transformers中提供的get_linear_schedule_with_warmup()\ntrain_steps = len(train_dl)*5\nscheduler = get_linear_schedule_with_warmup(optimizer,               #指定学习率的优化器\n                                           num_warmup_steps=100,     #预热阶段的步数\n                                           num_training_steps=train_steps)  #训练阶段的总步数","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:41:16.889300Z","iopub.execute_input":"2025-06-12T06:41:16.890175Z","iopub.status.idle":"2025-06-12T06:41:17.361235Z","shell.execute_reply.started":"2025-06-12T06:41:16.890148Z","shell.execute_reply":"2025-06-12T06:41:17.360671Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:13:20.801940Z","iopub.execute_input":"2025-06-12T07:13:20.802723Z","iopub.status.idle":"2025-06-12T07:13:20.806642Z","shell.execute_reply.started":"2025-06-12T07:13:20.802698Z","shell.execute_reply":"2025-06-12T07:13:20.805920Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for item in train_dl:\n    print(item['input_ids'].shape, \n          item['token_type_ids'].shape, \n          item['attention_mask'].shape, \n          item['labels'].shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T05:41:25.280374Z","iopub.execute_input":"2025-06-12T05:41:25.280882Z","iopub.status.idle":"2025-06-12T05:41:25.324395Z","shell.execute_reply.started":"2025-06-12T05:41:25.280857Z","shell.execute_reply":"2025-06-12T05:41:25.323834Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 512]) torch.Size([16, 512]) torch.Size([16, 512]) torch.Size([16, 512])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### 差分学习 动态学习率 warmup训练","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nDEVICE = 'cuda'\n\nfor epoch in range(5):\n    model2.train()\n    tpbar = tqdm(train_dl)   #一个batch一个batch的包拿过来循环对每个batch进行处理\n    for item in tpbar:\n        items = {k:v.to(DEVICE) for k,v in item.items()}  #这里就是tokenizer处理后的三个信息数值转成\n        optimizer.zero_grad()\n        logits = model2(**items)#**这里面会将item的几个key对应到模型中\n        loss =logits.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        tpbar.set_description(f'Epoch:{epoch+1} ' + \n                  f'bert_lr:{scheduler.get_lr()[0]} ' + \n                  f'classifier_lr:{scheduler.get_lr()[1]} '+\n                  f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:41:24.698776Z","iopub.execute_input":"2025-06-12T06:41:24.699266Z","iopub.status.idle":"2025-06-12T06:43:42.432001Z","shell.execute_reply.started":"2025-06-12T06:41:24.699243Z","shell.execute_reply":"2025-06-12T06:43:42.430907Z"}},"outputs":[{"name":"stderr","text":"Epoch:1 bert_lr:9e-06 classifier_lr:0.0009000000000000001 Loss:0.0668:   3%|▎         | 90/2813 [02:17<1:09:25,  1.53s/it]                  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1638603594.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#一个batch一个batch的包拿过来循环对每个batch进行处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m#这里就是tokenizer处理后的三个信息数值转成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[0;32m--> 171\u001b[0;31m                     {\n\u001b[0m\u001b[1;32m    172\u001b[0m                         key: collate(\n\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [512] at entry 0 and [710] at entry 13"],"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [512] at entry 0 and [710] at entry 13","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"print(len(train_dl))\nfor batch in train_dl:\n    print(\"input_ids shape:\", batch['input_ids'].shape)\n    print(\"labels shape:\", batch['labels'].shape)\n    print(\"input_ids:\", batch['input_ids'])\n    print(\"labels:\", batch['labels'])\n    break  # 只查看第一个批次","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:55:12.463636Z","iopub.execute_input":"2025-06-12T06:55:12.464388Z","iopub.status.idle":"2025-06-12T06:55:12.484053Z","shell.execute_reply.started":"2025-06-12T06:55:12.464338Z","shell.execute_reply":"2025-06-12T06:55:12.483402Z"}},"outputs":[{"name":"stdout","text":"2813\ninput_ids shape: torch.Size([16, 512])\nlabels shape: torch.Size([16, 512])\ninput_ids: tensor([[ 100, 1762,  686,  ...,    0,    0,    0],\n        [2769,  812, 2792,  ...,    0,    0,    0],\n        [ 686, 4518, 7942,  ...,    0,    0,    0],\n        ...,\n        [3315, 2237, 1300,  ...,    0,    0,    0],\n        [1420, 1400, 8024,  ...,    0,    0,    0],\n        [3219, 1921, 1762,  ...,    0,    0,    0]])\nlabels: tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [3, 4, 4,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### 混合精度训练","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom transformers import get_linear_schedule_with_warmup\n\ntrain_dl = DataLoader(ds1['train'],shuffle=True,batch_size=16)\n# 模型创建\nid2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                        num_labels=7,\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel.to('cuda')\n\n# 模型参数分组\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=100, \n                                            num_training_steps=train_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:27:18.426646Z","iopub.execute_input":"2025-06-12T07:27:18.426933Z","iopub.status.idle":"2025-06-12T07:27:18.739265Z","shell.execute_reply.started":"2025-06-12T07:27:18.426913Z","shell.execute_reply":"2025-06-12T07:27:18.738748Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nDEVICE = 'cuda'\n\nscaler =torch.GradScaler()\n\nfor epoch in range(5):\n    pbar=tqdm(train_dl)\n    for item in pbar:\n        items = {k:v.to(DEVICE) for k,v in item.items()}\n        optimizer.zero_grad()\n        \n        with torch.autocast(device_type='cuda'):\n            logits = model(**items)\n        loss = logits.loss\n        # 缩放loss后，调用backward\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n    pbar.set_description(f'Epoch:{epoch+1} ' + \n          f'bert_lr:{scheduler.get_lr()[0]} ' + \n          f'classifier_lr:{scheduler.get_lr()[1]} '+\n          f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:12:12.873746Z","iopub.execute_input":"2025-06-12T08:12:12.874367Z","iopub.status.idle":"2025-06-12T08:12:21.271380Z","shell.execute_reply.started":"2025-06-12T08:12:12.874329Z","shell.execute_reply":"2025-06-12T08:12:21.270383Z"}},"outputs":[{"name":"stderr","text":"  1%|          | 21/2813 [00:08<18:32,  2.51it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3533298744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[0;32m--> 171\u001b[0;31m                     {\n\u001b[0m\u001b[1;32m    172\u001b[0m                         key: collate(\n\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [512] at entry 0 and [699] at entry 6"],"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [512] at entry 0 and [699] at entry 6","output_type":"error"}],"execution_count":30},{"cell_type":"markdown","source":"### 分布式训练","metadata":{}},{"cell_type":"code","source":"%%writefile ddp_ner.py #jupyter notebook中的命令，作用是将 当前单元格中的代码写入指定的文件中\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\nfrom datasets import load_dataset\n\n#1、设置分布式环境\ndef setup(rank,world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group('nccl',rank=rank,world_size=world_size)\n\n#2、清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n\n#3、定义循环训练\ndef train(rank,world_size):\n    setup(rank,world_size)\n    #加载数据\n    ds = load_dataset('doushabao4766/msra_ner_k_V3')\n    #entites index\n    entites = ['O'] + list({'PER', 'LOC', 'ORG'})\n    tags = ['O']\n    for ent in entites[1:]:\n        tags.append('B-' + ent)\n        tags.append('I-' + ent)\n    entite_index = [k:i for i,k in enumerate(entites)]\n\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    def data_input_proc(item):\n        input_data = tokenizer(item['tokens'],\n                              max_length=512,\n                              truncation=True,\n                              padding='max_length',\n                              add_special_tokens=False,\n                              is_split_into_words=True)\n        input_data['labels'] = [tag + [0]*(512-len(tag)) for tag in item['ner_tags']]\n        return input_data\n\n    ds1 = ds.map(data_input_proc,batched=True) #batched == 1000\n    \n    local_rank = rank\n            \n    id2lbl = {i:tag for i, tag in enumerate(tags)}\n    lbl2id = {tag:i for i, tag in enumerate(tags)}\n    \n    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                            num_labels=21,\n                                                            id2label=id2lbl,\n                                                            label2id=lbl2id)\n    model.to(local_rank)\n    \n    # 配置TrainingArguments 和 trainer\n    args = TrainingArguments(\n        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n        num_train_epochs = 3,    # 训练 epoch\n        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n        per_device_train_batch_size=16,  # 训练批次设置\n        per_device_eval_batch_size=16,   # 验证批次设置\n        report_to='tensorboard',  # 训练输出记录\n        eval_strategy=\"epoch\",    #每个epoch结束后评估一次\n        local_rank=local_rank,   # 当前进程 RANK\n        fp16=True,               # 使用混合精度\n        lr_scheduler_type='linear',  # 动态学习率, 线性平滑的学习率\n        warmup_steps=100,        # 预热步数\n        ddp_find_unused_parameters=False  # 优化DDP性能\n    )\n\n    # 目的是计算模型的性能指标，使用了 seqeval 库进行序列标注任务的评估\n    def compute_metric(result):\n        # result 是一个tuple (predicts, labels)\n        \n        # 评估对象 seqeval:是一个用于序列标注任务（命名实体），可以计算精确率（precision）、召回率recall和F1分值F1-score\n        seqeval = evaluate.load('seqeval')\n        predicts,labels = result\n        predicts = np.argmax(prdicts, axis=2)\n        \n        # 准备评估数据\n        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        results = seqeval.compute(predictions=predicts, references=labels) # 获取评估结果：模型的性能指标\n    \n        return results\n\n    # 用于处理序列标注任务的数据批处理 相当于dataloader\n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n    \n    trainer = Trainer(\n        model,\n        args,   #训练参数，通常是一个 TrainingArguments 对象，包含学习率、批次大小、训练轮数等。。。\n        train_dataset=ds2['train'],\n        eval_dataset=ds2['validation'],\n        data_collator=data_collator,     #数据批处理工具，这里使用 DataCollatorForTokenClassification\n        compute_metrics=compute_metric   #评估函数，这里使用前面定义的 compute_metric 函数\n    )\n    \n    trainer.train()\n\ndef main():\n    world_size = torch.cuda.device_count() #计算GPU数量\n    #mp.spawn(fn, args=(), nprocs=1, join=True, daemon=False)\n    # fn：要启动的函数。这个函数会在每个进程中执行，args: fn 函数的额外参数,args=(world_size,) 表示将 world_size 作为参数传递给 train 函数,\n    # 注意，参数必须是元组，所以即使只有一个参数，也需要在末尾加逗号，如 (world_size,)\n    # nprocs:指定启动的进程总数，通常设置为可用GPU数量,join:主进程阻塞等待所有子进程结束,同步方式\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) #mp包中用spawn创建线程，\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:31:29.545117Z","iopub.execute_input":"2025-06-12T15:31:29.545793Z","iopub.status.idle":"2025-06-12T15:31:29.554231Z","shell.execute_reply.started":"2025-06-12T15:31:29.545759Z","shell.execute_reply":"2025-06-12T15:31:29.553064Z"}},"outputs":[{"name":"stderr","text":"UsageError: unrecognized arguments: #jupyter notebook中的命令，作用是将 当前单元格中的代码写入指定的文件中\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!python ddp_ner.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:31:23.311071Z","iopub.execute_input":"2025-06-12T15:31:23.311413Z","iopub.status.idle":"2025-06-12T15:31:23.551712Z","shell.execute_reply.started":"2025-06-12T15:31:23.311390Z","shell.execute_reply":"2025-06-12T15:31:23.550688Z"}},"outputs":[{"name":"stdout","text":"python3: can't open file '/kaggle/working/ddp_ner.py': [Errno 2] No such file or directory\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### 利用训练后的模型推理","metadata":{}},{"cell_type":"code","source":"# 加载保存的模型和分词器\nmodel = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"ner_model\")\n\n# 假设有一个测试样本\ntest_sample = {\n    \"tokens\": [\"北\", \"京\", \"市\", \"好\"]\n}\n\n# 对测试样本进行分词\ninputs = tokenizer(\n    test_sample[\"tokens\"],\n    return_tensors=\"pt\",\n    max_length=512,\n    truncation=True,\n    padding=\"max_length\",\n    add_special_tokens=False,\n    is_split_into_words=True\n)\n\n# 模型推理\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# 获取预测标签\npredictions = torch.argmax(outputs.logits, dim=2).numpy()\n\n# 转换预测结果为标签\npredicted_labels = [id2lbl[p] for p in predictions[0]]\n\nprint(\"Predicted labels:\", predicted_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:32:38.116447Z","iopub.execute_input":"2025-06-12T15:32:38.116747Z","iopub.status.idle":"2025-06-12T15:32:57.595671Z","shell.execute_reply.started":"2025-06-12T15:32:38.116723Z","shell.execute_reply":"2025-06-12T15:32:57.594446Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/ner_model/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1642\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1644\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1532\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1449\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    458\u001b[0m             )\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-684af329-7aa58a4a140c20397653314a;8ecaed6a-50cc-4751-a381-9589f7267d97)\n\nRepository Not Found for url: https://huggingface.co/ner_model/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4084766832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 加载保存的模型和分词器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    493\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# We cannot recover from them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: ner_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"],"ename":"OSError","evalue":"ner_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"for sample in ds1['train']:\n    assert len(sample['input_ids']) == len(sample['labels']), \"输入和标签长度不一致!\"\n# 检查数据集中每个样本的长度\n# for sample in ds1['train']:\n#     if assert len(sample['input_ids']) != len(sample['labels']):\n#         print(len(sample['input_ids']))\n#         print( len(sample['labels']))\n        \n\n    # print(\"input_ids length:\", len(sample['input_ids']))\n    # print(\"labels length:\", len(sample['labels']))\n    # # 如果发现长度不一致，打印出该样本\n    # if len(sample['input_ids']) != len(sample['labels']):\n    #     print(\"Mismatched lengths found!\")\n    #     print(\"input_ids:\", sample['input_ids'])\n    #     print(\"labels:\", sample['labels'])\n    #     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:27:59.299702Z","iopub.execute_input":"2025-06-12T08:27:59.299990Z","iopub.status.idle":"2025-06-12T08:28:02.129206Z","shell.execute_reply.started":"2025-06-12T08:27:59.299969Z","shell.execute_reply":"2025-06-12T08:28:02.128377Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2878755821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"输入和标签长度不一致!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 检查数据集中每个样本的长度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for sample in ds1['train']:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     if assert len(sample['input_ids']) != len(sample['labels']):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: 输入和标签长度不一致!"],"ename":"AssertionError","evalue":"输入和标签长度不一致!","output_type":"error"}],"execution_count":41},{"cell_type":"code","source":"token_index = tokenizer.encode('2000年2月add', add_special_tokens=False)\nprint(token_index)\ntokens = tokenizer.decode(token_index)\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T01:47:56.798168Z","iopub.execute_input":"2025-06-12T01:47:56.798471Z","iopub.status.idle":"2025-06-12T01:47:56.803806Z","shell.execute_reply.started":"2025-06-12T01:47:56.798450Z","shell.execute_reply":"2025-06-12T01:47:56.803137Z"}},"outputs":[{"name":"stdout","text":"[8202, 2399, 123, 3299, 10253]\n2000 年 2 月 add\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"input_data = tokenizer([list('2000年2月add')], add_special_tokens=False, truncation=True, \n                       is_split_into_words=True)\nprint(input_data)\n#中文Bert分词在日期时间和英文转换上会合并如上，影响ner的准确性，\n#所以用将文本拆分为list就变成单个词了需要在tokenizer中属性: is_split_into_words=True，这样可以一一对应如下。\n\ntokens = tokenizer.decode(token_index)\nprint(tokens) # 返回token对应逐个字符","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T02:06:34.669076Z","iopub.execute_input":"2025-06-12T02:06:34.669764Z","iopub.status.idle":"2025-06-12T02:06:34.675111Z","shell.execute_reply.started":"2025-06-12T02:06:34.669739Z","shell.execute_reply":"2025-06-12T02:06:34.674419Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[123, 121, 121, 121, 2399, 123, 3299, 143, 146, 146]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n2000 年 2 月 add\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"for item in ds1['train']:\n    print(len(item['input_ids']), len(item['token_type_ids']), len(item['attention_mask']), len(item['labels']))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T02:12:30.424387Z","iopub.execute_input":"2025-06-12T02:12:30.424670Z","iopub.status.idle":"2025-06-12T02:12:30.431494Z","shell.execute_reply.started":"2025-06-12T02:12:30.424652Z","shell.execute_reply":"2025-06-12T02:12:30.430613Z"}},"outputs":[{"name":"stdout","text":"50 50 50 50\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}