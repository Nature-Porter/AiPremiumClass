{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 实现基于豆瓣top250图书评论的简单推荐系统（TF-IDF及BM25两种算法实现）\n",
    "# 2. 使用自定义的文档文本，通过fasttext训练word2vec训练词向量模型，并计算词汇间的相关度。（选做：尝试tensorboard绘制词向量可视化图）\n",
    "# 3. 使用课堂示例cooking.stackexchange.txt，使用fasttext训练文本分类模型。（选做：尝试使用Kaggle中的Fake News数据集训练文本分类模型）\n",
    "# https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件到comments\n",
    "def load_comments(file_path):\n",
    "    comments =[]\n",
    "    try:\n",
    "        with open(file_path , 'r' , encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                comment = line.strip()\n",
    "                if comment:\n",
    "                        comments.append(comment)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误:文件{file_path}未找到。\")\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = load_comments(\"doubanbook_top250_comments.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(comments , k=1.5, b=0.75):\n",
    "    # 计算文档总数\n",
    "    N = len(comments)\n",
    "    # 初始化文档长度列表和词频字典\n",
    "    doc_length = []\n",
    "    word_doc_freq = {}\n",
    "    doc_term_dict = [{} for _ in range(N)]\n",
    "\n",
    "    for i , comment in enumerate(comments):\n",
    "        # 记录文档长度\n",
    "        doc_length.append(len(comment.split()))\n",
    "        unique_words = set()\n",
    "\n",
    "        for word in comment.split():\n",
    "            # 统计词频\n",
    "            doc_term_dict[i][word] = doc_term_dict[i].get(word , 0) + 1\n",
    "            # 上边这句的逻辑：i为当前文章索引，如果word在字典里面，获取+1，不在的话，设置为1\n",
    "            unique_words.add(word)\n",
    "            # 上边这局，是对当前文档，进行数据集\n",
    "\n",
    "        for word in unique_words:\n",
    "            word_doc_freq[word] = word_doc_freq.get(word,0) + 1\n",
    "        # 这个代码段是计算所有文档中，当前单词的频数\n",
    "    \n",
    "    # 计算每个单词的平均文档长度\n",
    "    avg_doc_len= sum(doc_length) / N\n",
    "\n",
    "    # 构建词汇表\n",
    "    vocabulary = list(word_doc_freq.keys())\n",
    "    word_index = {word:idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "    # 构建文档、词频矩阵\n",
    "    doc_term_matrix = np.zeros((N,len(vocabulary)))\n",
    "    for i in range(N):\n",
    "        for word , freq in doc_term_dict[i].items():\n",
    "            idx = word_index.get(word)\n",
    "            if idx is not None:\n",
    "                doc_term_matrix[i, idx] = freq\n",
    "\n",
    "    # 计算idf值\n",
    "    idf_numerator = N - np.array([word_doc_freq[word] for word in vocabulary]) + 0.5\n",
    "    idf_denominator = np.array([word_doc_freq[word] for word in vocabulary]) + 0.5\n",
    "    idf = np.log(idf_numerator / idf_denominator)\n",
    "    idf[idf_numerator <= 0] = 0 # 避免出现nan值\n",
    "\n",
    "    # 计算bm25值\n",
    "    doc_length = np.array(doc_length)\n",
    "    bm25_matrix = np.zeros((N, len(vocabulary)))\n",
    "    for i in range(N):\n",
    "        tf = doc_term_matrix[i]\n",
    "        bm25 = idf * (tf * (k + 1)) / (tf + k * (1 - b + b * doc_length[i] / avg_doc_len))\n",
    "        bm25_matrix[i] = bm25\n",
    "    \n",
    "    return bm25_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(comments , query , method = 'tfidf'):\n",
    "    if method == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(comments)\n",
    "        query_vector = vectorizer.transform([query])\n",
    "        scores = (tfidf_matrix * query_vector.T).toarray().flatten()\n",
    "    elif method == 'bm25':\n",
    "        bm25_matrix = bm25(comments)\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit(comments)\n",
    "        query_vector = vectorizer.transform([query]).toarray()[0]\n",
    "        vocabulary = vectorizer.get_feature_names_out()\n",
    "        word_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "        scores = []\n",
    "        for i in range(len(comments)):\n",
    "            score = 0\n",
    "            for word in query.split():\n",
    "                if word in word_index:\n",
    "                    idx = word_index[word]\n",
    "                    score += bm25_matrix[i][idx]\n",
    "            scores.append(score)\n",
    "    else:\n",
    "        print(\"不支持的方法，请选择 'tfidf' 或 'bm25'。\")\n",
    "        return []\n",
    "\n",
    "    ranked_indices = np.argsort(scores)[::-1]\n",
    "    return ranked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 110. GiB for an array with shape (99257, 148673) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m一本关于历史的好书\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m tfidf_recommendations \u001b[38;5;241m=\u001b[39m recommend_books(comments, query, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m bm25_recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_books\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbm25\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF - IDF 推荐结果（前5个）:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tfidf_recommendations))):\n",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m, in \u001b[0;36mrecommend_books\u001b[1;34m(comments, query, method)\u001b[0m\n\u001b[0;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m (tfidf_matrix \u001b[38;5;241m*\u001b[39m query_vector\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbm25\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m     bm25_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mbm25\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m     10\u001b[0m     vectorizer\u001b[38;5;241m.\u001b[39mfit(comments)\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mbm25\u001b[1;34m(comments, k, b)\u001b[0m\n\u001b[0;32m     30\u001b[0m word_index \u001b[38;5;241m=\u001b[39m {word:idx \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocabulary)}\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 构建文档、词频矩阵\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m doc_term_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word , freq \u001b[38;5;129;01min\u001b[39;00m doc_term_dict[i]\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 110. GiB for an array with shape (99257, 148673) and data type float64"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"doubanbook_top250_comments.txt\"\n",
    "    comments = load_comments(file_path)\n",
    "    query = \"一本关于历史的好书\"\n",
    "    tfidf_recommendations = recommend_books(comments, query, method='tfidf')\n",
    "    bm25_recommendations = recommend_books(comments, query, method='bm25')\n",
    "\n",
    "    print(\"TF - IDF 推荐结果（前5个）:\")\n",
    "    for i in range(min(5, len(tfidf_recommendations))):\n",
    "        print(comments[tfidf_recommendations[i]])\n",
    "\n",
    "    print(\"\\nBM25 推荐结果（前5个）:\")\n",
    "    for i in range(min(5, len(bm25_recommendations))):\n",
    "        print(comments[bm25_recommendations[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 使用自定义的文档文本，通过fasttext训练word2vec训练词向量模型，并计算词汇间的相关度。（选做：尝试tensorboard绘制词向量可视化图）\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfasttext\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfasttext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "# 使用自定义的文档文本，通过fasttext训练word2vec训练词向量模型，并计算词汇间的相关度。（选做：尝试tensorboard绘制词向量可视化图）\n",
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fasttext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(doc \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 训练 fastText 模型\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_unsupervised(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_text.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskipgram\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_fasttext_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fasttext' is not defined"
     ]
    }
   ],
   "source": [
    "# 自定义文档文本\n",
    "documents = [\n",
    "    \"我喜欢阅读书籍\",\n",
    "    \"书籍是知识的源泉\",\n",
    "    \"阅读能开阔视野\",\n",
    "    \"知识改变命运\"\n",
    "]\n",
    "\n",
    "# 将文档写入临时文件，fastText 训练需要从文件读取数据\n",
    "with open('custom_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for doc in documents:\n",
    "        f.write(doc + '\\n')\n",
    "\n",
    "# 训练 fastText 模型\n",
    "model = fasttext.train_unsupervised('custom_text.txt', model='skipgram')\n",
    "\n",
    "# 保存模型\n",
    "model.save_model(\"custom_fasttext_model.bin\")\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = fasttext.load_model(\"custom_fasttext_model.bin\")\n",
    "\n",
    "# 计算词汇间的相关度\n",
    "word1 = \"阅读\"\n",
    "word2 = \"书籍\"\n",
    "\n",
    "# 获取词向量\n",
    "vec1 = loaded_model.get_word_vector(word1)\n",
    "vec2 = loaded_model.get_word_vector(word2)\n",
    "\n",
    "# 计算余弦相似度来衡量相关度\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "vec1 = np.array(vec1).reshape(1, -1)\n",
    "vec2 = np.array(vec2).reshape(1, -1)\n",
    "similarity = cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "print(f\"词汇 '{word1}' 和 '{word2}' 的相关度为: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
