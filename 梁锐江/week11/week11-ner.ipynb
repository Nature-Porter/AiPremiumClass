{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification,AutoTokenizer,TrainingArguments,Trainer,DataCollatorForTokenClassification\nimport torch\nfrom datasets import load_dataset\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import Dataset\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:20.525000Z","iopub.execute_input":"2025-06-05T12:22:20.525649Z","iopub.status.idle":"2025-06-05T12:22:20.529322Z","shell.execute_reply.started":"2025-06-05T12:22:20.525626Z","shell.execute_reply":"2025-06-05T12:22:20.528491Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',num_labels=7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:20.533865Z","iopub.execute_input":"2025-06-05T12:22:20.534080Z","iopub.status.idle":"2025-06-05T12:22:21.804462Z","shell.execute_reply.started":"2025-06-05T12:22:20.534065Z","shell.execute_reply":"2025-06-05T12:22:21.803854Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:21.805909Z","iopub.execute_input":"2025-06-05T12:22:21.806138Z","iopub.status.idle":"2025-06-05T12:22:21.924752Z","shell.execute_reply.started":"2025-06-05T12:22:21.806115Z","shell.execute_reply":"2025-06-05T12:22:21.923932Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# message = \"命名实体识别\"\n# label = torch.tensor([0,1,0,2,5,4])\n\n# model_input = tokenizer([message],return_tensors='pt')\n# print(model_input)\n# result = model(**model_input)\n# print(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:21.925568Z","iopub.execute_input":"2025-06-05T12:22:21.925786Z","iopub.status.idle":"2025-06-05T12:22:21.929150Z","shell.execute_reply.started":"2025-06-05T12:22:21.925761Z","shell.execute_reply":"2025-06-05T12:22:21.928499Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# token_index = tokenizer.encode('2000年2月add', add_special_tokens=False)\n# print(token_index)\n# tokens = tokenizer.decode(token_index)\n# print(tokens)\n\n# input_data = tokenizer(['2000年2月add','testing and',], add_special_tokens=False, truncation=True)\n# print(input_data)\n# input_data.word_ids(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:21.930475Z","iopub.execute_input":"2025-06-05T12:22:21.930662Z","iopub.status.idle":"2025-06-05T12:22:21.944265Z","shell.execute_reply.started":"2025-06-05T12:22:21.930648Z","shell.execute_reply":"2025-06-05T12:22:21.943614Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# 加载dataset\nds = load_dataset('nlhappy/CLUE-NER')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:21.945104Z","iopub.execute_input":"2025-06-05T12:22:21.945612Z","iopub.status.idle":"2025-06-05T12:22:23.687642Z","shell.execute_reply.started":"2025-06-05T12:22:21.945589Z","shell.execute_reply":"2025-06-05T12:22:23.686983Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'ents'],\n        num_rows: 10748\n    })\n    validation: Dataset({\n        features: ['text', 'ents'],\n        num_rows: 1343\n    })\n})"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"for data in ds['train']:\n    print(data)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:23.688414Z","iopub.execute_input":"2025-06-05T12:22:23.688625Z","iopub.status.idle":"2025-06-05T12:22:23.693378Z","shell.execute_reply.started":"2025-06-05T12:22:23.688609Z","shell.execute_reply":"2025-06-05T12:22:23.692622Z"}},"outputs":[{"name":"stdout","text":"{'text': '浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，', 'ents': [{'indices': [9, 10, 11], 'is_continuous': True, 'label': 'name', 'text': '叶老桂'}, {'indices': [0, 1, 2, 3], 'is_continuous': True, 'label': 'company', 'text': '浙商银行'}]}\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"classication_set = set()\nfor data in ds['train']:\n    ents = data['ents']\n    for ent in ents:\n        label = ent['label']\n        classication_set.add(label)\nprint(classication_set)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:23.694266Z","iopub.execute_input":"2025-06-05T12:22:23.695011Z","iopub.status.idle":"2025-06-05T12:22:24.373740Z","shell.execute_reply.started":"2025-06-05T12:22:23.694991Z","shell.execute_reply":"2025-06-05T12:22:24.373135Z"}},"outputs":[{"name":"stdout","text":"{'company', 'organization', 'government', 'position', 'movie', 'game', 'address', 'book', 'name', 'scene'}\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"other_label = ['0']\nlabel_list = other_label + list(classication_set)\nlabel_list\n\nlabel_indx = {label:idx for idx,label in enumerate(label_list)}\nprint(label_indx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:24.374483Z","iopub.execute_input":"2025-06-05T12:22:24.374730Z","iopub.status.idle":"2025-06-05T12:22:24.379079Z","shell.execute_reply.started":"2025-06-05T12:22:24.374712Z","shell.execute_reply":"2025-06-05T12:22:24.378495Z"}},"outputs":[{"name":"stdout","text":"{'0': 0, 'company': 1, 'organization': 2, 'government': 3, 'position': 4, 'movie': 5, 'game': 6, 'address': 7, 'book': 8, 'name': 9, 'scene': 10}\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"tags = other_label\nfor label in classication_set:\n    tags.append('B-' + label.upper())\n    tags.append('I-' + label.upper())\nprint(tags)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:24.379670Z","iopub.execute_input":"2025-06-05T12:22:24.379892Z","iopub.status.idle":"2025-06-05T12:22:24.392234Z","shell.execute_reply.started":"2025-06-05T12:22:24.379876Z","shell.execute_reply":"2025-06-05T12:22:24.391608Z"}},"outputs":[{"name":"stdout","text":"['0', 'B-COMPANY', 'I-COMPANY', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-GOVERNMENT', 'I-GOVERNMENT', 'B-POSITION', 'I-POSITION', 'B-MOVIE', 'I-MOVIE', 'B-GAME', 'I-GAME', 'B-ADDRESS', 'I-ADDRESS', 'B-BOOK', 'I-BOOK', 'B-NAME', 'I-NAME', 'B-SCENE', 'I-SCENE']\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"def ent_indices_convert(item):\n    ents = item['ents']\n    convert_indices = [0] * len(item['text'])\n    for ent in ents:\n        indices = ent['indices']\n        label = ent['label']\n        convert_indices[indices[0]] = label_indx[label] * 2 - 1\n        for org_idx in indices[1:]:\n            convert_indices[org_idx] = label_indx[label] * 2\n    return {'converted_indices':convert_indices}\n    \n\nfirst_proc_ds = ds.map(ent_indices_convert)\nprint(first_proc_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:24.394632Z","iopub.execute_input":"2025-06-05T12:22:24.395042Z","iopub.status.idle":"2025-06-05T12:22:24.411718Z","shell.execute_reply.started":"2025-06-05T12:22:24.395024Z","shell.execute_reply":"2025-06-05T12:22:24.411167Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'ents', 'converted_indices'],\n        num_rows: 10748\n    })\n    validation: Dataset({\n        features: ['text', 'ents', 'converted_indices'],\n        num_rows: 1343\n    })\n})\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"# 训练集\nfor row in first_proc_ds['train']:\n    print(row['text'])\n    print(row['converted_indices'])\n    \n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:24.412345Z","iopub.execute_input":"2025-06-05T12:22:24.412538Z","iopub.status.idle":"2025-06-05T12:22:24.419939Z","shell.execute_reply.started":"2025-06-05T12:22:24.412524Z","shell.execute_reply":"2025-06-05T12:22:24.419405Z"}},"outputs":[{"name":"stdout","text":"浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，\n[1, 2, 2, 2, 0, 0, 0, 0, 0, 17, 18, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"def data_input_proc(item):\n    input_data = tokenizer(item['text'],truncation=True,add_special_tokens=False,max_length=512)\n    adjust_labels = []\n    for sample_index in range(len(input_data['input_ids'])):\n        s_input_idx =input_data.word_ids(sample_index)\n        # 拿到BI转化的序列\n        convert_indices = item['converted_indices'][sample_index]\n        sample_list = []\n        i,pre_idx = -1,-1\n        for idx in s_input_idx:\n            # [0, 0, 1] 上一索引与当前索引不一致\n            if idx != pre_idx:\n                i += 1\n                pre_idx = idx\n            sample_list.append(convert_indices[i])\n        adjust_labels.append(sample_list)\n    input_data['labels'] = adjust_labels\n    return input_data\n\n\nenc_label_map = first_proc_ds.map(data_input_proc,batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:24.420640Z","iopub.execute_input":"2025-06-05T12:22:24.420828Z","iopub.status.idle":"2025-06-05T12:22:25.412008Z","shell.execute_reply.started":"2025-06-05T12:22:24.420814Z","shell.execute_reply":"2025-06-05T12:22:25.411479Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389f0bd063564d4a879327e106e9fa44"}},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"# 记录转换为pytorch\nenc_label_map.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\nfor item in enc_label_map['train']:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:25.412701Z","iopub.execute_input":"2025-06-05T12:22:25.413002Z","iopub.status.idle":"2025-06-05T12:22:25.420499Z","shell.execute_reply.started":"2025-06-05T12:22:25.412984Z","shell.execute_reply":"2025-06-05T12:22:25.419834Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([3851, 1555, 7213, 6121,  821,  689,  928, 6587, 6956, 1383, 5439, 3424,\n        1300, 1894, 1156,  794, 1369,  671,  702, 6235, 2428, 2190,  758, 6887,\n        7305, 3546, 6822, 6121,  749, 6237, 6438,  511, 1383, 5439, 3424, 6371,\n         711, 8024, 2190, 4680, 1184, 1744, 1079, 1555,  689, 7213, 6121, 5445,\n        6241, 8024]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]), 'labels': tensor([ 1,  2,  2,  2,  0,  0,  0,  0,  0, 17, 18, 18,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])}\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"ner_train\",\n    num_train_epochs = 3,\n    save_safetensors = False,\n    per_device_train_batch_size = 32,\n    per_device_eval_batch_size=32,\n    report_to='tensorboard',  # 训练输出记录\n    eval_strategy=\"epoch\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:25.421322Z","iopub.execute_input":"2025-06-05T12:22:25.421970Z","iopub.status.idle":"2025-06-05T12:22:25.468877Z","shell.execute_reply.started":"2025-06-05T12:22:25.421953Z","shell.execute_reply":"2025-06-05T12:22:25.468192Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"id2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}\n\nner_model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                           num_labels=21,\n                                                           id2label=id2lbl,\n                                                           label2id=lbl2id)\nner_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:25.469724Z","iopub.execute_input":"2025-06-05T12:22:25.470336Z","iopub.status.idle":"2025-06-05T12:22:25.726320Z","shell.execute_reply.started":"2025-06-05T12:22:25.470318Z","shell.execute_reply":"2025-06-05T12:22:25.725837Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=21, bias=True)\n)"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"def compute_metric(result):\n    # 这是 Trainer 的约定，result 一定是一个 (predictions, labels) 的元组，所以可以直接解包。\n    \n    # 获取评估对象\n    seqeval = evaluate.load('seqeval')\n    predicts,labels = result\n    predicts = np.argmax(prdicts,axis=-1)\n\n    predicts = [[tags[p] for p,l in zip(ps,ls) if l != 100]\n        for ps,ls in zip(predicts,labels)]\n    labels = [[tags[l] for p,l in zip(ps,ls) if l != 100]\n        for ps,ls in zip(predicts,labels)]\n    results = seqeval.compute(predictions=predicts, references=labels)\n    return results\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:25.727025Z","iopub.execute_input":"2025-06-05T12:22:25.727247Z","iopub.status.idle":"2025-06-05T12:22:25.732073Z","shell.execute_reply.started":"2025-06-05T12:22:25.727225Z","shell.execute_reply":"2025-06-05T12:22:25.731392Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer,padding=True)\n\ntrainer = Trainer(\n    model=ner_model,\n    args=args,\n    train_dataset=enc_label_map['train'],\n    eval_dataset=enc_label_map['validation'],\n    data_collator=data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:25.732785Z","iopub.execute_input":"2025-06-05T12:22:25.732953Z","iopub.status.idle":"2025-06-05T12:22:25.922140Z","shell.execute_reply.started":"2025-06-05T12:22:25.732939Z","shell.execute_reply":"2025-06-05T12:22:25.921697Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:22:25.922618Z","iopub.execute_input":"2025-06-05T12:22:25.922784Z","iopub.status.idle":"2025-06-05T12:25:55.688264Z","shell.execute_reply.started":"2025-06-05T12:22:25.922771Z","shell.execute_reply":"2025-06-05T12:25:55.687603Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [504/504 03:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.383259</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.326066</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.400900</td>\n      <td>0.321430</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=504, training_loss=0.3997058844755566, metrics={'train_runtime': 209.2042, 'train_samples_per_second': 154.127, 'train_steps_per_second': 2.409, 'total_flos': 820469833815600.0, 'train_loss': 0.3997058844755566, 'epoch': 3.0})"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"result = trainer.predict(enc_label_map['validation'])\nresult","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:38:03.315822Z","iopub.execute_input":"2025-06-05T12:38:03.316514Z","iopub.status.idle":"2025-06-05T12:38:06.061067Z","shell.execute_reply.started":"2025-06-05T12:38:03.316483Z","shell.execute_reply":"2025-06-05T12:38:06.060509Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"PredictionOutput(predictions=array([[[ 1.13471544e+00,  6.38880134e-01, -1.69897747e+00, ...,\n          1.86063623e+00,  3.46176744e-01, -1.66656399e+00],\n        [ 1.95383072e+00, -9.62811172e-01,  4.07076478e-01, ...,\n          8.42767239e+00, -1.56653976e+00,  1.61329672e-01],\n        [ 2.09611177e+00, -1.07191491e+00,  4.76606131e-01, ...,\n          8.38572407e+00, -1.67243266e+00,  2.24312007e-01],\n        ...,\n        [ 1.00073423e+01, -3.70441943e-01, -1.45898744e-01, ...,\n          1.79699197e-01, -1.46042418e+00, -1.08878112e+00],\n        [ 1.01190090e+01, -3.18227559e-01, -2.52026111e-01, ...,\n         -9.04206336e-02, -1.32528567e+00, -1.02481127e+00],\n        [ 9.38657188e+00,  1.00098252e+00, -2.83375293e-01, ...,\n         -8.11910868e-01, -1.22550249e+00, -2.01015973e+00]],\n\n       [[ 3.95751214e+00,  1.52460325e+00, -1.45915794e+00, ...,\n         -6.17582262e-01,  7.94928849e-01, -1.75260198e+00],\n        [ 3.96355271e+00, -1.77755272e+00,  7.45102644e-01, ...,\n          3.00777912e+00, -2.16947699e+00,  1.28593898e+00],\n        [ 8.68539047e+00, -1.74266458e+00,  5.55824518e-01, ...,\n         -3.10719665e-02, -2.24995327e+00, -3.81390572e-01],\n        ...,\n        [ 1.00840559e+01, -8.99470210e-01, -1.04798444e-01, ...,\n         -8.60209465e-01, -1.70533288e+00, -1.03756976e+00],\n        [ 1.03273344e+01, -7.92879581e-01, -9.51622576e-02, ...,\n         -8.79704237e-01, -1.55825663e+00, -1.01205564e+00],\n        [ 1.05036955e+01, -6.21042550e-01, -2.38176987e-01, ...,\n         -8.97487283e-01, -1.39308155e+00, -1.07156265e+00]],\n\n       [[ 8.57137680e-01, -1.27654254e+00, -1.29339600e+00, ...,\n         -9.40387845e-01, -6.75831214e-02, -1.06125653e+00],\n        [ 9.12933409e-01, -1.98103023e+00, -7.88227022e-01, ...,\n         -3.53754014e-01, -8.74764621e-01, -7.24986374e-01],\n        [ 7.95141459e-01, -1.88315690e+00, -5.61260879e-01, ...,\n         -1.32834896e-01, -1.10444081e+00, -3.55782568e-01],\n        ...,\n        [ 6.47550046e-01, -1.78543413e+00, -6.34877205e-01, ...,\n         -3.04455876e-01, -1.09712529e+00, -3.17907631e-01],\n        [ 6.58914268e-01, -1.78584480e+00, -6.46344364e-01, ...,\n         -2.56306559e-01, -1.11483073e+00, -3.03238750e-01],\n        [ 8.15815687e-01, -1.74030638e+00, -6.85563207e-01, ...,\n         -3.43570769e-01, -1.04712677e+00, -3.91372710e-01]],\n\n       ...,\n\n       [[ 1.02848434e+01, -4.26685989e-01, -8.61209393e-01, ...,\n         -7.84126580e-01, -1.09927082e+00, -7.33010292e-01],\n        [ 1.03383656e+01, -6.66969419e-01, -8.89247298e-01, ...,\n         -5.87600470e-01, -1.09184885e+00, -5.91996312e-01],\n        [ 1.03895550e+01, -5.69645047e-01, -9.72952783e-01, ...,\n         -7.55066514e-01, -1.00992119e+00, -7.32876301e-01],\n        ...,\n        [ 8.14399529e+00,  2.37877280e-01, -1.89586210e+00, ...,\n         -1.61240602e+00, -6.74095094e-01, -1.94632828e+00],\n        [ 3.91001844e+00,  8.94704580e-01, -2.25066900e+00, ...,\n         -1.81851149e+00, -1.05608687e-01, -2.37577629e+00],\n        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n\n       [[ 1.07591219e+01, -3.65883797e-01, -6.19167566e-01, ...,\n         -9.16027367e-01, -5.06931424e-01, -4.31244165e-01],\n        [ 1.06840487e+01, -4.51564819e-01, -5.93882382e-01, ...,\n         -8.52292001e-01, -6.00641251e-01, -3.56163979e-01],\n        [ 1.06881676e+01, -4.54437286e-01, -6.65824950e-01, ...,\n         -8.26418638e-01, -6.18486404e-01, -4.50499952e-01],\n        ...,\n        [ 1.05372210e+01, -2.96653152e-01, -7.87105858e-01, ...,\n         -8.75173032e-01, -6.22620225e-01, -6.39160037e-01],\n        [ 1.05226307e+01, -2.73550779e-01, -8.18079114e-01, ...,\n         -9.04133737e-01, -6.17004395e-01, -7.13293552e-01],\n        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]],\n\n       [[ 8.04875088e+00, -3.13432515e-02,  2.65130550e-01, ...,\n         -7.81079769e-01, -1.05488670e+00, -4.49267328e-01],\n        [ 8.34219170e+00, -4.08477902e-01,  3.87797534e-01, ...,\n         -6.51252031e-01, -1.27010298e+00, -2.97922492e-01],\n        [ 5.08994436e+00,  1.10560961e-01, -4.06837523e-01, ...,\n         -8.12358081e-01,  1.85353160e-01, -9.12092686e-01],\n        ...,\n        [ 5.50747395e+00,  2.64497668e-01, -3.12741518e-01, ...,\n         -6.82850063e-01, -9.96357650e-02, -9.68377292e-01],\n        [ 5.35459423e+00, -9.19669092e-01,  4.47215468e-01, ...,\n         -4.25359517e-01, -1.31345510e+00,  1.23008728e-01],\n        [-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, ...,\n         -1.00000000e+02, -1.00000000e+02, -1.00000000e+02]]],\n      dtype=float32), label_ids=array([[  17,   18,   18, ..., -100, -100, -100],\n       [  17,   18,    0, ..., -100, -100, -100],\n       [  11,   12,   12, ..., -100, -100, -100],\n       ...,\n       [   0,    0,    0, ..., -100, -100, -100],\n       [   0,    0,    0, ..., -100, -100, -100],\n       [   0,    0,    0, ..., -100, -100, -100]]), metrics={'test_loss': 0.3214300274848938, 'test_runtime': 2.7331, 'test_samples_per_second': 491.376, 'test_steps_per_second': 7.683})"},"metadata":{}}],"execution_count":97},{"cell_type":"code","source":"sample_index =0\nprint(first_proc_ds['validation'][sample_index]['text'])\nprint(enc_label_map['validation'][sample_index]['labels'])\nprint(result.label_ids[sample_index])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:38:10.501471Z","iopub.execute_input":"2025-06-05T12:38:10.501712Z","iopub.status.idle":"2025-06-05T12:38:10.508689Z","shell.execute_reply.started":"2025-06-05T12:38:10.501696Z","shell.execute_reply":"2025-06-05T12:38:10.507978Z"}},"outputs":[{"name":"stdout","text":"彭小军认为，国内银行现在走的是台湾的发卡模式，先通过跑马圈地再在圈的地里面选择客户，\ntensor([17, 18, 18,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 13, 14,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0])\n[  17   18   18    0    0    0    0    0    0    0    0    0    0    0\n    0   13   14    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n -100 -100 -100 -100 -100 -100 -100 -100]\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"[(p,l) for p,l in zip(result.label_ids[10],enc_label_map['validation'][10]['labels'])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:25:58.318456Z","iopub.execute_input":"2025-06-05T12:25:58.318701Z","iopub.status.idle":"2025-06-05T12:26:00.203052Z","shell.execute_reply.started":"2025-06-05T12:25:58.318669Z","shell.execute_reply":"2025-06-05T12:26:00.202256Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"[(0, tensor(0)),\n (0, tensor(0)),\n (5, tensor(5)),\n (6, tensor(6)),\n (6, tensor(6)),\n (6, tensor(6)),\n (6, tensor(6)),\n (6, tensor(6)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (7, tensor(7)),\n (8, tensor(8)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0)),\n (0, tensor(0))]"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"# 示例：原始文本数据\nraw_data = {\"text\": [\"腾讯和阿里一起研发AI\"]}\ntest_dataset = Dataset.from_dict(raw_data)\n\n# 使用训练时的 tokenizer 处理数据\ntokenized_test = test_dataset.map(\n    lambda x: tokenizer(x[\"text\"], truncation=True, padding=True),\n    batched=True\n)\n\n# 对数据集批量预测\nresult = trainer.predict(tokenized_test)\npredicted_classes = np.argmax(result.predictions, axis=-1)\n\npredicted_labels = [[tags[idx] for idx in sample] for sample in predicted_classes]\nentitys = []\nfor i, (text, labels) in enumerate(zip(raw_data[\"text\"], predicted_labels)):\n    print(labels)\n    for token,label in zip(list(text),labels):\n        pass\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T12:59:37.305068Z","iopub.execute_input":"2025-06-05T12:59:37.305346Z","iopub.status.idle":"2025-06-05T12:59:37.361132Z","shell.execute_reply.started":"2025-06-05T12:59:37.305325Z","shell.execute_reply":"2025-06-05T12:59:37.360573Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a604666559474710926051544da37b73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"['0', 'B-COMPANY', 'I-COMPANY', '0', 'B-COMPANY', 'I-COMPANY', '0', '0', '0', '0', '0', '0']\n","output_type":"stream"}],"execution_count":119}]}