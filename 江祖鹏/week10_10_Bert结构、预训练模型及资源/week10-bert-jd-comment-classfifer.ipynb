{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":318737,"sourceType":"datasetVersion","datasetId":134082},{"sourceId":11909529,"sourceType":"datasetVersion","datasetId":7487054},{"sourceId":11918105,"sourceType":"datasetVersion","datasetId":7492444}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:27.411911Z","iopub.execute_input":"2025-05-23T14:21:27.412156Z","iopub.status.idle":"2025-05-23T14:21:27.704129Z","shell.execute_reply.started":"2025-05-23T14:21:27.412137Z","shell.execute_reply":"2025-05-23T14:21:27.703406Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx\n/kaggle/input/jd-data/data.bin\n/kaggle/input/stopword/stopwords.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !cp /kaggle/input/jd-data/data.bin /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:27.705440Z","iopub.execute_input":"2025-05-23T14:21:27.705816Z","iopub.status.idle":"2025-05-23T14:21:27.709437Z","shell.execute_reply.started":"2025-05-23T14:21:27.705798Z","shell.execute_reply":"2025-05-23T14:21:27.708530Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport torch \nimport re\n\n#导入停用词\nstopwords_path = '/kaggle/input/stopword/stopwords.txt'\nwith open(stopwords_path, 'r', encoding='utf-8' ) as f:\n    stopwords = [line.strip() for line in f.readlines()]\n    \n# 构建正则表达式模式（匹配停用词边界）\nstopwords_pattern = r'\\b(' + '|'.join(re.escape(word) for word in stopwords) + r')\\b'\n\n#不需要分词，后续交给tokennizer处理\ndef remove_stopwords(text):\n    # 使用正则表达式替换停用词为空字符串\n    cleaned_text = re.sub(stopwords_pattern, '', str(text))\n    # 合并多余空格\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n    return cleaned_text\n\n#读取\ncomments =[]\nscores = []\ndf = pd.read_excel('/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx', )\nfor index, row in df.iterrows():\n    score_value = int(row['评分（总分5分）(score)'])\n    comment_text = row['评价内容(content)']   \n    # clear_stopwords = [comments for comments in comment_text if comments not in stopwords]\n    \n    # 清洗停用词（保留完整文本）\n    cleaned_text = remove_stopwords(comment_text)\n    comments.append(cleaned_text)\n    scores.append(score_value)\n\nprint(len(comments))\nprint(len(scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:27.710251Z","iopub.execute_input":"2025-05-23T14:21:27.710500Z","iopub.status.idle":"2025-05-23T14:21:45.289894Z","shell.execute_reply.started":"2025-05-23T14:21:27.710479Z","shell.execute_reply":"2025-05-23T14:21:45.289191Z"}},"outputs":[{"name":"stdout","text":"71818\n71818\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(comments[:30])\nprint(scores[:30])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:45.291599Z","iopub.execute_input":"2025-05-23T14:21:45.291935Z","iopub.status.idle":"2025-05-23T14:21:45.296315Z","shell.execute_reply.started":"2025-05-23T14:21:45.291916Z","shell.execute_reply":"2025-05-23T14:21:45.295662Z"}},"outputs":[{"name":"stdout","text":"['此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '一般般一分钱一分货吧', '此用户未填写评价内容', '此用户未填写评价内容', '商品质量很好很满意配送速度快啊而且配送员态度也非常好。', '。。。', '此用户未填写评价内容', '此用户未填写评价内容', '刘慧敏提莫摸摸摸休息泽TCL退咯的一组婆婆破鼓规土局', '此用户未填写评价内容', '此用户未填写评价内容', '还好还好还好还好红红火火好很好好', '此用户未填写评价内容', '此用户未填写评价内容', '此用户未填写评价内容', '好好好好好好好好好好好好好很好']\n[5, 5, 3, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n\n# data_len = [c for c in comments]\n# plt.hist(data_len, bins=10)\n# plt.xticks(np.arange(0, max_len + 30, 30))\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:45.297018Z","iopub.execute_input":"2025-05-23T14:21:45.297338Z","iopub.status.idle":"2025-05-23T14:21:45.318873Z","shell.execute_reply.started":"2025-05-23T14:21:45.297318Z","shell.execute_reply":"2025-05-23T14:21:45.318274Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# #赛选无关标点符号格式控制符号\n# comments = [cmt.replace('。', ' ').replace('，',' ')  for cmt in comments]\n\n# print(len(comments))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:45.319506Z","iopub.execute_input":"2025-05-23T14:21:45.319706Z","iopub.status.idle":"2025-05-23T14:21:45.333616Z","shell.execute_reply.started":"2025-05-23T14:21:45.319679Z","shell.execute_reply":"2025-05-23T14:21:45.332900Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#保存字典\nimport pickle\nwith open(\"data.bin\", 'wb') as f:\n    pickle.dump((comments,scores), f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:45.334343Z","iopub.execute_input":"2025-05-23T14:21:45.334595Z","iopub.status.idle":"2025-05-23T14:21:45.365057Z","shell.execute_reply.started":"2025-05-23T14:21:45.334571Z","shell.execute_reply":"2025-05-23T14:21:45.364578Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pickle\n#加载字典\nwith open('data.bin','rb') as f:\n    comments, labels = pickle.load(f)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:45.365696Z","iopub.execute_input":"2025-05-23T14:21:45.365955Z","iopub.status.idle":"2025-05-23T14:21:45.390006Z","shell.execute_reply.started":"2025-05-23T14:21:45.365922Z","shell.execute_reply":"2025-05-23T14:21:45.389540Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#加载Google的bert中文模型\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=6)\n\n#加载预训练模型配置参数\n# from transformers import AutoConfig\n# config = AutoConfig.from_pretrained('google-bert/bert-base-chinese')\n\n#加载tokenizer\nfrom transformers import AutoTokenizer\n# 加载词典创建分词器\ntokenizer=  AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n\n#冻结bert参数\n# model.bert.trainable=False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:21:45.390688Z","iopub.execute_input":"2025-05-23T14:21:45.390915Z","iopub.status.idle":"2025-05-23T14:22:11.851560Z","shell.execute_reply.started":"2025-05-23T14:21:45.390895Z","shell.execute_reply":"2025-05-23T14:22:11.850645Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fd0b0f2db014a56b33db821010ae879"}},"metadata":{}},{"name":"stderr","text":"2025-05-23 14:21:55.329992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748010115.530279      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748010115.601694      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ca769418c540e0858c0b94e3698240"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"234b0e3c70f94ea382ae455cc3d4836a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291334e41f424510b74a158018088eb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7e51672afa742749f4dbf9531836597"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def build_collate(tokenizer):\n    def collate_fn(batch):\n        comments = [item[0] for item in batch]\n        labels = [item[1] for item in batch]\n\n        #tokenizer转换,truncation = True,超过模型指定最大长度token序列，裁剪\n        model_inputs = tokenizer(comments,return_tensors='pt',padding=True,truncation=True,max_length=256)\n\n        input_ids = model_inputs['input_ids']\n        attention_mask = model_inputs['attention_mask']\n        label = torch.tensor(labels)\n        return input_ids, attention_mask, label\n    return collate_fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:22:11.854411Z","iopub.execute_input":"2025-05-23T14:22:11.855026Z","iopub.status.idle":"2025-05-23T14:22:11.862082Z","shell.execute_reply.started":"2025-05-23T14:22:11.855006Z","shell.execute_reply":"2025-05-23T14:22:11.860642Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import  SummaryWriter\n\nwriter = SummaryWriter()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:22:11.863202Z","iopub.execute_input":"2025-05-23T14:22:11.863604Z","iopub.status.idle":"2025-05-23T14:22:12.220911Z","shell.execute_reply.started":"2025-05-23T14:22:11.863584Z","shell.execute_reply":"2025-05-23T14:22:12.220323Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset = list(zip(comments, labels))\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=build_collate(tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:22:12.221583Z","iopub.execute_input":"2025-05-23T14:22:12.221781Z","iopub.status.idle":"2025-05-23T14:22:12.234307Z","shell.execute_reply.started":"2025-05-23T14:22:12.221765Z","shell.execute_reply":"2025-05-23T14:22:12.233681Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#构建模型，把模型注册到cuda\nmodel.to(device)\n\n#损失函数\ncriterion = nn.CrossEntropyLoss()\n\n#优化器\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\n\n\n#训练模型\ntrain_loss_cnt = 0\nmodel.train()\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    total_loss = 0\n    tpbar = tqdm(dataloader)\n    for input_ids, attention_mask, label in tpbar:\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        label = label.to(device)\n\n        #前向传播\n        out = model(input_ids,attention_mask)\n        #计算损失\n        loss = criterion(out.logits,label)\n\n        #反向传播\n        optimizer.zero_grad()\n        loss.backward()\n\n        #梯度裁剪\n        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n\n        #更新参数\n        optimizer.step()\n        \n        # 释放显存\n        del input_ids, attention_mask, label, out\n        torch.cuda.empty_cache()\n        \n        total_loss += loss.item()\n        \n        # if (i+1) % 200 == 0:\n        tpbar.set_description(f'Epoch[{epoch+1}/{num_epochs}],loss:{loss.item():.4f}')\n        #记录tensorboard跟踪记录\n        writer.add_scalar('train_loss',loss.item(),train_loss_cnt)\n        train_loss_cnt += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T14:22:12.235252Z","iopub.execute_input":"2025-05-23T14:22:12.235458Z","iopub.status.idle":"2025-05-23T15:57:22.509231Z","shell.execute_reply.started":"2025-05-23T14:22:12.235443Z","shell.execute_reply":"2025-05-23T15:57:22.508619Z"}},"outputs":[{"name":"stderr","text":"Epoch[1/5],loss:0.0512: 100%|██████████| 1123/1123 [18:58<00:00,  1.01s/it]\nEpoch[2/5],loss:0.0889: 100%|██████████| 1123/1123 [18:56<00:00,  1.01s/it]\nEpoch[3/5],loss:0.0098: 100%|██████████| 1123/1123 [19:03<00:00,  1.02s/it]\nEpoch[4/5],loss:0.0210: 100%|██████████| 1123/1123 [19:17<00:00,  1.03s/it]\nEpoch[5/5],loss:0.0180: 100%|██████████| 1123/1123 [18:54<00:00,  1.01s/it]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"torch.save(model.state_dict(),'jd_model.bin')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T15:57:22.510070Z","iopub.execute_input":"2025-05-23T15:57:22.510502Z","iopub.status.idle":"2025-05-23T15:57:23.079135Z","shell.execute_reply.started":"2025-05-23T15:57:22.510483Z","shell.execute_reply":"2025-05-23T15:57:23.078599Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#test\n\n#加载字典\nimport pickle\nwith open('data.bin','rb') as f:\n    comments, labels = pickle.load(f)\n\n#测试数据\ntext = ['非常差劲',\n      '买回来非常好用，但是又有点难用，说好用，又不好用，说不好用吧，又好用',\n      '牛逼666非常垃圾',\n      '这一坨真是给我来了个惊喜']\nmodel_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n\n#加载模型\nmodel.eval()\n# model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=6)\nmodel.load_state_dict(torch.load('jd_model.bin'))\nmodel.to(device)\n\n#模型推理\nwith torch.no_grad():\n    predict = model(**model_input)\n\npred = torch.argmax(predict.logits,dim=1).tolist()  # 转换为列表\nprint(f'预测结果：{pred}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T15:57:23.080049Z","iopub.execute_input":"2025-05-23T15:57:23.080321Z","iopub.status.idle":"2025-05-23T15:57:23.401501Z","shell.execute_reply.started":"2025-05-23T15:57:23.080300Z","shell.execute_reply":"2025-05-23T15:57:23.400685Z"}},"outputs":[{"name":"stdout","text":"预测结果：[5, 5, 5, 5]\n","output_type":"stream"}],"execution_count":15}]}