{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2726695,"sourceType":"datasetVersion","datasetId":1661983}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:34.495855Z","iopub.execute_input":"2025-05-05T03:35:34.496426Z","iopub.status.idle":"2025-05-05T03:35:34.508488Z","shell.execute_reply.started":"2025-05-05T03:35:34.496402Z","shell.execute_reply":"2025-05-05T03:35:34.507837Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/chinese-couplets/couplet/vocabs\n/kaggle/input/chinese-couplets/couplet/test/out.txt\n/kaggle/input/chinese-couplets/couplet/test/in.txt\n/kaggle/input/chinese-couplets/couplet/test/.in.txt.swp\n/kaggle/input/chinese-couplets/couplet/test/.out.txt.swp\n/kaggle/input/chinese-couplets/couplet/train/out.txt\n/kaggle/input/chinese-couplets/couplet/train/in.txt\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import tensorboard\nprint(\"TensorBoard 版本:\", tensorboard.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:34.509911Z","iopub.execute_input":"2025-05-05T03:35:34.510429Z","iopub.status.idle":"2025-05-05T03:35:34.522502Z","shell.execute_reply.started":"2025-05-05T03:35:34.510409Z","shell.execute_reply":"2025-05-05T03:35:34.521806Z"}},"outputs":[{"name":"stdout","text":"TensorBoard 版本: 2.18.0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"#### 带有attention的seq2seq模型","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:34.523137Z","iopub.execute_input":"2025-05-05T03:35:34.523402Z","iopub.status.idle":"2025-05-05T03:35:34.535931Z","shell.execute_reply.started":"2025-05-05T03:35:34.523384Z","shell.execute_reply":"2025-05-05T03:35:34.535189Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 编码器\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, dropout, hidden_form = 'concat'):\n        super(Encoder, self).__init__()\n        # 定义嵌入层\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        # 定义LSTM层\n        self.rnn = nn.LSTM(emb_dim, hidden_dim, dropout=dropout, num_layers=2,\n                          batch_first=True, bidirectional=True)\n        self.hidden_form = hidden_form\n\n    def forward(self, token_seq):\n        # token_seq: [batch_size, seq_len]\n        # embedded: [batch_size, seq_len, emb_dim]\n        embedded = self.embedding(token_seq)\n        # outputs: [batch_size, seq_len, hidden_dim * 2]\n        # hidden: [4, batch_size, hidden_dim]\n        outputs, (h_n, c_n)  = self.rnn(embedded)\n        if (self.hidden_form == 'concat'):\n            hidden_concat = torch.cat([h_n[0], h_n[1]], dim=1)\n            hidden_concat = hidden_concat.unsqueeze(0).repeat(2, 1, 1)\n            return hidden_concat, outputs\n        elif (self.hidden_form == 'add'):\n            hidden_sum = h_n.sum(dim=0)\n            hidden_sum = hidden_sum.unsqueeze(0).repeat(2, 1, 1)\n            return hidden_sum, outputs\n\n# 测试\nhidden_form1 = 'concat'\n# hidden_form1 = 'add'\n\ninput_dim1 = 200\nemb_dim1 = 100\nhidden_dim1 = 256\ndropout1 = 0.5\nbatch_size1 = 4\nseq_len1 = 10\n\nencoder1 = Encoder(input_dim1, emb_dim1, hidden_dim1, dropout1, hidden_form = hidden_form1)\ntoken_seq1 = torch.randint(0, input_dim1, (batch_size1, seq_len1))\nprint(f'token_seq1.shape : {token_seq1.shape}')\nhidden_state1, outputs1 = encoder1(token_seq1)\nprint(f'hidden_state1.shape : {hidden_state1.shape}')\nprint(f'outputs1.shape : {outputs1.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:34.537555Z","iopub.execute_input":"2025-05-05T03:35:34.537750Z","iopub.status.idle":"2025-05-05T03:35:34.587578Z","shell.execute_reply.started":"2025-05-05T03:35:34.537736Z","shell.execute_reply":"2025-05-05T03:35:34.586838Z"}},"outputs":[{"name":"stdout","text":"token_seq1.shape : torch.Size([4, 10])\nhidden_state1.shape : torch.Size([2, 4, 512])\noutputs1.shape : torch.Size([4, 10, 512])\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# attention\nclass Attention(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, enc_output, dec_output):\n        # a_t = h_t @ h_s\n        a_t = torch.bmm(enc_output, dec_output.permute(0, 2, 1))\n        # 1.计算 结合解码token和编码token，关联的权重\n        a_t = torch.softmax(a_t, dim=1)\n        # 2.计算 关联权重和编码token 贡献值\n        c_t = torch.bmm(a_t.permute(0, 2, 1), enc_output)\n        return c_t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:34.588252Z","iopub.execute_input":"2025-05-05T03:35:34.588463Z","iopub.status.idle":"2025-05-05T03:35:34.593090Z","shell.execute_reply.started":"2025-05-05T03:35:34.588447Z","shell.execute_reply":"2025-05-05T03:35:34.592313Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 解码器\nclass Decoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, dropout, hidden_form = 'concat'):\n        super(Decoder, self).__init__()\n        # 定义嵌入层\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        # 定义LSTM层\n        self.rnn = nn.LSTM(emb_dim, hidden_dim * 2, dropout=dropout,\n                          num_layers=2,batch_first=True)\n        # 定义线性层\n        self.fc = nn.Linear(hidden_dim * 2, input_dim)  # 解码词典中词汇概率\n        # attention层\n        self.atteniton = Attention()\n        # attention结果转换线性层\n        self.atteniton_fc = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n        self.hidden_form = hidden_form\n\n    def forward(self, token_seq, hidden_state, enc_output):\n        # token_seq: [batch_size, seq_len]\n        # embedded: [batch_size, seq_len, emb_dim]\n        embedded = self.embedding(token_seq)\n        # outputs: [batch_size, seq_len, hidden_dim * 2]\n        # hidden: [2, batch_size, hidden_dim * 2]\n        if (self.hidden_form == 'add'):\n            hidden_state = hidden_state.repeat(1, 1, 2)\n        dec_output, (h_n, c_n) = self.rnn(embedded, (hidden_state, torch.zeros_like(hidden_state)))\n        \n        # attention运算\n        c_t = self.atteniton(enc_output, dec_output)\n        # [attention, dec_output]\n        cat_output = torch.cat((c_t, dec_output), dim=-1)\n        # 线性运算\n        out = torch.tanh(self.atteniton_fc(cat_output))\n\n        # out: [batch_size, seq_len, hidden_dim * 2]\n        logits = self.fc(out)\n        return logits, h_n\n\n# 测试\ninput_dim2 = 200\nemb_dim2 = 100\nhidden_dim2 = 256\ndropout2 = 0.5\nbatch_size2 = 4\nseq_len2 = 10\n\ndecoder2 = Decoder(input_dim2, emb_dim2, hidden_dim2, dropout2, hidden_form = hidden_form1)\ntoken_seq2 = torch.randint(0, input_dim2, (batch_size2, seq_len2))\nprint(f'token_seq2.shape : {token_seq2.shape}')\nlogits2, hidden_state2= decoder2(token_seq2, hidden_state1, outputs1)\nprint(f'logits2.shape : {logits2.shape}')\nprint(f'hidden_state2.shape : {hidden_state2.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:34.593870Z","iopub.execute_input":"2025-05-05T03:35:34.594078Z","iopub.status.idle":"2025-05-05T03:35:34.671931Z","shell.execute_reply.started":"2025-05-05T03:35:34.594063Z","shell.execute_reply":"2025-05-05T03:35:34.671171Z"}},"outputs":[{"name":"stdout","text":"token_seq2.shape : torch.Size([4, 10])\nlogits2.shape : torch.Size([4, 10, 200])\nhidden_state2.shape : torch.Size([2, 4, 512])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n\n    def __init__(self,\n                 enc_emb_size,\n                 dec_emb_size,\n                 emb_dim,\n                 hidden_size,\n                 dropout=0.5,\n                 hidden_form = 'concat'\n                 ):\n\n        super().__init__()\n        # encoder\n        self.encoder = Encoder(enc_emb_size, emb_dim, hidden_size, dropout=dropout, hidden_form = hidden_form)\n        # decoder\n        self.decoder = Decoder(dec_emb_size, emb_dim, hidden_size, dropout=dropout, hidden_form = hidden_form)\n\n    def forward(self, enc_input, dec_input):\n        # encoder last hidden state\n        encoder_state, outputs = self.encoder(enc_input)\n        output,hidden = self.decoder(dec_input, encoder_state, outputs)\n\n        return output,hidden\n\n# 测试\nseq2seq = Seq2Seq(\n        enc_emb_size=input_dim2,\n        dec_emb_size=input_dim2,\n        emb_dim=emb_dim2,\n        hidden_size=hidden_dim2,\n        dropout=dropout2,\n        hidden_form = hidden_form1\n)\n\nlogits,_ = seq2seq(\n    enc_input=torch.randint(0, input_dim1, (batch_size1, seq_len1)),\n    dec_input=torch.randint(0, input_dim2, (batch_size2, seq_len2))\n)\nprint(logits.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:34.673493Z","iopub.execute_input":"2025-05-05T03:35:34.673744Z","iopub.status.idle":"2025-05-05T03:35:34.747982Z","shell.execute_reply.started":"2025-05-05T03:35:34.673726Z","shell.execute_reply":"2025-05-05T03:35:34.747184Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 10, 200])\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"#### 数据处理","metadata":{}},{"cell_type":"code","source":"def read_data(enc_data_file, dec_data_file):\n    \"\"\"\n    读取训练数据返回数据集合\n    \"\"\"\n    enc_data,dec_data = [],[]\n\n    with open(enc_data_file) as f1:\n        # 读取记录行\n        lines = f1.read().split('\\n')\n        for line in lines:\n            if line == ' ':\n                continue\n            enc_tks = line.split()\n            enc_data.append(enc_tks)\n\n    with open(dec_data_file) as f2:\n        # 读取记录行\n        lines = f2.read().split('\\n')\n        for line in lines:\n            if line == ' ':\n                continue\n            dec_tks = line.split()\n            dec_tks = ['BOS'] + dec_tks + ['EOS']\n            dec_data.append(dec_tks)\n\n    # 断言\n    assert len(enc_data) == len(dec_data), '编码数据与解码数据长度不一致！'\n\n    return enc_data, dec_data\n\n# 测试\nenc_data_file1 = '/kaggle/input/chinese-couplets/couplet/train/in.txt'\ndec_data_file1 = '/kaggle/input/chinese-couplets/couplet/train/out.txt'\n\nenc_data_file2 = '/kaggle/input/chinese-couplets/couplet/test/in.txt'\ndec_data_file2 = '/kaggle/input/chinese-couplets/couplet/test/out.txt'\n\nenc_data1, dec_data1 = read_data(enc_data_file1, dec_data_file1)\nprint(len(enc_data1))\nprint(len(dec_data1))\nprint(enc_data1[:5])\nprint(dec_data1[:5])\nprint()\n\nenc_data2, dec_data2 = read_data(enc_data_file2, dec_data_file2)\nprint(len(enc_data2))\nprint(len(dec_data2))\nprint(enc_data2[:5])\nprint(dec_data2[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:34.748867Z","iopub.execute_input":"2025-05-05T03:35:34.749121Z","iopub.status.idle":"2025-05-05T03:35:39.335436Z","shell.execute_reply.started":"2025-05-05T03:35:34.749093Z","shell.execute_reply":"2025-05-05T03:35:39.334693Z"}},"outputs":[{"name":"stdout","text":"770492\n770492\n[['晚', '风', '摇', '树', '树', '还', '挺'], ['愿', '景', '天', '成', '无', '墨', '迹'], ['丹', '枫', '江', '冷', '人', '初', '去'], ['忽', '忽', '几', '晨', '昏', '，', '离', '别', '间', '之', '，', '疾', '病', '间', '之', '，', '不', '及', '终', '年', '同', '静', '好'], ['闲', '来', '野', '钓', '人', '稀', '处']]\n[['BOS', '晨', '露', '润', '花', '花', '更', '红', 'EOS'], ['BOS', '万', '方', '乐', '奏', '有', '于', '阗', 'EOS'], ['BOS', '绿', '柳', '堤', '新', '燕', '复', '来', 'EOS'], ['BOS', '茕', '茕', '小', '儿', '女', '，', '孱', '羸', '若', '此', '，', '娇', '憨', '若', '此', '，', '更', '烦', '二', '老', '费', '精', '神', 'EOS'], ['BOS', '兴', '起', '高', '歌', '酒', '醉', '中', 'EOS']]\n\n4001\n4001\n[['腾', '飞', '上', '铁', '，', '锐', '意', '改', '革', '谋', '发', '展', '，', '勇', '当', '千', '里', '马'], ['风', '弦', '未', '拨', '心', '先', '乱'], ['花', '梦', '粘', '于', '春', '袖', '口'], ['晋', '世', '文', '章', '昌', '二', '陆'], ['一', '句', '相', '思', '吟', '岁', '月']]\n[['BOS', '和', '谐', '南', '供', '，', '安', '全', '送', '电', '保', '畅', '通', '，', '争', '做', '领', '头', '羊', 'EOS'], ['BOS', '夜', '幕', '已', '沉', '梦', '更', '闲', 'EOS'], ['BOS', '莺', '声', '溅', '落', '柳', '枝', '头', 'EOS'], ['BOS', '魏', '家', '词', '赋', '重', '三', '曹', 'EOS'], ['BOS', '千', '杯', '美', '酒', '醉', '风', '情', 'EOS']]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def words_to_vocab(words_list):\n        no_repeat_tokens = set()\n\n        for word in words_list:\n            no_repeat_tokens.update(list(word))  \n\n        tokens = ['PAD','UNK'] + list(no_repeat_tokens)\n\n        vocab = { tk:i for i, tk in enumerate(tokens)}\n\n        return vocab\n\n# 测试\nimport random\n\nenc_vocab1 = words_to_vocab(enc_data1)\ndec_vocab1 = words_to_vocab(dec_data1)\n\nenc_vocab2 = words_to_vocab(enc_data2)\ndec_vocab2 = words_to_vocab(dec_data2)\n\nenc_keys1 = random.sample(list(enc_vocab1.keys()), 5)\nrandom_enc_elements1 = {key: enc_vocab1[key] for key in enc_keys1}\nprint(f'random_enc_elements1:\\n{random_enc_elements1}\\n')\n\ndec_keys1 = random.sample(list(dec_vocab1.keys()), 5)\nrandom_dec_elements1 = {key: dec_vocab1[key] for key in dec_keys1}\nprint(f'random_dec_elements1:\\n{random_dec_elements1}\\n')\n\nenc_keys2 = random.sample(list(enc_vocab2.keys()), 5)\nrandom_enc_elements2 = {key: enc_vocab2[key] for key in enc_keys2}\nprint(f'random_enc_elements2:\\n{random_enc_elements2}\\n')\n\ndec_keys2 = random.sample(list(dec_vocab2.keys()), 5)\nrandom_dec_elements2 = {key: dec_vocab2[key] for key in dec_keys2}\nprint(f'random_dec_elements2:\\n{random_dec_elements2}\\n')\n\nimport pickle\nwith open('vocab1.bin','wb') as f:\n    pickle.dump((enc_vocab1, dec_vocab1),f)\nwith open('vocab2.bin','wb') as f:\n    pickle.dump((enc_vocab2, dec_vocab2),f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:39.336127Z","iopub.execute_input":"2025-05-05T03:35:39.336344Z","iopub.status.idle":"2025-05-05T03:35:40.399103Z","shell.execute_reply.started":"2025-05-05T03:35:39.336318Z","shell.execute_reply":"2025-05-05T03:35:40.398420Z"}},"outputs":[{"name":"stdout","text":"random_enc_elements1:\n{'崕': 731, '概': 1919, '軽': 6464, '跟': 6885, '窿': 7102}\n\nrandom_dec_elements1:\n{'臻': 3176, '陵': 3200, '胎': 3144, '吼': 2850, '妟': 6682}\n\nrandom_enc_elements2:\n{'德': 1346, '耘': 2981, '毫': 995, '软': 497, '渰': 2891}\n\nrandom_dec_elements2:\n{'藉': 2878, '辞': 923, '忑': 1867, '葩': 2152, '念': 2227}\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def get_proc(enc_voc, dec_voc):\n\n    # 嵌套函数定义\n    # 外部函数变量生命周期会延续到内部函数调用结束 （闭包）\n\n    def batch_proc(data):\n        \"\"\"\n        批次数据处理并返回\n        \"\"\"\n        enc_ids, dec_ids, labels = [],[],[]\n        for enc,dec in data:\n            # token -> token index\n            enc_idx = [enc_voc[tk] for tk in enc]\n            dec_idx = [dec_voc[tk] for tk in dec]\n\n            # encoder_input\n            enc_ids.append(torch.tensor(enc_idx))\n            # decoder_input\n            dec_ids.append(torch.tensor(dec_idx[:-1]))\n            # label\n            labels.append(torch.tensor(dec_idx[1:]))\n\n        # 数据转换张量 [batch, max_token_len]\n        # 用批次中最长token序列构建张量\n        enc_input = pad_sequence(enc_ids, batch_first=True)\n        dec_input = pad_sequence(dec_ids, batch_first=True)\n        targets = pad_sequence(labels, batch_first=True)\n\n        # 返回数据都是模型训练和推理的需要\n        return enc_input, dec_input, targets\n\n    # 返回回调函数\n    return batch_proc\n\n# 测试\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nenc_input1, dec_input1, targets1 = [],[],[]\n\ndataset1 = list(zip(enc_data1, dec_data1))\n\ndataloader1 = DataLoader(\n        dataset1,\n        batch_size=2,\n        shuffle=True,\n        collate_fn=get_proc(enc_vocab1, dec_vocab1)\n)\n\nfor enc_input1, dec_input1, targets1 in dataloader1:\n    print(f'enc_input1.shape : {enc_input1.shape}')\n    print(f'dec_input1.shape : {dec_input1.shape}')\n    print(f'targets1.shape :   {targets1.shape}')\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T03:35:40.400762Z","iopub.execute_input":"2025-05-05T03:35:40.401193Z","iopub.status.idle":"2025-05-05T03:35:41.761393Z","shell.execute_reply.started":"2025-05-05T03:35:40.401176Z","shell.execute_reply":"2025-05-05T03:35:41.760695Z"}},"outputs":[{"name":"stdout","text":"enc_input1.shape : torch.Size([2, 19])\ndec_input1.shape : torch.Size([2, 20])\ntargets1.shape :   torch.Size([2, 20])\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"#### 模型训练","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\ntrain_loss_cnt = 0\n\ndataloader2 = DataLoader(\n        dataset1,\n        batch_size=512,\n        shuffle=True,\n        collate_fn=get_proc(enc_vocab1, dec_vocab1)\n)\n\nmodel = Seq2Seq(\n        enc_emb_size=len(enc_vocab1),\n        dec_emb_size=len(dec_vocab1),\n        emb_dim=200,\n        hidden_size=256,\n        dropout=0.4,\n        hidden_form = hidden_form1\n    )\nmodel.to(device)\n\n# 优化器、损失\noptimizer = optim.AdamW(model.parameters(), weight_decay=0.01, lr=1e-3)\n# optimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# 训练\nfor epoch in range(10):\n    model.train()\n    tpbar = tqdm(dataloader2)\n    for enc_input, dec_input, targets in tpbar:\n        enc_input = enc_input.to(device)\n        dec_input = dec_input.to(device)\n        targets = targets.to(device)\n\n        # 前向传播\n        logits, _ = model(enc_input, dec_input)\n\n        # 计算损失\n        # CrossEntropyLoss需要将logits和targets展平\n        # logits: [batch_size, seq_len, vocab_size]\n        # targets: [batch_size, seq_len]\n        # 展平为 [batch_size * seq_len, vocab_size] 和 [batch_size * seq_len]\n        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tpbar.set_description(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n        writer.add_scalar('Loss/train', loss.item(), train_loss_cnt)\n        train_loss_cnt += 1\n\ntorch.save(model.state_dict(), 'seq2seq_state.bin')","metadata":{"execution":{"iopub.status.busy":"2025-05-05T03:36:18.520177Z","iopub.execute_input":"2025-05-05T03:36:18.520889Z","iopub.status.idle":"2025-05-05T04:24:48.913125Z","shell.execute_reply.started":"2025-05-05T03:36:18.520850Z","shell.execute_reply":"2025-05-05T04:24:48.912287Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Epoch 1, Loss: 1.6375: 100%|██████████| 1505/1505 [04:50<00:00,  5.19it/s]\nEpoch 2, Loss: 1.5420: 100%|██████████| 1505/1505 [04:49<00:00,  5.20it/s]\nEpoch 3, Loss: 1.2387: 100%|██████████| 1505/1505 [04:51<00:00,  5.17it/s]\nEpoch 4, Loss: 1.2604: 100%|██████████| 1505/1505 [04:52<00:00,  5.15it/s]\nEpoch 5, Loss: 1.2163: 100%|██████████| 1505/1505 [04:51<00:00,  5.17it/s]\nEpoch 6, Loss: 1.2318: 100%|██████████| 1505/1505 [04:50<00:00,  5.18it/s]\nEpoch 7, Loss: 1.1362: 100%|██████████| 1505/1505 [04:51<00:00,  5.16it/s]\nEpoch 8, Loss: 1.2142: 100%|██████████| 1505/1505 [04:50<00:00,  5.18it/s]\nEpoch 9, Loss: 1.1876: 100%|██████████| 1505/1505 [04:50<00:00,  5.18it/s]\nEpoch 10, Loss: 1.0951: 100%|██████████| 1505/1505 [04:51<00:00,  5.16it/s]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"#### seq2seq attention版的推理实现","metadata":{}},{"cell_type":"code","source":" # 创建解码器反向字典\ndvoc_inv = {v:k for k,v in dec_vocab1.items()}\n\n# 随机选取测试样本\nrnd_idx = random.randint(0, len(enc_data2))\nenc_input = enc_data2[rnd_idx]\ndec_output = dec_data2[rnd_idx]\n\nenc_idx = torch.tensor([[enc_vocab1[tk] for tk in enc_input]])\n\nprint(f'enc_idx: {enc_idx}')\nprint(f'enc_idx.shape: {enc_idx.shape}')\n\n# 最大解码长度\nmax_dec_len = len(dec_output)\nprint(f'max_dec_len: {max_dec_len}')\n\nprint('enc_input：',''.join(enc_input))\nprint(\"dec_output：\", ''.join(dec_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:25:59.012403Z","iopub.execute_input":"2025-05-05T04:25:59.012968Z","iopub.status.idle":"2025-05-05T04:25:59.026630Z","shell.execute_reply.started":"2025-05-05T04:25:59.012943Z","shell.execute_reply":"2025-05-05T04:25:59.026073Z"}},"outputs":[{"name":"stdout","text":"enc_idx: tensor([[ 302,   22,  550, 1501, 5016, 3517,  978]])\nenc_idx.shape: torch.Size([1, 7])\nmax_dec_len: 9\nenc_input： 咬文嚼字常添瘦\ndec_output： BOS忍气吞声每犯愁EOS\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# 推理\nenc_idx = enc_idx.to(device)\nmodel.eval()\nwith torch.no_grad():\n    # 编码器输出\n    hidden_state, enc_outputs = model.encoder(enc_idx)\n\n    # 解码器输入 shape [1,1]\n    dec_input = torch.tensor([[dec_vocab1['BOS']]])\n    dec_input = dec_input.to(device)\n\n    # 循环decoder\n    dec_tokens = []\n    while True:\n        if len(dec_tokens) >= max_dec_len:\n            break\n        logits, hidden_state = model.decoder(dec_input, hidden_state, enc_outputs)\n        next_token = torch.argmax(logits, dim=-1)\n        if dvoc_inv[next_token.squeeze().item()] == 'EOS':\n            break\n        dec_tokens.append(next_token.squeeze().item())\n        dec_input = next_token\n\nprint(\"dec_eval：\", ''.join([dvoc_inv[tk] for tk in dec_tokens]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:59:49.873404Z","iopub.execute_input":"2025-05-05T04:59:49.873966Z","iopub.status.idle":"2025-05-05T04:59:49.887679Z","shell.execute_reply.started":"2025-05-05T04:59:49.873944Z","shell.execute_reply":"2025-05-05T04:59:49.886866Z"}},"outputs":[{"name":"stdout","text":"dec_eval： 把酒临风不减肥梅自\n","output_type":"stream"}],"execution_count":39}]}