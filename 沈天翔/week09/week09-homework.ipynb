{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2726695,"sourceType":"datasetVersion","datasetId":1661983}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:12:55.072680Z","iopub.execute_input":"2025-05-15T14:12:55.073432Z","iopub.status.idle":"2025-05-15T14:12:55.098109Z","shell.execute_reply.started":"2025-05-15T14:12:55.073406Z","shell.execute_reply":"2025-05-15T14:12:55.097511Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/chinese-couplets/couplet/vocabs\n/kaggle/input/chinese-couplets/couplet/test/out.txt\n/kaggle/input/chinese-couplets/couplet/test/in.txt\n/kaggle/input/chinese-couplets/couplet/test/.in.txt.swp\n/kaggle/input/chinese-couplets/couplet/test/.out.txt.swp\n/kaggle/input/chinese-couplets/couplet/train/out.txt\n/kaggle/input/chinese-couplets/couplet/train/in.txt\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 数据处理","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:12:55.099256Z","iopub.execute_input":"2025-05-15T14:12:55.099875Z","iopub.status.idle":"2025-05-15T14:12:55.104243Z","shell.execute_reply.started":"2025-05-15T14:12:55.099855Z","shell.execute_reply":"2025-05-15T14:12:55.103420Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def read_data(enc_data_file, dec_data_file):\n    \"\"\"\n    读取训练数据返回数据集合\n    \"\"\"\n    enc_data,dec_data = [],[]\n\n    with open(enc_data_file) as f1:\n        # 读取记录行\n        lines = f1.read().split('\\n')\n        for line in lines:\n            if line == ' ':\n                continue\n            enc_tks = line.split()\n            enc_data.append(enc_tks)\n\n    with open(dec_data_file) as f2:\n        # 读取记录行\n        lines = f2.read().split('\\n')\n        for line in lines:\n            if line == ' ':\n                continue\n            dec_tks = line.split()\n            dec_tks = ['BOS'] + dec_tks + ['EOS']\n            dec_data.append(dec_tks)\n\n    # 断言\n    assert len(enc_data) == len(dec_data), '编码数据与解码数据长度不一致！'\n\n    return enc_data, dec_data\n\n# 测试\nenc_data_file1 = '/kaggle/input/chinese-couplets/couplet/train/in.txt'\ndec_data_file1 = '/kaggle/input/chinese-couplets/couplet/train/out.txt'\n\nenc_data_file2 = '/kaggle/input/chinese-couplets/couplet/test/in.txt'\ndec_data_file2 = '/kaggle/input/chinese-couplets/couplet/test/out.txt'\n\nenc_data1, dec_data1 = read_data(enc_data_file1, dec_data_file1)\nprint(len(enc_data1))\nprint(len(dec_data1))\nprint(enc_data1[:5])\nprint(dec_data1[:5])\nprint()\n\nenc_data2, dec_data2 = read_data(enc_data_file2, dec_data_file2)\nprint(len(enc_data2))\nprint(len(dec_data2))\nprint(enc_data2[:5])\nprint(dec_data2[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:12:55.105027Z","iopub.execute_input":"2025-05-15T14:12:55.105231Z","iopub.status.idle":"2025-05-15T14:12:58.878148Z","shell.execute_reply.started":"2025-05-15T14:12:55.105216Z","shell.execute_reply":"2025-05-15T14:12:58.877362Z"}},"outputs":[{"name":"stdout","text":"770492\n770492\n[['晚', '风', '摇', '树', '树', '还', '挺'], ['愿', '景', '天', '成', '无', '墨', '迹'], ['丹', '枫', '江', '冷', '人', '初', '去'], ['忽', '忽', '几', '晨', '昏', '，', '离', '别', '间', '之', '，', '疾', '病', '间', '之', '，', '不', '及', '终', '年', '同', '静', '好'], ['闲', '来', '野', '钓', '人', '稀', '处']]\n[['BOS', '晨', '露', '润', '花', '花', '更', '红', 'EOS'], ['BOS', '万', '方', '乐', '奏', '有', '于', '阗', 'EOS'], ['BOS', '绿', '柳', '堤', '新', '燕', '复', '来', 'EOS'], ['BOS', '茕', '茕', '小', '儿', '女', '，', '孱', '羸', '若', '此', '，', '娇', '憨', '若', '此', '，', '更', '烦', '二', '老', '费', '精', '神', 'EOS'], ['BOS', '兴', '起', '高', '歌', '酒', '醉', '中', 'EOS']]\n\n4001\n4001\n[['腾', '飞', '上', '铁', '，', '锐', '意', '改', '革', '谋', '发', '展', '，', '勇', '当', '千', '里', '马'], ['风', '弦', '未', '拨', '心', '先', '乱'], ['花', '梦', '粘', '于', '春', '袖', '口'], ['晋', '世', '文', '章', '昌', '二', '陆'], ['一', '句', '相', '思', '吟', '岁', '月']]\n[['BOS', '和', '谐', '南', '供', '，', '安', '全', '送', '电', '保', '畅', '通', '，', '争', '做', '领', '头', '羊', 'EOS'], ['BOS', '夜', '幕', '已', '沉', '梦', '更', '闲', 'EOS'], ['BOS', '莺', '声', '溅', '落', '柳', '枝', '头', 'EOS'], ['BOS', '魏', '家', '词', '赋', '重', '三', '曹', 'EOS'], ['BOS', '千', '杯', '美', '酒', '醉', '风', '情', 'EOS']]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def words_to_vocab(words_list):\n        no_repeat_tokens = set()\n\n        for word in words_list:\n            no_repeat_tokens.update(list(word))  \n\n        tokens = ['PAD','UNK'] + list(no_repeat_tokens)\n\n        vocab = { tk:i for i, tk in enumerate(tokens)}\n\n        return vocab\n\n# 测试\nimport random\n\nenc_vocab1 = words_to_vocab(enc_data1)\ndec_vocab1 = words_to_vocab(dec_data1)\n\nenc_vocab2 = words_to_vocab(enc_data2)\ndec_vocab2 = words_to_vocab(dec_data2)\n\nenc_keys1 = random.sample(list(enc_vocab1.keys()), 5)\nrandom_enc_elements1 = {key: enc_vocab1[key] for key in enc_keys1}\nprint(f'random_enc_elements1:\\n{random_enc_elements1}\\n')\n\ndec_keys1 = random.sample(list(dec_vocab1.keys()), 5)\nrandom_dec_elements1 = {key: dec_vocab1[key] for key in dec_keys1}\nprint(f'random_dec_elements1:\\n{random_dec_elements1}\\n')\n\nenc_keys2 = random.sample(list(enc_vocab2.keys()), 5)\nrandom_enc_elements2 = {key: enc_vocab2[key] for key in enc_keys2}\nprint(f'random_enc_elements2:\\n{random_enc_elements2}\\n')\n\ndec_keys2 = random.sample(list(dec_vocab2.keys()), 5)\nrandom_dec_elements2 = {key: dec_vocab2[key] for key in dec_keys2}\nprint(f'random_dec_elements2:\\n{random_dec_elements2}\\n')\n\nimport pickle\nwith open('vocab1.bin','wb') as f:\n    pickle.dump((enc_vocab1, dec_vocab1),f)\nwith open('vocab2.bin','wb') as f:\n    pickle.dump((enc_vocab2, dec_vocab2),f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:12:58.880127Z","iopub.execute_input":"2025-05-15T14:12:58.880384Z","iopub.status.idle":"2025-05-15T14:12:59.969185Z","shell.execute_reply.started":"2025-05-15T14:12:58.880367Z","shell.execute_reply":"2025-05-15T14:12:59.968428Z"}},"outputs":[{"name":"stdout","text":"random_enc_elements1:\n{'楝': 6521, '缭': 2234, '寅': 3760, '墅': 2527, '赉': 3902}\n\nrandom_dec_elements1:\n{'缈': 6551, '裏': 1733, '炆': 7502, '辘': 2013, '孮': 7035}\n\nrandom_enc_elements2:\n{'幼': 1423, '伙': 2760, '避': 1556, '燎': 2198, '笙': 1992}\n\nrandom_dec_elements2:\n{'窥': 719, '伤': 1392, '始': 1476, '绽': 1972, '诊': 1489}\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## transformer模型构建","metadata":{}},{"cell_type":"code","source":"# 位置编码矩阵\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, emb_size, dropout=0.1, maxlen=5000):\n        super().__init__()\n        # 行缩放指数值\n        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        # 位置编码索引 (5000,1)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        # 编码矩阵 (5000, emb_size)\n        pos_embdding = torch.zeros((maxlen, emb_size))\n        pos_embdding[:, 0::2] = torch.sin(pos * den)\n        pos_embdding[:, 1::2] = torch.cos(pos * den)\n        # 添加和batch对应维度 (1, 5000, emb_size)\n        pos_embdding = pos_embdding.unsqueeze(0)\n        # dropout\n        self.dropout = nn.Dropout(dropout)\n        # 注册当前矩阵不参与参数更新\n        self.register_buffer('pos_embedding', pos_embdding)\n\n    def forward(self, token_embdding):\n        token_len = token_embdding.size(1)  # token长度\n        # (1, token_len, emb_size)\n        add_emb = self.pos_embedding[:, :token_len, :] + token_embdding\n        return self.dropout(add_emb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:12:59.970035Z","iopub.execute_input":"2025-05-15T14:12:59.970268Z","iopub.status.idle":"2025-05-15T14:12:59.976102Z","shell.execute_reply.started":"2025-05-15T14:12:59.970251Z","shell.execute_reply":"2025-05-15T14:12:59.975473Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class Seq2SeqTransformer(nn.Module):\n\n    def __init__(self, d_model, nhead, num_enc_layers, num_dec_layers,\n                 dim_forward, dropout, enc_voc_size, dec_voc_size):\n        super().__init__()\n        # transformer\n        self.transformer = nn.Transformer(d_model=d_model,\n                                          nhead=nhead,\n                                          num_encoder_layers=num_enc_layers,\n                                          num_decoder_layers=num_dec_layers,\n                                          dim_feedforward=dim_forward,\n                                          dropout=dropout,\n                                          batch_first=True)\n        # encoder input embedding\n        self.enc_emb = nn.Embedding(enc_voc_size, d_model)\n        # decoder input embedding\n        self.dec_emb = nn.Embedding(dec_voc_size, d_model)\n        # predict generate linear\n        self.predict = nn.Linear(d_model, dec_voc_size)  # token预测基于解码器词典\n        # positional encoding\n        self.pos_encoding = PositionalEncoding(d_model, dropout)\n\n    def forward(self, enc_inp, dec_inp, tgt_mask, enc_pad_mask, dec_pad_mask):\n        # multi head attention之前基于位置编码embedding生成\n        enc_emb = self.pos_encoding(self.enc_emb(enc_inp))\n        dec_emb = self.pos_encoding(self.dec_emb(dec_inp))\n        # 调用transformer计算\n        outs = self.transformer(src=enc_emb, tgt=dec_emb, tgt_mask=tgt_mask,\n                         src_key_padding_mask=enc_pad_mask,\n                         tgt_key_padding_mask=dec_pad_mask)\n        # 推理\n        return self.predict(outs)\n\n    # 推理环节使用方法\n    def encode(self, enc_inp):\n        enc_emb = self.pos_encoding(self.enc_emb(enc_inp))\n        return self.transformer.encoder(enc_emb)\n\n    def decode(self, dec_inp, memory, dec_mask):\n        dec_emb = self.pos_encoding(self.dec_emb(dec_inp))\n        return self.transformer.decoder(dec_emb, memory, dec_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:12:59.976742Z","iopub.execute_input":"2025-05-15T14:12:59.976990Z","iopub.status.idle":"2025-05-15T14:12:59.994693Z","shell.execute_reply.started":"2025-05-15T14:12:59.976963Z","shell.execute_reply":"2025-05-15T14:12:59.993997Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def get_proc(enc_voc, dec_voc):\n\n    # 嵌套函数定义\n    # 外部函数变量生命周期会延续到内部函数调用结束 （闭包）\n\n    def batch_proc(data):\n        \"\"\"\n        批次数据处理并返回\n        \"\"\"\n        enc_ids, dec_ids, labels = [],[],[]\n        for enc,dec in data:\n            # token -> token index\n            enc_idx = [enc_voc[tk] for tk in enc]\n            dec_idx = [dec_voc[tk] for tk in dec]\n\n            # encoder_input\n            enc_ids.append(torch.tensor(enc_idx))\n            # decoder_input\n            dec_ids.append(torch.tensor(dec_idx[:-1]))\n            # label\n            labels.append(torch.tensor(dec_idx[1:]))\n\n        # 数据转换张量 [batch, max_token_len]\n        # 用批次中最长token序列构建张量\n        enc_input = pad_sequence(enc_ids, batch_first=True)\n        dec_input = pad_sequence(dec_ids, batch_first=True)\n        targets = pad_sequence(labels, batch_first=True)\n\n        # 返回数据都是模型训练和推理的需要\n        return enc_input, dec_input, targets\n\n    # 返回回调函数\n    return batch_proc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:12:59.995412Z","iopub.execute_input":"2025-05-15T14:12:59.996189Z","iopub.status.idle":"2025-05-15T14:13:00.012725Z","shell.execute_reply.started":"2025-05-15T14:12:59.996172Z","shell.execute_reply":"2025-05-15T14:13:00.012030Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## 模型训练","metadata":{}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n \n# 超参数配置\nD_MODEL = 256    # 嵌入维度\nNHEAD = 4        # 注意力头数\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\nDIM_FEEDFORWARD = D_MODEL * 4\nDROPOUT = 0.3\n# BATCH_SIZE = 32\nBATCH_SIZE = 512\nMAX_LEN = 50     # 最大对联长度\nEPOCHS = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:18:21.430446Z","iopub.execute_input":"2025-05-15T14:18:21.430732Z","iopub.status.idle":"2025-05-15T14:18:21.435084Z","shell.execute_reply.started":"2025-05-15T14:18:21.430713Z","shell.execute_reply":"2025-05-15T14:18:21.434488Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# 创建数据加载器（需自行实现Dataset类）\nclass CoupletDataset(torch.utils.data.Dataset):\n    def __init__(self, enc_data, dec_data):\n        self.data = list(zip(enc_data, dec_data))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        enc, dec = self.data[idx]\n        return enc, dec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:13:00.029051Z","iopub.execute_input":"2025-05-15T14:13:00.029206Z","iopub.status.idle":"2025-05-15T14:13:00.041738Z","shell.execute_reply.started":"2025-05-15T14:13:00.029194Z","shell.execute_reply":"2025-05-15T14:13:00.041025Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# 数据加载\n# enc_data1 = enc_data1[:1024]\n# dec_data1 = dec_data1[:1024]\ndataset = CoupletDataset(enc_data1, dec_data1)\ndataloader = torch.utils.data.DataLoader(\n    dataset, \n    batch_size=BATCH_SIZE,\n    collate_fn=get_proc(enc_vocab1, dec_vocab1),\n    shuffle=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:18:26.548517Z","iopub.execute_input":"2025-05-15T14:18:26.548795Z","iopub.status.idle":"2025-05-15T14:18:27.312735Z","shell.execute_reply.started":"2025-05-15T14:18:26.548775Z","shell.execute_reply":"2025-05-15T14:18:27.312068Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import math\n# 初始化模型\nmodel = Seq2SeqTransformer(\n    d_model=D_MODEL,\n    nhead=NHEAD,\n    num_enc_layers=NUM_ENCODER_LAYERS,\n    num_dec_layers=NUM_DECODER_LAYERS,\n    dim_forward=DIM_FEEDFORWARD,\n    dropout=DROPOUT,\n    enc_voc_size=len(enc_vocab1),\n    dec_voc_size=len(dec_vocab1)\n).to(device)\n \n# 优化器\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略填充位","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:18:43.199852Z","iopub.execute_input":"2025-05-15T14:18:43.200552Z","iopub.status.idle":"2025-05-15T14:18:43.327642Z","shell.execute_reply.started":"2025-05-15T14:18:43.200525Z","shell.execute_reply":"2025-05-15T14:18:43.326969Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# 生成掩码\ndef create_mask(size, device):\n    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n    return mask.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:18:47.556896Z","iopub.execute_input":"2025-05-15T14:18:47.557472Z","iopub.status.idle":"2025-05-15T14:18:47.560893Z","shell.execute_reply.started":"2025-05-15T14:18:47.557449Z","shell.execute_reply":"2025-05-15T14:18:47.560335Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from tqdm import tqdm\n# 训练循环\nfor epoch in range(EPOCHS):\n    model.train()\n    tpbar = tqdm(dataloader)\n    \n    for enc_input, dec_input, targets in tpbar:\n        # 数据移至设备\n        enc_input = enc_input.to(device)\n        dec_input = dec_input.to(device)\n        targets = targets.to(device)\n        \n        # 创建掩码\n        tgt_mask = create_mask(dec_input.size(1), device)\n        enc_pad_mask = (enc_input == enc_vocab1['PAD'])\n        dec_pad_mask = (dec_input == dec_vocab1['PAD'])\n        \n        # 前向传播\n        outputs = model(\n            enc_inp=enc_input,\n            dec_inp=dec_input,\n            tgt_mask=tgt_mask,\n            enc_pad_mask=enc_pad_mask,\n            dec_pad_mask=dec_pad_mask\n        )\n        \n        # 计算损失（targets需要是(batch_size, seq_len)）\n        loss = criterion(\n            outputs.view(-1, len(dec_vocab1)),\n            targets.view(-1)\n        )\n        \n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tpbar.set_description(f'Epoch {epoch+1}/{EPOCHS} | Loss: {loss.item():.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T14:29:31.998413Z","iopub.execute_input":"2025-05-15T14:29:31.998945Z","iopub.status.idle":"2025-05-15T15:17:35.354693Z","shell.execute_reply.started":"2025-05-15T14:29:31.998919Z","shell.execute_reply":"2025-05-15T15:17:35.354106Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10 | Loss: 4.7934: 100%|██████████| 1505/1505 [04:47<00:00,  5.24it/s]\nEpoch 2/10 | Loss: 4.6006: 100%|██████████| 1505/1505 [04:48<00:00,  5.21it/s]\nEpoch 3/10 | Loss: 4.3728: 100%|██████████| 1505/1505 [04:49<00:00,  5.20it/s]\nEpoch 4/10 | Loss: 4.2023: 100%|██████████| 1505/1505 [04:47<00:00,  5.23it/s]\nEpoch 5/10 | Loss: 4.1045: 100%|██████████| 1505/1505 [04:49<00:00,  5.21it/s]\nEpoch 6/10 | Loss: 3.9753: 100%|██████████| 1505/1505 [04:48<00:00,  5.22it/s]\nEpoch 7/10 | Loss: 4.0236: 100%|██████████| 1505/1505 [04:47<00:00,  5.24it/s]\nEpoch 8/10 | Loss: 3.8615: 100%|██████████| 1505/1505 [04:47<00:00,  5.23it/s]\nEpoch 9/10 | Loss: 3.9718: 100%|██████████| 1505/1505 [04:49<00:00,  5.21it/s]\nEpoch 10/10 | Loss: 3.8202: 100%|██████████| 1505/1505 [04:48<00:00,  5.21it/s]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# 随机选取测试样本\nrnd_idx = random.randint(0, len(enc_data2))\nenc_input = enc_data2[rnd_idx]\ndec_output = dec_data2[rnd_idx]\n\nenc_idx = torch.tensor([[enc_vocab2[tk] for tk in enc_input]])\n\nprint(f'enc_idx: {enc_idx}')\nprint(f'enc_idx.shape: {enc_idx.shape}')\n\n# 最大解码长度\nmax_dec_len = len(dec_output)\nprint(f'max_dec_len: {max_dec_len}')\n\nprint('enc_input：',''.join(enc_input))\nprint(\"dec_output：\", ''.join(dec_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:06:38.637359Z","iopub.execute_input":"2025-05-15T16:06:38.637937Z","iopub.status.idle":"2025-05-15T16:06:38.644020Z","shell.execute_reply.started":"2025-05-15T16:06:38.637915Z","shell.execute_reply":"2025-05-15T16:06:38.643444Z"}},"outputs":[{"name":"stdout","text":"enc_idx: tensor([[2514, 2024, 1140, 2155, 2862]])\nenc_idx.shape: torch.Size([1, 5])\nmax_dec_len: 7\nenc_input： 岸影几家柳\ndec_output： BOS莺声百啭歌EOS\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"def generate_couplet(model, enc_input, enc_vocab, dec_vocab, max_len=50):\n    model.eval()\n\n    # 处理输入张量（避免重复构造）\n    if not isinstance(enc_input, torch.Tensor):\n        # 若输入是列表/数组，直接构造到目标设备\n        enc_input = torch.tensor(enc_input, dtype=torch.long, device=device)\n    else:\n        # 若已经是张量，转移到设备并确保类型正确\n        enc_input = enc_input.to(dtype=torch.long, device=device)\n    \n    # 添加batch维度（若输入是1D）\n    if enc_input.dim() == 1:\n        enc_input = enc_input.unsqueeze(0)  # (1, seq_len)\n    \n    # 编码\n    memory = model.encode(enc_input)\n    \n    # 初始化解码输入（使用传入的dec_vocab参数）\n    bos_token = dec_vocab['BOS']\n    dec_input = torch.tensor([[bos_token]], dtype=torch.long, device=device)  # (1,1)\n    generated = []\n    \n    for _ in range(max_len):\n        # 动态生成解码掩码\n        current_seq_len = dec_input.size(1)\n        dec_mask = create_mask(current_seq_len, device)\n        \n        # 解码（参数名dec_inp需与类定义一致）\n        output = model.decode(\n            dec_inp=dec_input,\n            memory=memory,\n            dec_mask=dec_mask\n        )\n        \n        # 预测下一个token\n        next_token = output.argmax(dim=-1)[:, -1:]  # (1,1)\n        \n        # 检查EOS（使用传入的dec_vocab参数）\n        if next_token.item() == dec_vocab['EOS']:\n            break\n        \n        generated.append(next_token.item())\n        dec_input = torch.cat([dec_input, next_token], dim=-1)\n    \n    return generated\n\n# 使用示例（确保传入正确的词汇表）\ngenerated_dec_idx = generate_couplet(\n    model,\n    enc_idx,          # 确保enc_idx是张量或列表\n    enc_vocab2,       # 编码器词汇表\n    dec_vocab2        # 解码器词汇表（包含BOS/EOS）\n)\n\n# 转换索引到token\ngenerated_dec = [dec_vocab2.get(idx, 'UNK') for idx in generated_dec_idx]\nprint(\"dec_eval：\", ''.join(generated_dec))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:16:34.727652Z","iopub.execute_input":"2025-05-15T16:16:34.728239Z","iopub.status.idle":"2025-05-15T16:16:34.926237Z","shell.execute_reply.started":"2025-05-15T16:16:34.728219Z","shell.execute_reply":"2025-05-15T16:16:34.925641Z"}},"outputs":[{"name":"stdout","text":"dec_eval： UNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNKUNK\n","output_type":"stream"}],"execution_count":69}]}