{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:01.477093Z","iopub.execute_input":"2025-05-29T15:29:01.477437Z","iopub.status.idle":"2025-05-29T15:29:01.482634Z","shell.execute_reply.started":"2025-05-29T15:29:01.477405Z","shell.execute_reply":"2025-05-29T15:29:01.481777Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"!pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:01.483996Z","iopub.execute_input":"2025-05-29T15:29:01.484252Z","iopub.status.idle":"2025-05-29T15:29:04.482200Z","shell.execute_reply.started":"2025-05-29T15:29:01.484237Z","shell.execute_reply":"2025-05-29T15:29:04.481416Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:04.483239Z","iopub.execute_input":"2025-05-29T15:29:04.483545Z","iopub.status.idle":"2025-05-29T15:29:07.717558Z","shell.execute_reply.started":"2025-05-29T15:29:04.483520Z","shell.execute_reply":"2025-05-29T15:29:07.716555Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoTokenizer\nfrom transformers import DataCollatorForTokenClassification\nimport torch\nimport numpy as np\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:07.719858Z","iopub.execute_input":"2025-05-29T15:29:07.720125Z","iopub.status.idle":"2025-05-29T15:29:07.725301Z","shell.execute_reply.started":"2025-05-29T15:29:07.720102Z","shell.execute_reply":"2025-05-29T15:29:07.724411Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\nprint(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:07.726192Z","iopub.execute_input":"2025-05-29T15:29:07.726502Z","iopub.status.idle":"2025-05-29T15:29:07.951353Z","shell.execute_reply.started":"2025-05-29T15:29:07.726468Z","shell.execute_reply":"2025-05-29T15:29:07.950536Z"}},"outputs":[{"name":"stdout","text":"BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"dataset_name = \"doushabao4766/msra_ner_k_V3\"\nmy_dataset = load_dataset(dataset_name)\nprint(my_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:07.952224Z","iopub.execute_input":"2025-05-29T15:29:07.952500Z","iopub.status.idle":"2025-05-29T15:29:09.100028Z","shell.execute_reply.started":"2025-05-29T15:29:07.952478Z","shell.execute_reply":"2025-05-29T15:29:09.099196Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 3443\n    })\n})\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"train_dataset = my_dataset['train']\nprint(train_dataset)\nprint()\n\nprint(train_dataset.features)\nprint()\n\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:09.100924Z","iopub.execute_input":"2025-05-29T15:29:09.101764Z","iopub.status.idle":"2025-05-29T15:29:09.109179Z","shell.execute_reply.started":"2025-05-29T15:29:09.101744Z","shell.execute_reply":"2025-05-29T15:29:09.108343Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n    num_rows: 45001\n})\n\n{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None), 'knowledge': Value(dtype='string', id=None)}\n\n{'id': '0', 'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'knowledge': ''}\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# 原始文本转换模型需要token_idx,生成和 token_idx对齐label\ndef data_input_proc(items):\n    text = [''.join(token) for token in items['tokens']]\n    input_data = tokenizer(\n        text, \n        truncation=True, \n        add_special_tokens=False,\n        max_length=512,\n    )\n\n    adjust_labels = []  # 所有修正后label索引列表\n    # 上一步骤生成ner_tag中索引和token对齐\n    for k in range(len(input_data['input_ids'])):\n        # 每条记录token对应word_ids\n        word_ids = input_data.word_ids(k)\n        # 批次ner_tag长度和token长度对齐\n        tags = items['ner_tags'][k]\n\n        adjusted_label_ids = []\n        i, prev_word_id = -1,-1\n        for word_id in word_ids:\n            if (word_id != prev_word_id):   #  word_ids [0, 1, 2, 3, 4, 4]\n                i += 1 # token对应检索位置+1\n                prev_word_id = word_id\n            if i < len(tags):\n                adjusted_label_ids.append(tags[i])\n            else:\n                # 处理超出范围的情况，例如填充默认标签\n                adjusted_label_ids.append(-100)  # 假设 -100 是填充标签\n        adjust_labels.append(adjusted_label_ids)\n    # 修正后label添加到input_data\n    input_data['labels'] = adjust_labels\n    return input_data\n\nmy_dataset1 = my_dataset.map(data_input_proc, batched=True)  # batched 每次传入自定义方法样本数量多个\nprint(my_dataset1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:09.110058Z","iopub.execute_input":"2025-05-29T15:29:09.110296Z","iopub.status.idle":"2025-05-29T15:29:09.856108Z","shell.execute_reply.started":"2025-05-29T15:29:09.110275Z","shell.execute_reply":"2025-05-29T15:29:09.855031Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeabc8b17d9e4ac6bc76783813f231a1"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3443\n    })\n})\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"print(my_dataset1[\"train\"][0][\"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:09.857075Z","iopub.execute_input":"2025-05-29T15:29:09.857422Z","iopub.status.idle":"2025-05-29T15:29:09.863940Z","shell.execute_reply.started":"2025-05-29T15:29:09.857382Z","shell.execute_reply":"2025-05-29T15:29:09.863003Z"}},"outputs":[{"name":"stdout","text":"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"my_dataset1.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\nprint(my_dataset1[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:09.866774Z","iopub.execute_input":"2025-05-29T15:29:09.867072Z","iopub.status.idle":"2025-05-29T15:29:09.892467Z","shell.execute_reply.started":"2025-05-29T15:29:09.867049Z","shell.execute_reply":"2025-05-29T15:29:09.891576Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n        1159, 8013]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])}\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"ClassLabels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n\nid2label = {i:ClassLabel for i, ClassLabel in enumerate(ClassLabels)}\nlabel2id = {ClassLabel:i for i, ClassLabel in enumerate(ClassLabels)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                        num_labels=len(ClassLabels),\n                                                        id2label=id2label,\n                                                        label2id=label2id)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:09.893232Z","iopub.execute_input":"2025-05-29T15:29:09.893935Z","iopub.status.idle":"2025-05-29T15:29:10.217848Z","shell.execute_reply.started":"2025-05-29T15:29:09.893908Z","shell.execute_reply":"2025-05-29T15:29:10.217018Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n    num_train_epochs = 6,    # 训练 epoch\n    save_safetensors=False,  # 设置 False 保存文件可以通过torch.load加载\n    per_device_train_batch_size=32,  # 训练批次\n    per_device_eval_batch_size=32,   # 验证批次\n    report_to='tensorboard',  # 训练输出记录\n    eval_strategy=\"epoch\",\n)\n\n# metric 方法\ndef compute_metric(result):\n    # result 是一个tuple (predicts, labels)\n\n    # 获取评估对象\n    seqeval = evaluate.load('seqeval')\n    predicts,labels = result\n    predicts = np.argmax(predicts, axis=2)\n\n    # 准备评估数据\n    predicts = [[ClassLabels[p] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    labels = [[ClassLabels[l] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    results = seqeval.compute(predictions=predicts, references=labels)\n\n    return results\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=my_dataset1['train'],\n    eval_dataset=my_dataset1['test'],\n    data_collator=data_collator,\n    compute_metrics=compute_metric\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:29:56.872663Z","iopub.execute_input":"2025-05-29T15:29:56.872951Z","iopub.status.idle":"2025-05-29T15:29:56.915089Z","shell.execute_reply.started":"2025-05-29T15:29:56.872930Z","shell.execute_reply":"2025-05-29T15:29:56.914531Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T15:30:00.748367Z","iopub.execute_input":"2025-05-29T15:30:00.748624Z","iopub.status.idle":"2025-05-29T16:50:44.977537Z","shell.execute_reply.started":"2025-05-29T15:30:00.748607Z","shell.execute_reply":"2025-05-29T16:50:44.976918Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4224' max='4224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4224/4224 1:20:42, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Loc</th>\n      <th>Org</th>\n      <th>Per</th>\n      <th>Overall Precision</th>\n      <th>Overall Recall</th>\n      <th>Overall F1</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.101400</td>\n      <td>0.049429</td>\n      <td>{'precision': 0.8584392014519057, 'recall': 0.8306989813839128, 'f1': 0.8443413066761871, 'number': 2847}</td>\n      <td>{'precision': 0.7329149232914923, 'recall': 0.7968157695223654, 'f1': 0.7635306937885943, 'number': 1319}</td>\n      <td>{'precision': 0.8587859424920128, 'recall': 0.8918380889183809, 'f1': 0.8750000000000001, 'number': 1507}</td>\n      <td>0.827251</td>\n      <td>0.839062</td>\n      <td>0.833115</td>\n      <td>0.983566</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.046200</td>\n      <td>0.051089</td>\n      <td>{'precision': 0.8422770123277737, 'recall': 0.8159466104671584, 'f1': 0.8289027653880465, 'number': 2847}</td>\n      <td>{'precision': 0.8058035714285714, 'recall': 0.821076573161486, 'f1': 0.8133683815245963, 'number': 1319}</td>\n      <td>{'precision': 0.8677524429967427, 'recall': 0.8838752488387525, 'f1': 0.8757396449704141, 'number': 1507}</td>\n      <td>0.840518</td>\n      <td>0.835184</td>\n      <td>0.837843</td>\n      <td>0.984703</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.024700</td>\n      <td>0.045283</td>\n      <td>{'precision': 0.8887692859705777, 'recall': 0.8700386371619249, 'f1': 0.8793042243521477, 'number': 2847}</td>\n      <td>{'precision': 0.8295964125560538, 'recall': 0.8415466262319939, 'f1': 0.8355287918705306, 'number': 1319}</td>\n      <td>{'precision': 0.8981058131939909, 'recall': 0.9124087591240876, 'f1': 0.9052007899934167, 'number': 1507}</td>\n      <td>0.877298</td>\n      <td>0.874669</td>\n      <td>0.875982</td>\n      <td>0.988060</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.018300</td>\n      <td>0.048601</td>\n      <td>{'precision': 0.8705592683784734, 'recall': 0.8693361433087461, 'f1': 0.8699472759226713, 'number': 2847}</td>\n      <td>{'precision': 0.8107511045655376, 'recall': 0.8347232752084913, 'f1': 0.8225625700410909, 'number': 1319}</td>\n      <td>{'precision': 0.8835526315789474, 'recall': 0.8911745189117452, 'f1': 0.8873472084572184, 'number': 1507}</td>\n      <td>0.859815</td>\n      <td>0.867090</td>\n      <td>0.863437</td>\n      <td>0.987483</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.012800</td>\n      <td>0.048968</td>\n      <td>{'precision': 0.9179856115107914, 'recall': 0.8963821566561293, 'f1': 0.9070552692376045, 'number': 2847}</td>\n      <td>{'precision': 0.8412816691505216, 'recall': 0.8559514783927218, 'f1': 0.8485531754979331, 'number': 1319}</td>\n      <td>{'precision': 0.917989417989418, 'recall': 0.9210351692103517, 'f1': 0.9195097714474992, 'number': 1507}</td>\n      <td>0.899716</td>\n      <td>0.893531</td>\n      <td>0.896613</td>\n      <td>0.989215</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.010700</td>\n      <td>0.049934</td>\n      <td>{'precision': 0.913867047891351, 'recall': 0.8981383912890762, 'f1': 0.9059344552701506, 'number': 2847}</td>\n      <td>{'precision': 0.8404726735598228, 'recall': 0.8627748294162244, 'f1': 0.8514777403666294, 'number': 1319}</td>\n      <td>{'precision': 0.9215039577836411, 'recall': 0.927007299270073, 'f1': 0.9242474363215349, 'number': 1507}</td>\n      <td>0.898377</td>\n      <td>0.897585</td>\n      <td>0.897981</td>\n      <td>0.989468</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.8584392014519057, 'recall': 0.8306989813839128, 'f1': 0.8443413066761871, 'number': 2847}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7329149232914923, 'recall': 0.7968157695223654, 'f1': 0.7635306937885943, 'number': 1319}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8587859424920128, 'recall': 0.8918380889183809, 'f1': 0.8750000000000001, 'number': 1507}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.8422770123277737, 'recall': 0.8159466104671584, 'f1': 0.8289027653880465, 'number': 2847}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8058035714285714, 'recall': 0.821076573161486, 'f1': 0.8133683815245963, 'number': 1319}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8677524429967427, 'recall': 0.8838752488387525, 'f1': 0.8757396449704141, 'number': 1507}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.8887692859705777, 'recall': 0.8700386371619249, 'f1': 0.8793042243521477, 'number': 2847}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8295964125560538, 'recall': 0.8415466262319939, 'f1': 0.8355287918705306, 'number': 1319}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8981058131939909, 'recall': 0.9124087591240876, 'f1': 0.9052007899934167, 'number': 1507}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.8705592683784734, 'recall': 0.8693361433087461, 'f1': 0.8699472759226713, 'number': 2847}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8107511045655376, 'recall': 0.8347232752084913, 'f1': 0.8225625700410909, 'number': 1319}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8835526315789474, 'recall': 0.8911745189117452, 'f1': 0.8873472084572184, 'number': 1507}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.9179856115107914, 'recall': 0.8963821566561293, 'f1': 0.9070552692376045, 'number': 2847}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8412816691505216, 'recall': 0.8559514783927218, 'f1': 0.8485531754979331, 'number': 1319}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.917989417989418, 'recall': 0.9210351692103517, 'f1': 0.9195097714474992, 'number': 1507}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.913867047891351, 'recall': 0.8981383912890762, 'f1': 0.9059344552701506, 'number': 2847}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8404726735598228, 'recall': 0.8627748294162244, 'f1': 0.8514777403666294, 'number': 1319}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9215039577836411, 'recall': 0.927007299270073, 'f1': 0.9242474363215349, 'number': 1507}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4224, training_loss=0.031716337032390365, metrics={'train_runtime': 4843.5188, 'train_samples_per_second': 55.746, 'train_steps_per_second': 0.872, 'total_flos': 2.349446673461672e+16, 'train_loss': 0.031716337032390365, 'epoch': 6.0})"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"def predict_entities(text):\n    # 预处理输入文本\n    input_data = tokenizer(\n        text, \n        return_tensors=\"pt\",\n        truncation=True, \n        add_special_tokens=False,\n        max_length=512,\n    )\n\n    # 确保数据和模型在同一设备上\n    device = next(model.parameters()).device  # 获取模型所在的设备\n    for key in input_data:\n        if isinstance(input_data[key], torch.Tensor):\n            input_data[key] = input_data[key].to(device)\n\n    # 运行模型推理\n    with torch.no_grad():\n        outputs = model(**input_data)\n\n    # 获取预测结果\n    predictions = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n\n    # 获取分词后的 token\n    tokens = tokenizer.convert_ids_to_tokens(input_data[\"input_ids\"].squeeze().tolist())\n\n    # 后处理输出，提取实体\n    entities = []\n    current_entity = None\n    current_label = None\n\n    for i, (token, pred) in enumerate(zip(tokens, predictions)):\n        label = id2label[pred]\n\n        if label.startswith(\"B-\"):\n            if current_entity is not None:\n                entities.append(current_entity)\n            current_entity = {\"entity\": label[2:], \"content\": token}\n            current_label = label\n        elif label.startswith(\"I-\") and current_entity is not None and label[2:] == current_entity[\"entity\"]:\n            current_entity[\"content\"] += token\n        else:\n            if current_entity is not None:\n                entities.append(current_entity)\n                current_entity = None\n\n    # 添加最后一个实体（如果有）\n    if current_entity is not None:\n        entities.append(current_entity)\n\n    return entities\n\n# 示例调用\ntext = \"双方确定了今后发展中美关系的指导方针。\"\nentities = predict_entities(text)\nprint(entities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:54:03.495746Z","iopub.execute_input":"2025-05-29T16:54:03.496044Z","iopub.status.idle":"2025-05-29T16:54:03.541992Z","shell.execute_reply.started":"2025-05-29T16:54:03.496023Z","shell.execute_reply":"2025-05-29T16:54:03.541185Z"}},"outputs":[{"name":"stdout","text":"[{'entity': 'LOC', 'content': '中'}, {'entity': 'LOC', 'content': '美'}]\n","output_type":"stream"}],"execution_count":54}]}