{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:53:53.398609Z","iopub.execute_input":"2025-06-12T15:53:53.399233Z","iopub.status.idle":"2025-06-12T15:53:53.654645Z","shell.execute_reply.started":"2025-06-12T15:53:53.399207Z","shell.execute_reply":"2025-06-12T15:53:53.653894Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q evaluate\n!pip install -q seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:54:03.096293Z","iopub.execute_input":"2025-06-13T09:54:03.096851Z","iopub.status.idle":"2025-06-13T09:54:14.772528Z","shell.execute_reply.started":"2025-06-13T09:54:03.096823Z","shell.execute_reply":"2025-06-13T09:54:14.771581Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:54:31.215417Z","iopub.execute_input":"2025-06-13T09:54:31.215730Z","iopub.status.idle":"2025-06-13T09:54:31.221317Z","shell.execute_reply.started":"2025-06-13T09:54:31.215708Z","shell.execute_reply":"2025-06-13T09:54:31.220082Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## 训练数据准备","metadata":{}},{"cell_type":"code","source":"# 加载hf中dataset\nds = load_dataset('nlhappy/CLUE-NER')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:54:40.138461Z","iopub.execute_input":"2025-06-13T09:54:40.139138Z","iopub.status.idle":"2025-06-13T09:54:42.968157Z","shell.execute_reply.started":"2025-06-13T09:54:40.139097Z","shell.execute_reply":"2025-06-13T09:54:42.967285Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01cf505656274189997c19e005316d9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/970 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b706dc1144413f84515e332105a6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-a33d0e4276aef9b4.parquet:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522169f3726b4f7a9de415e0d7f0074f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-07f476b71c5edde6.parquet:   0%|          | 0.00/178k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124acd8e880b4aa888aeae5a29cc5da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f35ed71ffa04400a54ab9c0dea1830c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1343 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051ff6b025514a2e9cf265f6363b162f"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'ents'],\n        num_rows: 10748\n    })\n    validation: Dataset({\n        features: ['text', 'ents'],\n        num_rows: 1343\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_data = ds['train']\nprint(\"train_data[0]:\", train_data[0])\n \n# 初始化一个集合来存储所有唯一的标签\nall_labels = set()\n \n# 遍历每个样本\nfor sample in train_data:\n    # 检查样本中是否包含 'ents' 字段\n    if 'ents' in sample:\n        # 遍历每个实体\n        for entity in sample['ents']:\n            # 提取标签并添加到集合中\n            if 'label' in entity:\n                all_labels.add(entity['label'])\n \n# 打印所有唯一的标签\nprint(\"All unique labels:\", all_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:55:13.513196Z","iopub.execute_input":"2025-06-13T09:55:13.513509Z","iopub.status.idle":"2025-06-13T09:55:14.302565Z","shell.execute_reply.started":"2025-06-13T09:55:13.513485Z","shell.execute_reply":"2025-06-13T09:55:14.301778Z"}},"outputs":[{"name":"stdout","text":"train_data[0]: {'text': '浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，', 'ents': [{'indices': [9, 10, 11], 'is_continuous': True, 'label': 'name', 'text': '叶老桂'}, {'indices': [0, 1, 2, 3], 'is_continuous': True, 'label': 'company', 'text': '浙商银行'}]}\nAll unique labels: {'organization', 'company', 'scene', 'book', 'name', 'game', 'movie', 'address', 'position', 'government'}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# entity_index\nentites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n           'company', 'scene', 'book', 'organization', 'government'})\ntags = ['O']\nfor entity in entites[1:]:\n    tags.append('B-' + entity.upper())\n    tags.append('I-' + entity.upper())\n\nentity_index = {entity:i for i, entity in enumerate(entites)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:55:21.865082Z","iopub.execute_input":"2025-06-13T09:55:21.865729Z","iopub.status.idle":"2025-06-13T09:55:21.870754Z","shell.execute_reply.started":"2025-06-13T09:55:21.865701Z","shell.execute_reply":"2025-06-13T09:55:21.870010Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def entity_tags_proc(item):\n    # item即是dataset中记录\n    text_len = len(item['text'])  # 根据文本长度生成tags列表\n    tags = [0] * text_len    # 初始值为‘O’\n    # 遍历实体列表，所有实体类别标记填入tags\n    entites = item['ents']\n    for ent in entites:\n        indices = ent['indices']  # 实体索引\n        label = ent['label']   # 实体名\n        tags[indices[0]] = entity_index[label] * 2 - 1\n        for idx in indices[1:]:\n            tags[idx] = entity_index[label] * 2\n    return {'ent_tag': tags}\n\n# 使用自定义回调函数处理数据集记录\nds1 = ds.map(entity_tags_proc)\nds1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:55:24.722743Z","iopub.execute_input":"2025-06-13T09:55:24.723346Z","iopub.status.idle":"2025-06-13T09:55:26.308911Z","shell.execute_reply.started":"2025-06-13T09:55:24.723320Z","shell.execute_reply":"2025-06-13T09:55:26.308094Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1158337d806843cba6dde93e6b4bf10f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1343 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f528e4e54e4808afea8cf015c962fd"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'ents', 'ent_tag'],\n        num_rows: 10748\n    })\n    validation: Dataset({\n        features: ['text', 'ents', 'ent_tag'],\n        num_rows: 1343\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:55:29.760752Z","iopub.execute_input":"2025-06-13T09:55:29.761063Z","iopub.status.idle":"2025-06-13T09:55:32.980477Z","shell.execute_reply.started":"2025-06-13T09:55:29.761039Z","shell.execute_reply":"2025-06-13T09:55:32.979538Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a6a5d9a0c3d404287a20a0c943103b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c87687701b46cf87045e52bd1ddb39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"573636c9368943998b62411210b282b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f57ef734cf6b402bbc37f079ca1351fc"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def data_input_proc(item):\n    # 输入文本先拆分为字符，再转换为模型输入的token索引\n    batch_texts = [list(text) for text in item['text']]\n    # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n    input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512,\n                           is_split_into_words=True, padding='max_length')\n    input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n    return input_data\n\n\nds2 = ds1.map(data_input_proc, batched=True)  # batched 每次传入自定义方法样本数量多个\n\n# 记录转换为pytorch\nds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\nds2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:55:35.154957Z","iopub.execute_input":"2025-06-13T09:55:35.155706Z","iopub.status.idle":"2025-06-13T09:55:41.787452Z","shell.execute_reply.started":"2025-06-13T09:55:35.155680Z","shell.execute_reply":"2025-06-13T09:55:41.786775Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10748 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be29dcaa189b443d915aac4ab7fbef10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1343 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad836085c0fa4b488562be89848d2d66"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'ents', 'ent_tag', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 10748\n    })\n    validation: Dataset({\n        features: ['text', 'ents', 'ent_tag', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 1343\n    })\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n\nid2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:55:44.056850Z","iopub.execute_input":"2025-06-13T09:55:44.057612Z","iopub.status.idle":"2025-06-13T09:55:44.062051Z","shell.execute_reply.started":"2025-06-13T09:55:44.057588Z","shell.execute_reply":"2025-06-13T09:55:44.061185Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(id2lbl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:55:46.779345Z","iopub.execute_input":"2025-06-13T09:55:46.779628Z","iopub.status.idle":"2025-06-13T09:55:46.784324Z","shell.execute_reply.started":"2025-06-13T09:55:46.779610Z","shell.execute_reply":"2025-06-13T09:55:46.783371Z"}},"outputs":[{"name":"stdout","text":"{0: 'O', 1: 'B-ORGANIZATION', 2: 'I-ORGANIZATION', 3: 'B-COMPANY', 4: 'I-COMPANY', 5: 'B-SCENE', 6: 'I-SCENE', 7: 'B-BOOK', 8: 'I-BOOK', 9: 'B-NAME', 10: 'I-NAME', 11: 'B-GAME', 12: 'I-GAME', 13: 'B-MOVIE', 14: 'I-MOVIE', 15: 'B-ADDRESS', 16: 'I-ADDRESS', 17: 'B-POSITION', 18: 'I-POSITION', 19: 'B-GOVERNMENT', 20: 'I-GOVERNMENT'}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(lbl2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:55:48.820362Z","iopub.execute_input":"2025-06-13T09:55:48.821038Z","iopub.status.idle":"2025-06-13T09:55:48.825166Z","shell.execute_reply.started":"2025-06-13T09:55:48.821012Z","shell.execute_reply":"2025-06-13T09:55:48.824307Z"}},"outputs":[{"name":"stdout","text":"{'O': 0, 'B-ORGANIZATION': 1, 'I-ORGANIZATION': 2, 'B-COMPANY': 3, 'I-COMPANY': 4, 'B-SCENE': 5, 'I-SCENE': 6, 'B-BOOK': 7, 'I-BOOK': 8, 'B-NAME': 9, 'I-NAME': 10, 'B-GAME': 11, 'I-GAME': 12, 'B-MOVIE': 13, 'I-MOVIE': 14, 'B-ADDRESS': 15, 'I-ADDRESS': 16, 'B-POSITION': 17, 'I-POSITION': 18, 'B-GOVERNMENT': 19, 'I-GOVERNMENT': 20}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 模型训练","metadata":{}},{"cell_type":"markdown","source":"### 1. 动态学习率","metadata":{}},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nDEVICE='cuda'\n\nmodel1 = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                        num_labels=len(tags),\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel1.to(DEVICE)\n\n# 模型参数分组\nparam_optimizer1 = list(model1.named_parameters())\nbert_params1, classifier_params1 = [],[]\n\nfor name,params in param_optimizer1:\n    if 'bert' in name:\n        bert_params1.append(params)\n    else:\n        classifier_params1.append(params)\n\nparam_groups1 = [\n    {'params':bert_params1, 'lr':1e-5},\n    {'params':classifier_params1, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n# optimizer\noptimizer1 = optim.AdamW(param_groups1) # 优化器\n\nepochs = 2\n\n# 学习率调度器\ntrain_steps = len(train_dl) * epochs\nscheduler1 = get_linear_schedule_with_warmup(optimizer1,\n                                            num_warmup_steps=100,\n                                            num_training_steps=train_steps)\n\nfor epoch in range(epochs):\n    model1.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        optimizer1.zero_grad()\n        outputs = model1(**items)\n        loss = outputs.loss\n        loss.backward()\n        optimizer1.step()\n        scheduler1.step()\n\n        tpbar.set_description(f'Epoch:{epoch+1} ' +\n                          f'bert_lr:{scheduler1.get_lr()[0]:.8f} ' +\n                          f'classifier_lr:{scheduler1.get_lr()[1]:.8f} '+\n                          f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T02:44:08.569179Z","iopub.execute_input":"2025-06-13T02:44:08.569448Z","iopub.status.idle":"2025-06-13T03:18:58.958270Z","shell.execute_reply.started":"2025-06-13T02:44:08.569430Z","shell.execute_reply":"2025-06-13T03:18:58.957666Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch:1 bert_lr:0.00000540 classifier_lr:0.00054019 Loss:0.0363: 100%|██████████| 672/672 [17:25<00:00,  1.56s/it]\nEpoch:2 bert_lr:0.00000000 classifier_lr:0.00000000 Loss:0.0313: 100%|██████████| 672/672 [17:24<00:00,  1.55s/it]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### 2.使用混合精度","metadata":{}},{"cell_type":"code","source":"import torch\n\nmodel2 = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                        num_labels=len(tags),\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel2.to(DEVICE)\n\n# 模型参数分组\nparam_optimizer2 = list(model2.named_parameters())\nbert_params2, classifier_params2 = [],[]\n\nfor name,params in param_optimizer2:\n    if 'bert' in name:\n        bert_params2.append(params)\n    else:\n        classifier_params2.append(params)\n\nparam_groups2 = [\n    {'params':bert_params2, 'lr':1e-5},\n    {'params':classifier_params2, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n# optimizer\noptimizer2 = optim.AdamW(param_groups2) # 优化器\n\nepochs = 2\n\n# 学习率调度器\ntrain_steps = len(train_dl) * epochs\nscheduler2 = get_linear_schedule_with_warmup(optimizer2,\n                                            num_warmup_steps=100,\n                                            num_training_steps=train_steps)\n\n# 梯度计算缩放器\nscaler = torch.GradScaler()\n\nfor epoch in range(epochs):\n    model2.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        optimizer2.zero_grad()\n\n        with torch.autocast(device_type='cuda'):\n            outputs = model2(**items)\n        loss = outputs.loss\n\n        # 缩放loss后，调用backward\n        scaler.scale(loss).backward()\n        scaler.step(optimizer2)\n        scaler.update()\n        scheduler2.step()\n\n        tpbar.set_description(f'Epoch:{epoch+1} ' +\n                          f'bert_lr:{scheduler2.get_lr()[0]:.8f} ' +\n                          f'classifier_lr:{scheduler2.get_lr()[1]:.8f} '+\n                          f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T03:27:15.025960Z","iopub.execute_input":"2025-06-13T03:27:15.026645Z","iopub.status.idle":"2025-06-13T03:36:18.970951Z","shell.execute_reply.started":"2025-06-13T03:27:15.026623Z","shell.execute_reply":"2025-06-13T03:36:18.970385Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch:1 bert_lr:0.00000540 classifier_lr:0.00054019 Loss:0.0375: 100%|██████████| 672/672 [04:32<00:00,  2.47it/s]\nEpoch:2 bert_lr:0.00000000 classifier_lr:0.00000000 Loss:0.0172: 100%|██████████| 672/672 [04:31<00:00,  2.48it/s]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### 3.使用 DDP 训练","metadata":{}},{"cell_type":"code","source":"%%writefile ddp_train.py\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom datasets import load_dataset\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import AutoTokenizer\nfrom transformers import DataCollatorForTokenClassification\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm import tqdm\n\n# 设置分布式环境\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# 清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n\n# 定义训练循环\ndef train(rank, world_size):\n    setup(rank, world_size)\n\n    # 数据集\n    ds = load_dataset('nlhappy/CLUE-NER')\n    # entity_index\n    entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n               'company', 'scene', 'book', 'organization', 'government'})\n    tags = ['O']\n    for entity in entites[1:]:\n        tags.append('B-' + entity.upper())\n        tags.append('I-' + entity.upper())\n\n    entity_index = {entity:i for i, entity in enumerate(entites)}\n\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n\n    def entity_tags_proc(item):\n        # item即是dataset中记录\n        text_len = len(item['text'])  # 根据文本长度生成tags列表\n        tags = [0] * text_len    # 初始值为‘O’\n        # 遍历实体列表，所有实体类别标记填入tags\n        entites = item['ents']\n        for ent in entites:\n            indices = ent['indices']  # 实体索引\n            label = ent['label']   # 实体名\n            tags[indices[0]] = entity_index[label] * 2 - 1\n            for idx in indices[1:]:\n                tags[idx] = entity_index[label] * 2\n        return {'ent_tag': tags}\n\n    # 使用自定义回调函数处理数据集记录\n    ds1 = ds.map(entity_tags_proc)\n\n    def data_input_proc(item):\n        # 输入文本先拆分为字符，再转换为模型输入的token索引\n        batch_texts = [list(text) for text in item['text']]\n        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n        input_data = tokenizer(\n            batch_texts,\n            truncation=True,\n            add_special_tokens=False,\n            max_length=512,\n            is_split_into_words=True,\n            padding='max_length'\n        )\n\n        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n        return input_data\n\n\n    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n\n    ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n    # 分布式训练采样器\n    sampler = DistributedSampler(ds2['train'], num_replicas=world_size, rank=rank)\n    train_dl = DataLoader(ds2['train'], sampler=sampler, batch_size=16)\n\n    id2lbl = {i:tag for i, tag in enumerate(tags)}\n    lbl2id = {tag:i for i, tag in enumerate(tags)}\n\n    # 定义模型并将其移动到对应的 GPU 设备端\n    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                            num_labels=len(tags),\n                                                            id2label=id2lbl,\n                                                            label2id=lbl2id)\n    model = model.to(rank)\n\n    ddp_model = DDP(model, device_ids=[rank])\n\n    # 模型参数分组\n    param_optimizer = list(model.named_parameters())\n    bert_params, classifier_params = [],[]\n\n    for name,params in param_optimizer:\n        if 'bert' in name:\n            bert_params.append(params)\n        else:\n            classifier_params.append(params)\n    \n    param_groups = [\n        {'params':bert_params, 'lr':1e-5},\n        {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n    ]\n    \n    # optimizer\n    optimizer = optim.AdamW(param_groups) # 优化器\n\n    epochs = 2\n    \n    # 学习率调度器\n    train_steps = len(train_dl) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=100,\n                                                num_training_steps=train_steps)\n\n    # 在训练开始时创建一次\n    scaler = torch.GradScaler()\n    # scaler = torch.cuda.amp.GradScaler()\n\n    for epoch in range(epochs):\n        ddp_model.train()\n        tpbar = tqdm(train_dl)\n        for items in tpbar:\n            items = {k:v.to(rank) for k,v in items.items()}\n            optimizer.zero_grad()\n    \n            # with torch.autocast(device_type='cuda'):\n            with torch.cuda.amp.autocast():\n                outputs = ddp_model(**items)\n            loss = outputs.loss\n    \n            # 缩放loss后，调用backward\n            scaler.scale(loss).backward()\n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n    \n            # print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n    \n            tpbar.set_description(f'Epoch:{epoch+1} ' +\n                              f'bert_lr:{scheduler.get_lr()[0]:.8f} ' +\n                              f'classifier_lr:{scheduler.get_lr()[1]:.8f} '+\n                              f'Loss:{loss.item():.4f}')\n\n    cleanup()\n\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:51:56.714911Z","iopub.execute_input":"2025-06-13T06:51:56.715265Z","iopub.status.idle":"2025-06-13T06:51:56.723744Z","shell.execute_reply.started":"2025-06-13T06:51:56.715237Z","shell.execute_reply":"2025-06-13T06:51:56.722958Z"}},"outputs":[{"name":"stdout","text":"Overwriting ddp_train.py\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!python ddp_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:52:01.504668Z","iopub.execute_input":"2025-06-13T06:52:01.505485Z","iopub.status.idle":"2025-06-13T06:57:36.204605Z","shell.execute_reply.started":"2025-06-13T06:52:01.505452Z","shell.execute_reply":"2025-06-13T06:57:36.203739Z"}},"outputs":[{"name":"stdout","text":"2025-06-13 06:52:07.243231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749797527.265326     663 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749797527.271913     663 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-13 06:52:16.279207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749797536.301761     677 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749797536.308920     677 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-13 06:52:16.328666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749797536.351479     678 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749797536.358308     678 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nMap: 100%|███████████████████████| 10748/10748 [00:01<00:00, 9717.03 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 9234.89 examples/s]\nMap: 100%|██████████████████████| 10748/10748 [00:01<00:00, 10190.81 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 6792.07 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:05<00:00, 1891.67 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:05<00:00, 1894.11 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:01<00:00, 1334.89 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1404.11 examples/s]\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  0%|                                                   | 0/336 [00:00<?, ?it/s]/kaggle/working/ddp_train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/kaggle/working/ddp_train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch:1 bert_lr:0.00000587 classifier_lr:0.00058741 Loss:0.0261: 100%|█| 336/336\nEpoch:1 bert_lr:0.00000587 classifier_lr:0.00058741 Loss:0.0498: 100%|█| 336/336\nEpoch:2 bert_lr:0.00000000 classifier_lr:0.00000000 Loss:0.0225: 100%|█| 336/336\nEpoch:2 bert_lr:0.00000000 classifier_lr:0.00000000 Loss:0.0453: 100%|█| 336/336\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### 4. Trainer 模型训练整合","metadata":{}},{"cell_type":"code","source":"%%writefile trainer_full_train.py\n\nimport os\nimport numpy as np\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n# 设置分布式环境\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# 清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train(rank, world_size):\n    setup(rank, world_size)\n    # 数据集\n    ds = load_dataset('nlhappy/CLUE-NER')\n    # entity_index\n    entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n               'company', 'scene', 'book', 'organization', 'government'})\n    tags = ['O']\n    for entity in entites[1:]:\n        tags.append('B-' + entity.upper())\n        tags.append('I-' + entity.upper())\n\n    entity_index = {entity:i for i, entity in enumerate(entites)}\n\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n\n    def entity_tags_proc(item):\n        # item即是dataset中记录\n        text_len = len(item['text'])  # 根据文本长度生成tags列表\n        tags = [0] * text_len    # 初始值为‘O’\n        # 遍历实体列表，所有实体类别标记填入tags\n        entites = item['ents']\n        for ent in entites:\n            indices = ent['indices']  # 实体索引\n            label = ent['label']   # 实体名\n            tags[indices[0]] = entity_index[label] * 2 - 1\n            for idx in indices[1:]:\n                tags[idx] = entity_index[label] * 2\n        return {'ent_tag': tags}\n\n    # 使用自定义回调函数处理数据集记录\n    ds1 = ds.map(entity_tags_proc)\n\n    def data_input_proc(item):\n        # 输入文本先拆分为字符，再转换为模型输入的token索引\n        batch_texts = [list(text) for text in item['text']]\n        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n        input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512,\n                               is_split_into_words=True, padding='max_length')\n        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n        return input_data\n\n\n    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n\n    local_rank = rank\n\n    id2lbl = {i:tag for i, tag in enumerate(tags)}\n    lbl2id = {tag:i for i, tag in enumerate(tags)}\n\n    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                            num_labels=len(tags),\n                                                            id2label=id2lbl,\n                                                            label2id=lbl2id)\n    model.to(local_rank)\n\n    # 冻结模型的所有参数，除了分类层\n    # for name, param in model.named_parameters():\n    #     if 'classifier' not in name:\n    #         param.requires_grad = False\n\n    args = TrainingArguments(\n        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n        num_train_epochs = 2,    # 训练 epoch\n        # save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n        per_device_train_batch_size=16, # 训练批次\n        per_device_eval_batch_size=16,\n        # report_to='tensorboard',  # 训练输出记录\n        report_to=\"none\",  # 禁用所有报告\n        # save_strategy=\"steps\",  # 减少保存频率\n        # save_steps=1000,\n        # logging_strategy=\"steps\",  # 减少日志频率\n        # logging_steps=1000,\n        eval_strategy=\"epoch\",\n        local_rank=local_rank,   # 当前进程 RANK\n        fp16=True,               # 使用混合精度\n        lr_scheduler_type='linear',  # 动态学习率\n        warmup_steps=100,        # 预热步数\n        # dataloader_num_workers=4,  # 增加数据加载线程数\n        ddp_find_unused_parameters=False  # 优化DDP性能\n    )\n\n    def compute_metric(result):\n        # result 是一个tuple (predicts, labels)\n\n        # 获取评估对象\n        seqeval = evaluate.load('seqeval')\n        predicts,labels = result\n        predicts = np.argmax(predicts, axis=2)\n\n        # 准备评估数据\n        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        results = seqeval.compute(predictions=predicts, references=labels)\n\n        return results\n\n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n\n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=ds2['train'],\n        eval_dataset=ds2['validation'],\n        data_collator=data_collator,\n        compute_metrics=compute_metric\n    )\n\n    trainer.train()\n\n    torch.save(model.state_dict(), 'model.bin')\n\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T10:18:32.457761Z","iopub.execute_input":"2025-06-13T10:18:32.458603Z","iopub.status.idle":"2025-06-13T10:18:32.466237Z","shell.execute_reply.started":"2025-06-13T10:18:32.458572Z","shell.execute_reply":"2025-06-13T10:18:32.465408Z"}},"outputs":[{"name":"stdout","text":"Overwriting trainer_full_train.py\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!python trainer_full_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T10:18:37.339687Z","iopub.execute_input":"2025-06-13T10:18:37.340360Z","iopub.status.idle":"2025-06-13T11:04:06.806849Z","shell.execute_reply.started":"2025-06-13T10:18:37.340332Z","shell.execute_reply":"2025-06-13T11:04:06.805776Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"2025-06-13 10:18:43.253645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749809923.280253    1821 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749809923.287894    1821 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-13 10:18:52.698189: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-13 10:18:52.711314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749809932.720647    1835 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749809932.727571    1835 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749809932.733674    1836 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749809932.740933    1836 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[W613 10:18:57.798777767 socket.cpp:759] [c10d] The client socket has failed to connect to [localhost]:12355 (errno: 99 - Cannot assign requested address).\nMap: 100%|███████████████████████| 10748/10748 [00:01<00:00, 8330.09 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:01<00:00, 8283.62 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 7864.81 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 7820.63 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:06<00:00, 1765.04 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:06<00:00, 1754.24 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1836.44 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1926.79 examples/s]\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n  0%|                                                   | 0/672 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n 50%|████████████████████▌                    | 336/672 [21:29<21:05,  3.77s/it]\n  0%|                                                    | 0/42 [00:00<?, ?it/s]\u001b[A\n  0%|                                                    | 0/42 [00:00<?, ?it/s]\u001b[A\n  5%|██                                          | 2/42 [00:01<00:23,  1.72it/s]\u001b[A\n  5%|██                                          | 2/42 [00:01<00:24,  1.65it/s]\u001b[A\n  7%|███▏                                        | 3/42 [00:02<00:32,  1.19it/s]\u001b[A\n  7%|███▏                                        | 3/42 [00:02<00:33,  1.17it/s]\u001b[A\n 10%|████▏                                       | 4/42 [00:03<00:36,  1.03it/s]\u001b[A\n 10%|████▏                                       | 4/42 [00:03<00:37,  1.01it/s]\u001b[A\n 12%|█████▏                                      | 5/42 [00:04<00:39,  1.06s/it]\u001b[A\n 12%|█████▏                                      | 5/42 [00:04<00:39,  1.07s/it]\u001b[A\n 14%|██████▎                                     | 6/42 [00:05<00:39,  1.10s/it]\u001b[A\n 14%|██████▎                                     | 6/42 [00:06<00:40,  1.12s/it]\u001b[A\n 17%|███████▎                                    | 7/42 [00:07<00:39,  1.13s/it]\u001b[A\n 17%|███████▎                                    | 7/42 [00:07<00:40,  1.15s/it]\u001b[A\n 19%|████████▍                                   | 8/42 [00:08<00:38,  1.14s/it]\u001b[A\n 19%|████████▍                                   | 8/42 [00:08<00:40,  1.19s/it]\u001b[A\n 21%|█████████▍                                  | 9/42 [00:09<00:38,  1.16s/it]\u001b[A\n 21%|█████████▍                                  | 9/42 [00:09<00:39,  1.20s/it]\u001b[A\n 24%|██████████▏                                | 10/42 [00:10<00:37,  1.17s/it]\u001b[A\n 24%|██████████▏                                | 10/42 [00:11<00:38,  1.21s/it]\u001b[A\n 26%|███████████▎                               | 11/42 [00:11<00:36,  1.18s/it]\u001b[A\n 26%|███████████▎                               | 11/42 [00:12<00:37,  1.21s/it]\u001b[A\n 29%|████████████▎                              | 12/42 [00:13<00:35,  1.18s/it]\u001b[A\n 29%|████████████▎                              | 12/42 [00:13<00:36,  1.21s/it]\u001b[A\n 31%|█████████████▎                             | 13/42 [00:14<00:34,  1.19s/it]\u001b[A\n 31%|█████████████▎                             | 13/42 [00:14<00:35,  1.22s/it]\u001b[A\n 33%|██████████████▎                            | 14/42 [00:15<00:33,  1.19s/it]\u001b[A\n 33%|██████████████▎                            | 14/42 [00:15<00:34,  1.22s/it]\u001b[A\n 36%|███████████████▎                           | 15/42 [00:16<00:32,  1.19s/it]\u001b[A\n 36%|███████████████▎                           | 15/42 [00:17<00:32,  1.22s/it]\u001b[A\n 38%|████████████████▍                          | 16/42 [00:17<00:31,  1.19s/it]\u001b[A\n 38%|████████████████▍                          | 16/42 [00:18<00:31,  1.23s/it]\u001b[A\n 40%|█████████████████▍                         | 17/42 [00:19<00:29,  1.20s/it]\u001b[A\n 40%|█████████████████▍                         | 17/42 [00:19<00:30,  1.23s/it]\u001b[A\n 43%|██████████████████▍                        | 18/42 [00:20<00:28,  1.21s/it]\u001b[A\n 43%|██████████████████▍                        | 18/42 [00:20<00:29,  1.23s/it]\u001b[A\n 45%|███████████████████▍                       | 19/42 [00:21<00:27,  1.20s/it]\u001b[A\n 45%|███████████████████▍                       | 19/42 [00:22<00:28,  1.22s/it]\u001b[A\n 48%|████████████████████▍                      | 20/42 [00:22<00:26,  1.21s/it]\u001b[A\n 48%|████████████████████▍                      | 20/42 [00:23<00:26,  1.22s/it]\u001b[A\n 50%|█████████████████████▌                     | 21/42 [00:23<00:25,  1.21s/it]\u001b[A\n 50%|█████████████████████▌                     | 21/42 [00:24<00:25,  1.22s/it]\u001b[A\n 52%|██████████████████████▌                    | 22/42 [00:25<00:24,  1.21s/it]\u001b[A\n 52%|██████████████████████▌                    | 22/42 [00:25<00:24,  1.22s/it]\u001b[A\n 55%|███████████████████████▌                   | 23/42 [00:26<00:23,  1.22s/it]\u001b[A\n 55%|███████████████████████▌                   | 23/42 [00:26<00:23,  1.22s/it]\u001b[A\n 57%|████████████████████████▌                  | 24/42 [00:27<00:21,  1.22s/it]\u001b[A\n 57%|████████████████████████▌                  | 24/42 [00:28<00:21,  1.22s/it]\u001b[A\n 60%|█████████████████████████▌                 | 25/42 [00:28<00:20,  1.21s/it]\u001b[A\n 60%|█████████████████████████▌                 | 25/42 [00:29<00:20,  1.23s/it]\u001b[A\n 62%|██████████████████████████▌                | 26/42 [00:29<00:19,  1.20s/it]\u001b[A\n 62%|██████████████████████████▌                | 26/42 [00:30<00:19,  1.23s/it]\u001b[A\n 64%|███████████████████████████▋               | 27/42 [00:31<00:18,  1.21s/it]\u001b[A\n 64%|███████████████████████████▋               | 27/42 [00:31<00:18,  1.22s/it]\u001b[A\n 67%|████████████████████████████▋              | 28/42 [00:32<00:17,  1.22s/it]\u001b[A\n 67%|████████████████████████████▋              | 28/42 [00:33<00:17,  1.22s/it]\u001b[A\n 69%|█████████████████████████████▋             | 29/42 [00:33<00:16,  1.23s/it]\u001b[A\n 69%|█████████████████████████████▋             | 29/42 [00:34<00:15,  1.22s/it]\u001b[A\n 71%|██████████████████████████████▋            | 30/42 [00:34<00:14,  1.23s/it]\u001b[A\n 71%|██████████████████████████████▋            | 30/42 [00:35<00:14,  1.22s/it]\u001b[A\n 74%|███████████████████████████████▋           | 31/42 [00:36<00:13,  1.23s/it]\u001b[A\n 74%|███████████████████████████████▋           | 31/42 [00:36<00:13,  1.22s/it]\u001b[A\n 76%|████████████████████████████████▊          | 32/42 [00:37<00:12,  1.23s/it]\u001b[A\n 76%|████████████████████████████████▊          | 32/42 [00:37<00:12,  1.22s/it]\u001b[A\n 79%|█████████████████████████████████▊         | 33/42 [00:38<00:11,  1.23s/it]\u001b[A\n 79%|█████████████████████████████████▊         | 33/42 [00:39<00:10,  1.22s/it]\u001b[A\n 81%|██████████████████████████████████▊        | 34/42 [00:39<00:09,  1.23s/it]\u001b[A\n 81%|██████████████████████████████████▊        | 34/42 [00:40<00:09,  1.22s/it]\u001b[A\n 83%|███████████████████████████████████▊       | 35/42 [00:41<00:08,  1.22s/it]\u001b[A\n 83%|███████████████████████████████████▊       | 35/42 [00:41<00:08,  1.22s/it]\u001b[A\n 86%|████████████████████████████████████▊      | 36/42 [00:42<00:07,  1.22s/it]\u001b[A\n 86%|████████████████████████████████████▊      | 36/42 [00:42<00:07,  1.22s/it]\u001b[A\n 88%|█████████████████████████████████████▉     | 37/42 [00:43<00:06,  1.22s/it]\u001b[A\n 88%|█████████████████████████████████████▉     | 37/42 [00:44<00:06,  1.22s/it]\u001b[A\n 90%|██████████████████████████████████████▉    | 38/42 [00:44<00:04,  1.22s/it]\u001b[A\n 90%|██████████████████████████████████████▉    | 38/42 [00:45<00:04,  1.22s/it]\u001b[A\n 93%|███████████████████████████████████████▉   | 39/42 [00:45<00:03,  1.22s/it]\u001b[A\n 93%|███████████████████████████████████████▉   | 39/42 [00:46<00:03,  1.22s/it]\u001b[A\n 95%|████████████████████████████████████████▉  | 40/42 [00:47<00:02,  1.22s/it]\u001b[A\n 95%|████████████████████████████████████████▉  | 40/42 [00:47<00:02,  1.22s/it]\u001b[A\n 98%|█████████████████████████████████████████▉ | 41/42 [00:48<00:01,  1.22s/it]\u001b[A\n 98%|█████████████████████████████████████████▉ | 41/42 [00:48<00:01,  1.23s/it]\u001b[A\n100%|███████████████████████████████████████████| 42/42 [00:49<00:00,  1.18s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.01686859503388405, 'eval_ADDRESS': {'precision': 0.5241228070175439, 'recall': 0.6407506702412868, 'f1': 0.5765983112183353, 'number': 373}, 'eval_BOOK': {'precision': 0.6410256410256411, 'recall': 0.8116883116883117, 'f1': 0.7163323782234958, 'number': 154}, 'eval_COMPANY': {'precision': 0.6464208242950108, 'recall': 0.7883597883597884, 'f1': 0.7103694874851013, 'number': 378}, 'eval_GAME': {'precision': 0.7034883720930233, 'recall': 0.8203389830508474, 'f1': 0.7574334898278561, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6559485530546624, 'recall': 0.8259109311740891, 'f1': 0.7311827956989247, 'number': 247}, 'eval_MOVIE': {'precision': 0.7027027027027027, 'recall': 0.6887417218543046, 'f1': 0.6956521739130436, 'number': 151}, 'eval_NAME': {'precision': 0.8082191780821918, 'recall': 0.8881720430107527, 'f1': 0.846311475409836, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.6781914893617021, 'recall': 0.6948228882833788, 'f1': 0.686406460296097, 'number': 367}, 'eval_POSITION': {'precision': 0.631484794275492, 'recall': 0.815242494226328, 'f1': 0.7116935483870969, 'number': 433}, 'eval_SCENE': {'precision': 0.6335877862595419, 'recall': 0.7942583732057417, 'f1': 0.7048832271762208, 'number': 209}, 'eval_overall_precision': 0.6621584322384764, 'eval_overall_recall': 0.7809244791666666, 'eval_overall_f1': 0.7166542195668409, 'eval_overall_accuracy': 0.9948692293373046, 'eval_runtime': 58.2559, 'eval_samples_per_second': 23.053, 'eval_steps_per_second': 0.721, 'epoch': 1.0}\n 50%|████████████████████▌                    | 336/672 [22:27<21:05,  3.76s/it]\n100%|███████████████████████████████████████████| 42/42 [00:57<00:00,  1.18s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.01719084568321705, 'eval_ADDRESS': {'precision': 0.5074309978768577, 'recall': 0.6407506702412868, 'f1': 0.566350710900474, 'number': 373}, 'eval_BOOK': {'precision': 0.5776699029126213, 'recall': 0.7727272727272727, 'f1': 0.6611111111111111, 'number': 154}, 'eval_COMPANY': {'precision': 0.6532438478747203, 'recall': 0.7724867724867724, 'f1': 0.7078787878787879, 'number': 378}, 'eval_GAME': {'precision': 0.7167630057803468, 'recall': 0.8406779661016949, 'f1': 0.7737909516380655, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6476190476190476, 'recall': 0.8259109311740891, 'f1': 0.7259786476868327, 'number': 247}, 'eval_MOVIE': {'precision': 0.7664233576642335, 'recall': 0.695364238410596, 'f1': 0.7291666666666665, 'number': 151}, 'eval_NAME': {'precision': 0.8174603174603174, 'recall': 0.886021505376344, 'f1': 0.8503611971104231, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.6658097686375322, 'recall': 0.7057220708446866, 'f1': 0.6851851851851851, 'number': 367}, 'eval_POSITION': {'precision': 0.625886524822695, 'recall': 0.815242494226328, 'f1': 0.7081243731193582, 'number': 433}, 'eval_SCENE': {'precision': 0.630901287553648, 'recall': 0.7033492822966507, 'f1': 0.665158371040724, 'number': 209}, 'eval_overall_precision': 0.6583610188261351, 'eval_overall_recall': 0.7740885416666666, 'eval_overall_f1': 0.7115499700777977, 'eval_overall_accuracy': 0.9947965143335815, 'eval_runtime': 59.4099, 'eval_samples_per_second': 22.606, 'eval_steps_per_second': 0.707, 'epoch': 1.0}\n 50%|████████████████████▌                    | 336/672 [22:28<21:05,  3.77s/it]\n100%|███████████████████████████████████████████| 42/42 [00:58<00:00,  1.09s/it]\u001b[A\n{'loss': 0.1165, 'grad_norm': 11321.45703125, 'learning_rate': 1.5122377622377622e-05, 'epoch': 1.49}\n{'loss': 0.1161, 'grad_norm': 13074.662109375, 'learning_rate': 1.5122377622377622e-05, 'epoch': 1.49}\n 74%|██████████████████████████████▌          | 500/672 [32:55<09:13,  3.22s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n 75%|██████████████████████████████▌          | 501/672 [32:57<11:05,  3.89s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n100%|████████████████████████████████████████▉| 671/672 [43:52<00:03,  3.20s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n\n  0%|                                                    | 0/42 [00:00<?, ?it/s]\u001b[A\n100%|█████████████████████████████████████████| 672/672 [43:55<00:00,  3.14s/it]\u001b[A\n  7%|███▏                                        | 3/42 [00:01<00:26,  1.49it/s]\u001b[A\n 10%|████▏                                       | 4/42 [00:02<00:24,  1.53it/s]\u001b[A\n 12%|█████▏                                      | 5/42 [00:03<00:23,  1.55it/s]\u001b[A\n 14%|██████▎                                     | 6/42 [00:03<00:22,  1.57it/s]\u001b[A\n 17%|███████▎                                    | 7/42 [00:04<00:22,  1.57it/s]\u001b[A\n 19%|████████▍                                   | 8/42 [00:05<00:21,  1.58it/s]\u001b[A/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n\n 21%|█████████▍                                  | 9/42 [00:06<00:24,  1.34it/s]\u001b[A\n  0%|                                                    | 0/42 [00:00<?, ?it/s]\u001b[A\n 24%|██████████▏                                | 10/42 [00:07<00:28,  1.12it/s]\u001b[A\n  5%|██                                          | 2/42 [00:01<00:23,  1.67it/s]\u001b[A\n 26%|███████████▎                               | 11/42 [00:08<00:30,  1.01it/s]\u001b[A\n  7%|███▏                                        | 3/42 [00:02<00:33,  1.16it/s]\u001b[A\n 29%|████████████▎                              | 12/42 [00:09<00:31,  1.06s/it]\u001b[A\n 10%|████▏                                       | 4/42 [00:03<00:37,  1.00it/s]\u001b[A\n 31%|█████████████▎                             | 13/42 [00:10<00:31,  1.09s/it]\u001b[A\n 12%|█████▏                                      | 5/42 [00:04<00:40,  1.08s/it]\u001b[A\n 33%|██████████████▎                            | 14/42 [00:12<00:31,  1.13s/it]\u001b[A\n 14%|██████▎                                     | 6/42 [00:06<00:40,  1.13s/it]\u001b[A\n 36%|███████████████▎                           | 15/42 [00:13<00:30,  1.15s/it]\u001b[A\n 17%|███████▎                                    | 7/42 [00:07<00:40,  1.16s/it]\u001b[A\n 38%|████████████████▍                          | 16/42 [00:14<00:30,  1.16s/it]\u001b[A\n 19%|████████▍                                   | 8/42 [00:08<00:40,  1.18s/it]\u001b[A\n 40%|█████████████████▍                         | 17/42 [00:15<00:29,  1.18s/it]\u001b[A\n 21%|█████████▍                                  | 9/42 [00:09<00:39,  1.19s/it]\u001b[A\n 43%|██████████████████▍                        | 18/42 [00:16<00:28,  1.18s/it]\u001b[A\n 24%|██████████▏                                | 10/42 [00:10<00:38,  1.19s/it]\u001b[A\n 45%|███████████████████▍                       | 19/42 [00:18<00:27,  1.19s/it]\u001b[A\n 26%|███████████▎                               | 11/42 [00:12<00:37,  1.20s/it]\u001b[A\n 48%|████████████████████▍                      | 20/42 [00:19<00:26,  1.19s/it]\u001b[A\n 29%|████████████▎                              | 12/42 [00:13<00:35,  1.20s/it]\u001b[A\n 50%|█████████████████████▌                     | 21/42 [00:20<00:25,  1.19s/it]\u001b[A\n 31%|█████████████▎                             | 13/42 [00:14<00:34,  1.20s/it]\u001b[A\n 52%|██████████████████████▌                    | 22/42 [00:21<00:23,  1.20s/it]\u001b[A\n 33%|██████████████▎                            | 14/42 [00:15<00:33,  1.20s/it]\u001b[A\n 55%|███████████████████████▌                   | 23/42 [00:22<00:22,  1.20s/it]\u001b[A\n 36%|███████████████▎                           | 15/42 [00:16<00:32,  1.20s/it]\u001b[A\n 57%|████████████████████████▌                  | 24/42 [00:24<00:21,  1.20s/it]\u001b[A\n 38%|████████████████▍                          | 16/42 [00:18<00:31,  1.20s/it]\u001b[A\n 60%|█████████████████████████▌                 | 25/42 [00:25<00:20,  1.20s/it]\u001b[A\n 40%|█████████████████▍                         | 17/42 [00:19<00:30,  1.20s/it]\u001b[A\n 62%|██████████████████████████▌                | 26/42 [00:26<00:19,  1.21s/it]\u001b[A\n 43%|██████████████████▍                        | 18/42 [00:20<00:28,  1.21s/it]\u001b[A\n 64%|███████████████████████████▋               | 27/42 [00:27<00:18,  1.21s/it]\u001b[A\n 45%|███████████████████▍                       | 19/42 [00:21<00:27,  1.21s/it]\u001b[A\n 67%|████████████████████████████▋              | 28/42 [00:28<00:16,  1.21s/it]\u001b[A\n 48%|████████████████████▍                      | 20/42 [00:23<00:26,  1.21s/it]\u001b[A\n 69%|█████████████████████████████▋             | 29/42 [00:30<00:15,  1.20s/it]\u001b[A\n 50%|█████████████████████▌                     | 21/42 [00:24<00:25,  1.21s/it]\u001b[A\n 71%|██████████████████████████████▋            | 30/42 [00:31<00:14,  1.20s/it]\u001b[A\n 52%|██████████████████████▌                    | 22/42 [00:25<00:23,  1.20s/it]\u001b[A\n 74%|███████████████████████████████▋           | 31/42 [00:32<00:13,  1.21s/it]\u001b[A\n 55%|███████████████████████▌                   | 23/42 [00:26<00:22,  1.19s/it]\u001b[A\n 76%|████████████████████████████████▊          | 32/42 [00:33<00:12,  1.20s/it]\u001b[A\n 57%|████████████████████████▌                  | 24/42 [00:27<00:21,  1.21s/it]\u001b[A\n 79%|█████████████████████████████████▊         | 33/42 [00:35<00:10,  1.20s/it]\u001b[A\n 60%|█████████████████████████▌                 | 25/42 [00:29<00:20,  1.21s/it]\u001b[A\n 81%|██████████████████████████████████▊        | 34/42 [00:36<00:09,  1.20s/it]\u001b[A\n 62%|██████████████████████████▌                | 26/42 [00:30<00:19,  1.21s/it]\u001b[A\n 83%|███████████████████████████████████▊       | 35/42 [00:37<00:08,  1.20s/it]\u001b[A\n 64%|███████████████████████████▋               | 27/42 [00:31<00:18,  1.21s/it]\u001b[A\n 86%|████████████████████████████████████▊      | 36/42 [00:38<00:07,  1.20s/it]\u001b[A\n 67%|████████████████████████████▋              | 28/42 [00:32<00:16,  1.21s/it]\u001b[A\n 88%|█████████████████████████████████████▉     | 37/42 [00:39<00:06,  1.20s/it]\u001b[A\n 69%|█████████████████████████████▋             | 29/42 [00:33<00:15,  1.20s/it]\u001b[A\n 90%|██████████████████████████████████████▉    | 38/42 [00:41<00:04,  1.20s/it]\u001b[A\n 71%|██████████████████████████████▋            | 30/42 [00:35<00:14,  1.20s/it]\u001b[A\n 93%|███████████████████████████████████████▉   | 39/42 [00:42<00:03,  1.20s/it]\u001b[A\n 74%|███████████████████████████████▋           | 31/42 [00:36<00:13,  1.20s/it]\u001b[A\n 95%|████████████████████████████████████████▉  | 40/42 [00:43<00:02,  1.20s/it]\u001b[A\n 76%|████████████████████████████████▊          | 32/42 [00:37<00:12,  1.20s/it]\u001b[A\n 98%|█████████████████████████████████████████▉ | 41/42 [00:44<00:01,  1.20s/it]\u001b[A\n 79%|█████████████████████████████████▊         | 33/42 [00:38<00:10,  1.21s/it]\u001b[A\n100%|███████████████████████████████████████████| 42/42 [00:45<00:00,  1.16s/it]\u001b[A\n 81%|██████████████████████████████████▊        | 34/42 [00:39<00:08,  1.10s/it]\u001b[A\n 83%|███████████████████████████████████▊       | 35/42 [00:40<00:06,  1.05it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 36/42 [00:40<00:05,  1.18it/s]\u001b[A\n 88%|█████████████████████████████████████▉     | 37/42 [00:41<00:03,  1.29it/s]\u001b[A\n 90%|██████████████████████████████████████▉    | 38/42 [00:41<00:02,  1.38it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 39/42 [00:42<00:02,  1.44it/s]\u001b[A\n 95%|████████████████████████████████████████▉  | 40/42 [00:43<00:01,  1.50it/s]\u001b[A\n 98%|█████████████████████████████████████████▉ | 41/42 [00:43<00:00,  1.54it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.015209793113172054, 'eval_ADDRESS': {'precision': 0.5650118203309693, 'recall': 0.6407506702412868, 'f1': 0.6005025125628141, 'number': 373}, 'eval_BOOK': {'precision': 0.7034883720930233, 'recall': 0.7857142857142857, 'f1': 0.7423312883435583, 'number': 154}, 'eval_COMPANY': {'precision': 0.7268292682926829, 'recall': 0.7883597883597884, 'f1': 0.7563451776649747, 'number': 378}, 'eval_GAME': {'precision': 0.7431192660550459, 'recall': 0.823728813559322, 'f1': 0.7813504823151126, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6966666666666667, 'recall': 0.8461538461538461, 'f1': 0.7641681901279707, 'number': 247}, 'eval_MOVIE': {'precision': 0.7469135802469136, 'recall': 0.8013245033112583, 'f1': 0.7731629392971245, 'number': 151}, 'eval_NAME': {'precision': 0.8329938900203666, 'recall': 0.8795698924731182, 'f1': 0.8556485355648534, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.7024390243902439, 'recall': 0.784741144414169, 'f1': 0.7413127413127414, 'number': 367}, 'eval_POSITION': {'precision': 0.6863905325443787, 'recall': 0.8036951501154734, 'f1': 0.7404255319148936, 'number': 433}, 'eval_SCENE': {'precision': 0.6414342629482072, 'recall': 0.7703349282296651, 'f1': 0.7000000000000001, 'number': 209}, 'eval_overall_precision': 0.7057631045467709, 'eval_overall_recall': 0.7932942708333334, 'eval_overall_f1': 0.7469731800766283, 'eval_overall_accuracy': 0.9953927773641102, 'eval_runtime': 54.076, 'eval_samples_per_second': 24.835, 'eval_steps_per_second': 0.777, 'epoch': 2.0}\n100%|█████████████████████████████████████████| 672/672 [44:46<00:00,  3.69s/it]\n100%|███████████████████████████████████████████| 42/42 [00:52<00:00,  1.16s/it]\u001b[A\n{'train_runtime': 2686.9192, 'train_samples_per_second': 8.0, 'train_steps_per_second': 0.25, 'train_loss': 0.08985399987016406, 'epoch': 2.0}\n100%|█████████████████████████████████████████| 672/672 [44:46<00:00,  4.00s/it]\n                                                                                \n\u001b[A{'eval_loss': 0.015125115402042866, 'eval_ADDRESS': {'precision': 0.557919621749409, 'recall': 0.6327077747989276, 'f1': 0.592964824120603, 'number': 373}, 'eval_BOOK': {'precision': 0.7735849056603774, 'recall': 0.7987012987012987, 'f1': 0.7859424920127797, 'number': 154}, 'eval_COMPANY': {'precision': 0.7197149643705463, 'recall': 0.8015873015873016, 'f1': 0.7584480600750938, 'number': 378}, 'eval_GAME': {'precision': 0.746177370030581, 'recall': 0.8271186440677966, 'f1': 0.7845659163987139, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6920529801324503, 'recall': 0.8461538461538461, 'f1': 0.761384335154827, 'number': 247}, 'eval_MOVIE': {'precision': 0.7857142857142857, 'recall': 0.8013245033112583, 'f1': 0.7934426229508197, 'number': 151}, 'eval_NAME': {'precision': 0.8221343873517787, 'recall': 0.8946236559139785, 'f1': 0.8568486096807416, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.7070707070707071, 'recall': 0.7629427792915532, 'f1': 0.7339449541284404, 'number': 367}, 'eval_POSITION': {'precision': 0.7108433734939759, 'recall': 0.8175519630484989, 'f1': 0.7604726100966702, 'number': 433}, 'eval_SCENE': {'precision': 0.6428571428571429, 'recall': 0.7751196172248804, 'f1': 0.7028199566160521, 'number': 209}, 'eval_overall_precision': 0.7120418848167539, 'eval_overall_recall': 0.796875, 'eval_overall_f1': 0.7520737327188941, 'eval_overall_accuracy': 0.9955003955696202, 'eval_runtime': 53.7968, 'eval_samples_per_second': 24.964, 'eval_steps_per_second': 0.781, 'epoch': 2.0}\n100%|█████████████████████████████████████████| 672/672 [44:53<00:00,  3.14s/it]\n100%|███████████████████████████████████████████| 42/42 [00:52<00:00,  1.62it/s]\u001b[A\n{'train_runtime': 2693.4106, 'train_samples_per_second': 7.981, 'train_steps_per_second': 0.249, 'train_loss': 0.08958166092634201, 'epoch': 2.0}\n100%|█████████████████████████████████████████| 672/672 [44:53<00:00,  4.01s/it]\n[rank0]:[W613 11:04:03.907930877 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## 加载模型并推理","metadata":{}},{"cell_type":"code","source":"eval_model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                                num_labels=len(tags),\n                                                                id2label=id2lbl,\n                                                                label2id=lbl2id)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\neval_model = eval_model.to(device)\n\nstate_dict = torch.load('/kaggle/working/model.bin', map_location=device)\n\neval_model.load_state_dict(state_dict)\n\neval_model.eval()\n\ndef predict_entities(text):\n\n    batch_texts = [list(text)]\n\n    # 预处理输入文本\n    input_data = tokenizer(\n        batch_texts, \n        truncation=True, \n        add_special_tokens=False, \n        max_length=512,\n        is_split_into_words=True, \n        padding='max_length',\n        return_tensors=\"pt\"  \n    )\n\n    for key in input_data:\n        if isinstance(input_data[key], torch.Tensor):\n            input_data[key] = input_data[key].to(device)\n\n    # 运行模型推理\n    with torch.no_grad():\n        outputs = eval_model(**input_data)\n\n    # 获取预测结果\n    predictions = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n\n    # 获取分词后的 token\n    tokens = tokenizer.convert_ids_to_tokens(input_data[\"input_ids\"].squeeze().tolist())\n\n    # 后处理输出，提取实体\n    entities = []\n    current_entity = None\n \n    for i, (token, pred) in enumerate(zip(tokens, predictions)):\n        label = id2lbl[pred]\n        # if label is not 'O':\n        #     print(i, label)\n \n        if label.startswith(\"B-\"):\n            if current_entity is not None:\n                entities.append(current_entity)\n            current_entity = {\"entity\": label[2:], \"content\": token}\n        elif label.startswith(\"I-\") and current_entity is not None and label[2:] == current_entity[\"entity\"]:\n            current_entity[\"content\"] += token\n        else:\n            if current_entity is not None:\n                entities.append(current_entity)\n                current_entity = None\n \n    # 添加最后一个实体（如果有）\n    if current_entity is not None:\n        entities.append(current_entity)\n\n    for entity in entities:\n        entity[\"entity\"] = entity[\"entity\"].lower()\n\n    return entities\n\n# 示例调用\ntext = \"我最近迷上了《星际穿越》这部电影，它的导演是克里斯托弗·诺兰，真的是一位才华横溢的导演。\\\n    周末我打算约上几个朋友，一起去位于上海市徐汇区的腾讯大厦附近的电竞馆，玩几局《英雄联盟》。\\\n    听说腾讯最近在深圳总部举办了一场盛大的游戏开发者大会，吸引了全球众多游戏爱好者的关注。\\\n    另外，我还在读一本关于人工智能的书籍，书名是《人工智能简史》，作者是尼克·波斯特洛姆，\\\n    他是未来生命研究所的创始人之一，这个组织一直致力于研究人工智能对人类社会的影响。\"\nentities = predict_entities(text)\n\nfor entity in entities:\n    print(entity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T11:51:54.515803Z","iopub.execute_input":"2025-06-13T11:51:54.516109Z","iopub.status.idle":"2025-06-13T11:51:55.109850Z","shell.execute_reply.started":"2025-06-13T11:51:54.516084Z","shell.execute_reply":"2025-06-13T11:51:55.108883Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'entity': 'position', 'content': '《星际穿越》'}\n{'entity': 'organization', 'content': '导演'}\n{'entity': 'movie', 'content': '克里斯托弗·诺兰'}\n{'entity': 'organization', 'content': '导演'}\n{'entity': 'name', 'content': '上海市徐汇区'}\n{'entity': 'name', 'content': '腾讯大厦'}\n{'entity': 'book', 'content': '《英雄联盟》'}\n{'entity': 'scene', 'content': '腾讯'}\n{'entity': 'name', 'content': '深圳'}\n{'entity': 'government', 'content': '《人工智能简史》'}\n{'entity': 'organization', 'content': '作者'}\n{'entity': 'movie', 'content': '尼克·波斯特洛姆'}\n{'entity': 'organization', 'content': '创始人'}\n","output_type":"stream"}],"execution_count":66}]}