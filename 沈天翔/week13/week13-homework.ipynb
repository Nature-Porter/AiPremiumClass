{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:42:12.460499Z","iopub.execute_input":"2025-06-19T15:42:12.460834Z","iopub.status.idle":"2025-06-19T15:42:12.800519Z","shell.execute_reply.started":"2025-06-19T15:42:12.460786Z","shell.execute_reply":"2025-06-19T15:42:12.799388Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import pipeline, set_seed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:42:12.802531Z","iopub.execute_input":"2025-06-19T15:42:12.803090Z","iopub.status.idle":"2025-06-19T15:42:45.862069Z","shell.execute_reply.started":"2025-06-19T15:42:12.803057Z","shell.execute_reply":"2025-06-19T15:42:45.861262Z"}},"outputs":[{"name":"stderr","text":"2025-06-19 15:42:25.096225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750347745.336606      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750347745.402581      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 本地化调⽤","metadata":{}},{"cell_type":"code","source":"generator = pipeline('text-generation', model='gpt2')\nset_seed(33)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:42:45.862900Z","iopub.execute_input":"2025-06-19T15:42:45.863462Z","iopub.status.idle":"2025-06-19T15:42:52.377052Z","shell.execute_reply.started":"2025-06-19T15:42:45.863431Z","shell.execute_reply":"2025-06-19T15:42:52.376201Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89873052e52e4bc4b9e5d34f90e16400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"585c563701ad4cc2b5069b8ba272a650"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0506b5323c4a7e8670cf859da54749"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11211d80ca0a43d8966ea0c498f88dd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a529c16d94e4019867ee177aa77198e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0c40b16b974a659f77f8610577b55a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b51ca0a57d04692bc506385ac381939"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"texts = generator(\"人生得意需尽欢\", max_length=50, num_return_sequences=5)\n\nfor text in texts:\n    print(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:42:52.379099Z","iopub.execute_input":"2025-06-19T15:42:52.379373Z","iopub.status.idle":"2025-06-19T15:42:55.613829Z","shell.execute_reply.started":"2025-06-19T15:42:52.379353Z","shell.execute_reply":"2025-06-19T15:42:55.612846Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"{'generated_text': '人生得意需尽欢展逝俠暗情行善 受扺己论其相取り�'}\n{'generated_text': '人生得意需尽欢 着手念長,最強站用輪重要邵技挑�'}\n{'generated_text': '人生得意需尽欢銀和頭蒼曷考沘化家的旍効。\\n\\n这些'}\n{'generated_text': '人生得意需尽欢長切,雄也取们那么茶不乎不好心忍可能�'}\n{'generated_text': '人生得意需尽欢守。 哗到答。 以是好殺。 以是好殺接的土'}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 远程调用","metadata":{}},{"cell_type":"code","source":"%%writefile app_test.py\n\nfrom transformers import pipeline, set_seed\nfrom flask import Flask, request, jsonify\nimport threading\nimport requests\nimport json\nimport time\n\napp = Flask(__name__)\n\nprint(\"正在加载模型...\")\ngenerator = pipeline('text-generation', model='gpt2')\nset_seed(33)\nprint(\"模型加载完成！\")\n\n@app.route('/api_test', methods=['POST'])\ndef api_test():\n    data = request.get_json()\n    text = data.get('text', '')\n    texts = generator(\n        text,\n        max_length=200,\n        num_return_sequences=1,\n        # temperature=0.7\n    )\n    return jsonify(texts)\n\ndef run_flask():\n    app.run(host='0.0.0.0', port=8080, debug=False, use_reloader=False)\n\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.daemon = True\nflask_thread.start()\n\nprint(\"等待服务器启动...\")\ntime.sleep(5)\nprint(\"Flask 服务器应在后台运行\")\n\nurl = \"http://localhost:8080/api_test\"\ndata = {\"text\": \"人生得意须尽欢\"}\n\ntry:\n    print(\"发送请求到服务器...\")\n    response = requests.post(url, json=data, timeout=30)\n    if response.status_code == 200:\n        print(\"\\n请求成功！服务器返回的数据：\")\n        data = response.json()\n        print(data[0]['generated_text'])\n    else:\n        print(f\"\\n请求失败，状态码：{response.status_code}\")\n        print(\"响应内容:\", response.text)\nexcept requests.exceptions.RequestException as e:\n    print(\"\\n请求发生异常：\", e)\n\n# try:\n#     while True:\n#         time.sleep(1)\n# except KeyboardInterrupt:\n#     print(\"\\n程序退出\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:42:55.614929Z","iopub.execute_input":"2025-06-19T15:42:55.615599Z","iopub.status.idle":"2025-06-19T15:42:55.623044Z","shell.execute_reply.started":"2025-06-19T15:42:55.615573Z","shell.execute_reply":"2025-06-19T15:42:55.621877Z"}},"outputs":[{"name":"stdout","text":"Writing app_test.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"! python app_test.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:42:55.625406Z","iopub.execute_input":"2025-06-19T15:42:55.626474Z","iopub.status.idle":"2025-06-19T15:43:23.782761Z","shell.execute_reply.started":"2025-06-19T15:42:55.626430Z","shell.execute_reply":"2025-06-19T15:43:23.781400Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"2025-06-19 15:43:00.851588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750347780.880539      99 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750347780.888673      99 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n正在加载模型...\nDevice set to use cpu\n模型加载完成！\n等待服务器启动...\n * Serving Flask app 'app_test'\n * Debug mode: off\nFlask 服务器应在后台运行\n发送请求到服务器...\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n请求成功！服务器返回的数据：\n人生得意须尽欢展逝俠暗情行型 受扺己访不论取慈。\n\n离没目稌座\n\n判代点防绝箤为玩洁照\n\n绫懑譯此型那着商。\n\n让懑譯預外露\n\n型己戻院元弯枕此贝如\n\n毎雙算不和\n\n俠湖瑤\n\n大向我先覕\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## OpenAI API 调用","metadata":{}},{"cell_type":"code","source":"! pip install -q --upgrade openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:43:23.784213Z","iopub.execute_input":"2025-06-19T15:43:23.784584Z","iopub.status.idle":"2025-06-19T15:43:31.377681Z","shell.execute_reply.started":"2025-06-19T15:43:23.784540Z","shell.execute_reply":"2025-06-19T15:43:31.375291Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.3/734.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"API_KEY = '5dfc039643a64e62aaf7a1ee5135bb9f.9LaF7US056rglgd0'\nBASE_URL = 'https://open.bigmodel.cn/api/paas/v4'\nMODEL_NAME = 'glm-4-flash-250414'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:56:27.878090Z","iopub.execute_input":"2025-06-19T15:56:27.878444Z","iopub.status.idle":"2025-06-19T15:56:27.883284Z","shell.execute_reply.started":"2025-06-19T15:56:27.878416Z","shell.execute_reply":"2025-06-19T15:56:27.882119Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from openai import OpenAI\n\n# 创建调用客户端\nclient = OpenAI(\n    api_key=API_KEY,\n    base_url=BASE_URL\n)\n\n# chat模式调用模型\nresponse1 = client.chat.completions.create(\n    # 模型名称\n    model = MODEL_NAME,\n    # 消息列表\n    messages=[  # 聊天历史信息\n        {'role':'system', 'content':'你是一个擅长与人聊天的AI助手'},\n        {'role':'user', 'content':'我是迈克，你好,我有一个帽衫, 我要在网上问问'},\n        {'role':'assistant', 'content':'他们说长得像谢弗涅'},\n        {'role':'user', 'content':'请问我是谁？'}\n    ],\n    # 模型参数\n    temperature=0,\n    # 最大token数\n    max_tokens=500,\n)\n\n# 打印结果\nprint(response1.choices[0].message.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:56:30.781848Z","iopub.execute_input":"2025-06-19T15:56:30.782239Z","iopub.status.idle":"2025-06-19T15:56:32.407947Z","shell.execute_reply.started":"2025-06-19T15:56:30.782210Z","shell.execute_reply":"2025-06-19T15:56:32.406986Z"}},"outputs":[{"name":"stdout","text":"你是迈克，你有一个帽衫，你想要在网上问问关于它的事情。\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"response2 = client.chat.completions.create(\n    # 模型名称\n    model = MODEL_NAME,\n    # 消息列表\n    messages=[  # 聊天历史信息\n        {'role':'system', 'content':'你是一个擅长与人聊天的AI助手'},\n        {'role':'user', 'content':'我是迈克，你好,我有一个帽衫, 我要在网上问问'},\n        {'role':'assistant', 'content':'他们说长得像谢弗涅'},\n        {'role':'user', 'content':'请问我是谁？'}\n    ],\n    # 模型参数\n    temperature=0.75,\n    # 最大token数\n    max_tokens=500,\n)\n\n# 打印结果\nprint(response2.choices[0].message.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:56:36.714639Z","iopub.execute_input":"2025-06-19T15:56:36.714973Z","iopub.status.idle":"2025-06-19T15:56:37.991851Z","shell.execute_reply.started":"2025-06-19T15:56:36.714949Z","shell.execute_reply":"2025-06-19T15:56:37.990864Z"}},"outputs":[{"name":"stdout","text":"你好迈克！很高兴认识你。你提到你有一个帽衫，并且想知道别人会怎么形容它，对吗？如果你需要帮助来描述你的帽衫或者想了解它的风格，我可以帮忙的。\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## 智能图书管理AI","metadata":{}},{"cell_type":"code","source":"# chat模式调用模型\nresponse3 = client.chat.completions.create(\n    # 模型名称\n    model = MODEL_NAME,\n    # 消息列表\n    messages=[  # 聊天历史信息\n        {\n            'role':'system', \n            'content':'你是一个智能图书管理AI，负责帮助用户借阅和归还图书。请完成以下操作：\\\n                           你亲自查询图书状态，登记借阅（默认7天）或归还（检查逾期）。若图书不可借，提示可借时间。\\\n                       最后你根据用户历史借阅、推断出用户偏好，你来推荐 3-5本相似书籍，并给出具体的书名，不用询问借阅人。\\\n                       300字左右'\n        },\n        {\n            'role':'user', \n            'content':'借一本《三国演义》。'\n        }\n    ],\n    # 模型参数\n    temperature=0.75,\n    # 最大token数\n    max_tokens=500,\n)\n\nprint(response3.choices[0].message.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T16:12:03.129672Z","iopub.execute_input":"2025-06-19T16:12:03.130070Z","iopub.status.idle":"2025-06-19T16:12:10.695773Z","shell.execute_reply.started":"2025-06-19T16:12:03.130045Z","shell.execute_reply":"2025-06-19T16:12:10.694504Z"}},"outputs":[{"name":"stdout","text":"好的，请稍等，我马上为您查询《三国演义》的状态。\n\n查询结果显示，《三国演义》目前有2本可借。请问您需要借阅哪一本？是A本还是B本？\n\n（假设用户选择了A本）\n\n好的，您已成功借阅《三国演义》A本，借阅期限为7天。请在7天内归还，以免产生逾期费用。归还时请将书籍带到服务台，我们会为您办理归还手续。\n\n根据您的历史借阅记录，您对历史、文学类书籍有较高的兴趣。以下是我为您推荐的几本相似书籍：\n\n1. 《水浒传》：一部描写北宋末年农民起义的长篇小说，与《三国演义》同为中国四大名著之一。\n2. 《红楼梦》：一部描写清代贵族家庭生活的长篇小说，被誉为中国古典小说的巅峰之作。\n3. 《西游记》：一部描写唐僧取经的长篇神话小说，充满了奇幻色彩和深刻寓意。\n\n希望您会喜欢这些推荐！如果您还有其他需求，请随时告诉我。\n","output_type":"stream"}],"execution_count":37}]}