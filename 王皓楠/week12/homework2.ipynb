{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:16.763777Z",
     "iopub.status.busy": "2025-06-13T22:57:16.763571Z",
     "iopub.status.idle": "2025-06-13T22:57:24.506656Z",
     "shell.execute_reply": "2025-06-13T22:57:24.505713Z",
     "shell.execute_reply.started": "2025-06-13T22:57:16.763758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (E:\\anaconda\\envs\\py311\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\argon2_cffi-23.1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\argon2_cffi_bindings-21.2.0-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\arrow-1.3.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\async_lru-2.0.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\attrs-25.2.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\babel-2.17.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\beautifulsoup4-4.13.3-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\bleach-6.2.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\certifi-2025.1.31-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\cffi-1.17.1-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\charset_normalizer-3.4.1-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\d2l-1.0.3-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\defusedxml-0.8.0rc2-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\fastjsonschema-2.21.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\fqdn-1.5.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\httpcore-0.13.7-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\httpx-1.0.0b0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\idna-3.10-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\ipywidgets-8.1.5-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\isoduration-20.11.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\json5-0.10.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jsonpointer-3.0.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jsonschema-4.23.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jsonschema_specifications-2024.10.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyter-1.0.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyterlab-4.4.0b0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyterlab_pygments-0.3.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyterlab_server-2.27.3-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyterlab_widgets-3.0.13-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyter_console-6.6.3-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyter_events-0.12.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyter_lsp-2.2.5-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyter_server-2.15.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\jupyter_server_terminals-0.5.3-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\matplotlib-3.8.2-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\matplotlib_inline-0.1.6-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\mistune-3.1.2-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\nbclient-0.10.2-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\nbconvert-7.16.6-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\nbformat-5.10.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\notebook-7.4.0b0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\notebook_shim-0.2.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\overrides-7.7.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\pandas-2.2.0-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\pandocfilters-1.5.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\prometheus_client-0.21.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\pycparser-2.22-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\python_json_logger-3.3.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\pywinpty-2.0.15-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\pyyaml-6.0.2-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\qtconsole-5.6.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\qtpy-2.4.3-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\referencing-0.36.2-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\requests-2.31.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\rfc3339_validator-0.1.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\rfc3986-1.5.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\rfc3986_validator-0.1.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\rpds_py-0.23.1-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\scipy-1.12.0-py3.11-win-amd64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\send2trash-1.8.3-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\sniffio-1.3.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\soupsieve-2.6-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\terminado-0.18.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\tinycss2-1.4.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\types_python_dateutil-2.9.0.20241206-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\uri_template-1.3.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\urllib3-2.3.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\webcolors-24.11.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\webencodings-0.5.1-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\websocket_client-1.8.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at e:\\anaconda\\envs\\py311\\lib\\site-packages\\widgetsnbextension-4.0.13-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~orch (E:\\anaconda\\envs\\py311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (E:\\anaconda\\envs\\py311\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "d2l 1.0.3 requires requests==2.31.0, but you have requests 2.32.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip -q install evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:24.509235Z",
     "iopub.status.busy": "2025-06-13T22:57:24.508681Z",
     "iopub.status.idle": "2025-06-13T22:57:52.440114Z",
     "shell.execute_reply": "2025-06-13T22:57:52.439508Z",
     "shell.execute_reply.started": "2025-06-13T22:57:24.509209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导入成功！\n"
     ]
    }
   ],
   "source": [
    "from fsspec.core import url_to_fs  # 测试 fsspec 核心模块\n",
    "from datasets import Dataset        # 测试 datasets 库\n",
    "print(\"导入成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:52.441111Z",
     "iopub.status.busy": "2025-06-13T22:57:52.440682Z",
     "iopub.status.idle": "2025-06-13T22:57:54.966247Z",
     "shell.execute_reply": "2025-06-13T22:57:54.965658Z",
     "shell.execute_reply.started": "2025-06-13T22:57:52.441093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d46054c75048d7a91631de9c2c915e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d7c31db11f4fbea0b0df287d6a2cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:54.967263Z",
     "iopub.status.busy": "2025-06-13T22:57:54.966977Z",
     "iopub.status.idle": "2025-06-13T22:57:54.973594Z",
     "shell.execute_reply": "2025-06-13T22:57:54.972807Z",
     "shell.execute_reply.started": "2025-06-13T22:57:54.967243Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:54.975936Z",
     "iopub.status.busy": "2025-06-13T22:57:54.975719Z",
     "iopub.status.idle": "2025-06-13T22:57:55.514662Z",
     "shell.execute_reply": "2025-06-13T22:57:55.514120Z",
     "shell.execute_reply.started": "2025-06-13T22:57:54.975919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086a93e5c87d4f4a9522baf94d10cf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1980c8ec57e14321b1206428e75ab972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d746fbf01e04a73804c21ac600a5542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:55.515403Z",
     "iopub.status.busy": "2025-06-13T22:57:55.515218Z",
     "iopub.status.idle": "2025-06-13T22:57:55.518919Z",
     "shell.execute_reply": "2025-06-13T22:57:55.518080Z",
     "shell.execute_reply.started": "2025-06-13T22:57:55.515388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "# message= \"命名实体识别\"\n",
    "# label = torch.tensor([0,1,0,2,5,4])\n",
    "\n",
    "# model_input = tokenizer([message], return_tensors='pt')\n",
    "# result = model(**model_input)\n",
    "\n",
    "# print(result.loss)\n",
    "# print(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:55.519955Z",
     "iopub.status.busy": "2025-06-13T22:57:55.519696Z",
     "iopub.status.idle": "2025-06-13T22:57:57.326134Z",
     "shell.execute_reply": "2025-06-13T22:57:57.325504Z",
     "shell.execute_reply.started": "2025-06-13T22:57:55.519930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60891c9b3f594bcab6185a4316dafb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\py311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\datasets--nlhappy--CLUE-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2ac64cfe214512a16d6cee00d82710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11f9032d32b42eeb86e9c44b1b466d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-a33d0e4276aef9b4.parquet:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd83e0e2dbc47bc9c79c478cd1bad1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-07f476b71c5edde6.parquet:   0%|          | 0.00/178k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4cf8b34f24494e888674922a32ab9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636ade3113d9441a84034a870a64f660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载hf中dataset\n",
    "ds = load_dataset('nlhappy/CLUE-NER')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实体映射数据集词典准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:57.327052Z",
     "iopub.status.busy": "2025-06-13T22:57:57.326816Z",
     "iopub.status.idle": "2025-06-13T22:57:57.331929Z",
     "shell.execute_reply": "2025-06-13T22:57:57.331260Z",
     "shell.execute_reply.started": "2025-06-13T22:57:57.327021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "           'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:57.333105Z",
     "iopub.status.busy": "2025-06-13T22:57:57.332793Z",
     "iopub.status.idle": "2025-06-13T22:57:57.559809Z",
     "shell.execute_reply": "2025-06-13T22:57:57.559062Z",
     "shell.execute_reply.started": "2025-06-13T22:57:57.333079Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'scene': 1,\n",
       " 'organization': 2,\n",
       " 'name': 3,\n",
       " 'government': 4,\n",
       " 'game': 5,\n",
       " 'company': 6,\n",
       " 'address': 7,\n",
       " 'book': 8,\n",
       " 'position': 9,\n",
       " 'movie': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:57.560885Z",
     "iopub.status.busy": "2025-06-13T22:57:57.560617Z",
     "iopub.status.idle": "2025-06-13T22:57:57.571868Z",
     "shell.execute_reply": "2025-06-13T22:57:57.571261Z",
     "shell.execute_reply.started": "2025-06-13T22:57:57.560868Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-SCENE',\n",
       " 'I-SCENE',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-NAME',\n",
       " 'I-NAME',\n",
       " 'B-GOVERNMENT',\n",
       " 'I-GOVERNMENT',\n",
       " 'B-GAME',\n",
       " 'I-GAME',\n",
       " 'B-COMPANY',\n",
       " 'I-COMPANY',\n",
       " 'B-ADDRESS',\n",
       " 'I-ADDRESS',\n",
       " 'B-BOOK',\n",
       " 'I-BOOK',\n",
       " 'B-POSITION',\n",
       " 'I-POSITION',\n",
       " 'B-MOVIE',\n",
       " 'I-MOVIE']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:57.572766Z",
     "iopub.status.busy": "2025-06-13T22:57:57.572553Z",
     "iopub.status.idle": "2025-06-13T22:57:58.965846Z",
     "shell.execute_reply": "2025-06-13T22:57:58.965057Z",
     "shell.execute_reply.started": "2025-06-13T22:57:57.572741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7479f43f55db4661a72f106c9375e7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9403266289407f8474aab04a2b5307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "           'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "\n",
    "def entity_tags_proc(item):\n",
    "    # item即是dataset中记录\n",
    "    text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "    tags = [0] * text_len    # 初始值为‘O’\n",
    "    # 遍历实体列表，所有实体类别标记填入tags\n",
    "    entites = item['ents']\n",
    "    for ent in entites:\n",
    "        indices = ent['indices']  # 实体索引\n",
    "        label = ent['label']   # 实体名\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "    return {'ent_tag': tags}\n",
    "\n",
    "# 使用自定义回调函数处理数据集记录\n",
    "ds1 = ds.map(entity_tags_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:58.967055Z",
     "iopub.status.busy": "2025-06-13T22:57:58.966748Z",
     "iopub.status.idle": "2025-06-13T22:57:58.971765Z",
     "shell.execute_reply": "2025-06-13T22:57:58.971055Z",
     "shell.execute_reply.started": "2025-06-13T22:57:58.967028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，\n",
      "[11, 12, 12, 12, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "for row in ds1['train']:\n",
    "    print(row['text'])\n",
    "    print(row['ent_tag'])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:58.973354Z",
     "iopub.status.busy": "2025-06-13T22:57:58.973090Z",
     "iopub.status.idle": "2025-06-13T22:57:58.993915Z",
     "shell.execute_reply": "2025-06-13T22:57:58.993293Z",
     "shell.execute_reply.started": "2025-06-13T22:57:58.973330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文bert分词在日期时间和英文转换token过程中，出现合并。影响ner标注准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:58.997332Z",
     "iopub.status.busy": "2025-06-13T22:57:58.996824Z",
     "iopub.status.idle": "2025-06-13T22:57:59.007448Z",
     "shell.execute_reply": "2025-06-13T22:57:59.006610Z",
     "shell.execute_reply.started": "2025-06-13T22:57:58.997307Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8202, 2399, 123, 3299, 10253]\n",
      "2000 年 2 月 add\n"
     ]
    }
   ],
   "source": [
    "token_index = tokenizer.encode('2000年2月add', add_special_tokens=False)\n",
    "print(token_index)\n",
    "tokens = tokenizer.decode(token_index)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:59.008437Z",
     "iopub.status.busy": "2025-06-13T22:57:59.008231Z",
     "iopub.status.idle": "2025-06-13T22:57:59.013322Z",
     "shell.execute_reply": "2025-06-13T22:57:59.012744Z",
     "shell.execute_reply.started": "2025-06-13T22:57:59.008423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[123, 121, 121, 121, 2399, 123, 3299, 143, 146, 146]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "2000 年 2 月 add\n"
     ]
    }
   ],
   "source": [
    "input_data = tokenizer([list('2000年2月add')], add_special_tokens=False, truncation=True,\n",
    "                       is_split_into_words=True)\n",
    "print(input_data)\n",
    "\n",
    "tokens = tokenizer.decode(token_index)\n",
    "print(tokens) # 返回token对应逐个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:57:59.014295Z",
     "iopub.status.busy": "2025-06-13T22:57:59.013948Z",
     "iopub.status.idle": "2025-06-13T22:58:05.128862Z",
     "shell.execute_reply": "2025-06-13T22:58:05.128053Z",
     "shell.execute_reply.started": "2025-06-13T22:57:59.014258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55b458cef1d441299813e72bb0cf798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1a91dfb5364b2690a20f8b92ac0495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "           'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "\n",
    "def entity_tags_proc(item):\n",
    "    # item即是dataset中记录\n",
    "    text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "    tags = [0] * text_len    # 初始值为‘O’\n",
    "    # 遍历实体列表，所有实体类别标记填入tags\n",
    "    entites = item['ents']\n",
    "    for ent in entites:\n",
    "        indices = ent['indices']  # 实体索引\n",
    "        label = ent['label']   # 实体名\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "    return {'ent_tag': tags}\n",
    "\n",
    "# 使用自定义回调函数处理数据集记录\n",
    "ds1 = ds.map(entity_tags_proc)\n",
    "\n",
    "def data_input_proc(item):\n",
    "    # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "    batch_texts = [list(text) for text in item['text']]\n",
    "    # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "    input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512,\n",
    "                           is_split_into_words=True, padding='max_length')\n",
    "    input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "    return input_data\n",
    "\n",
    "\n",
    "ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:05.130306Z",
     "iopub.status.busy": "2025-06-13T22:58:05.129890Z",
     "iopub.status.idle": "2025-06-13T22:58:05.135647Z",
     "shell.execute_reply": "2025-06-13T22:58:05.135133Z",
     "shell.execute_reply.started": "2025-06-13T22:58:05.130264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 512 512 512\n"
     ]
    }
   ],
   "source": [
    "for item in ds2['train']:\n",
    "    print(len(item['input_ids']), len(item['token_type_ids']), len(item['attention_mask']), len(item['labels']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:05.136435Z",
     "iopub.status.busy": "2025-06-13T22:58:05.136195Z",
     "iopub.status.idle": "2025-06-13T22:58:07.754402Z",
     "shell.execute_reply": "2025-06-13T22:58:07.753634Z",
     "shell.execute_reply.started": "2025-06-13T22:58:05.136419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 记录转换为pytorch\n",
    "ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "# ds_new = ds2.with_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型手动训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:07.755709Z",
     "iopub.status.busy": "2025-06-13T22:58:07.755397Z",
     "iopub.status.idle": "2025-06-13T22:58:07.768128Z",
     "shell.execute_reply": "2025-06-13T22:58:07.767447Z",
     "shell.execute_reply.started": "2025-06-13T22:58:07.755685Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert.embeddings.word_embeddings.weight',\n",
       " 'bert.embeddings.position_embeddings.weight',\n",
       " 'bert.embeddings.token_type_embeddings.weight',\n",
       " 'bert.embeddings.LayerNorm.weight',\n",
       " 'bert.embeddings.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.attention.self.query.weight',\n",
       " 'bert.encoder.layer.0.attention.self.query.bias',\n",
       " 'bert.encoder.layer.0.attention.self.key.weight',\n",
       " 'bert.encoder.layer.0.attention.self.key.bias',\n",
       " 'bert.encoder.layer.0.attention.self.value.weight',\n",
       " 'bert.encoder.layer.0.attention.self.value.bias',\n",
       " 'bert.encoder.layer.0.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.0.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.0.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.0.output.dense.weight',\n",
       " 'bert.encoder.layer.0.output.dense.bias',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.attention.self.query.weight',\n",
       " 'bert.encoder.layer.1.attention.self.query.bias',\n",
       " 'bert.encoder.layer.1.attention.self.key.weight',\n",
       " 'bert.encoder.layer.1.attention.self.key.bias',\n",
       " 'bert.encoder.layer.1.attention.self.value.weight',\n",
       " 'bert.encoder.layer.1.attention.self.value.bias',\n",
       " 'bert.encoder.layer.1.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.1.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.1.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.1.output.dense.weight',\n",
       " 'bert.encoder.layer.1.output.dense.bias',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.attention.self.query.weight',\n",
       " 'bert.encoder.layer.2.attention.self.query.bias',\n",
       " 'bert.encoder.layer.2.attention.self.key.weight',\n",
       " 'bert.encoder.layer.2.attention.self.key.bias',\n",
       " 'bert.encoder.layer.2.attention.self.value.weight',\n",
       " 'bert.encoder.layer.2.attention.self.value.bias',\n",
       " 'bert.encoder.layer.2.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.2.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.2.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.2.output.dense.weight',\n",
       " 'bert.encoder.layer.2.output.dense.bias',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.attention.self.query.weight',\n",
       " 'bert.encoder.layer.3.attention.self.query.bias',\n",
       " 'bert.encoder.layer.3.attention.self.key.weight',\n",
       " 'bert.encoder.layer.3.attention.self.key.bias',\n",
       " 'bert.encoder.layer.3.attention.self.value.weight',\n",
       " 'bert.encoder.layer.3.attention.self.value.bias',\n",
       " 'bert.encoder.layer.3.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.3.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.3.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.3.output.dense.weight',\n",
       " 'bert.encoder.layer.3.output.dense.bias',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.attention.self.query.weight',\n",
       " 'bert.encoder.layer.4.attention.self.query.bias',\n",
       " 'bert.encoder.layer.4.attention.self.key.weight',\n",
       " 'bert.encoder.layer.4.attention.self.key.bias',\n",
       " 'bert.encoder.layer.4.attention.self.value.weight',\n",
       " 'bert.encoder.layer.4.attention.self.value.bias',\n",
       " 'bert.encoder.layer.4.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.4.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.4.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.4.output.dense.weight',\n",
       " 'bert.encoder.layer.4.output.dense.bias',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.attention.self.query.weight',\n",
       " 'bert.encoder.layer.5.attention.self.query.bias',\n",
       " 'bert.encoder.layer.5.attention.self.key.weight',\n",
       " 'bert.encoder.layer.5.attention.self.key.bias',\n",
       " 'bert.encoder.layer.5.attention.self.value.weight',\n",
       " 'bert.encoder.layer.5.attention.self.value.bias',\n",
       " 'bert.encoder.layer.5.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.5.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.5.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.5.output.dense.weight',\n",
       " 'bert.encoder.layer.5.output.dense.bias',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.attention.self.query.weight',\n",
       " 'bert.encoder.layer.6.attention.self.query.bias',\n",
       " 'bert.encoder.layer.6.attention.self.key.weight',\n",
       " 'bert.encoder.layer.6.attention.self.key.bias',\n",
       " 'bert.encoder.layer.6.attention.self.value.weight',\n",
       " 'bert.encoder.layer.6.attention.self.value.bias',\n",
       " 'bert.encoder.layer.6.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.6.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.6.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.6.output.dense.weight',\n",
       " 'bert.encoder.layer.6.output.dense.bias',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.attention.self.query.weight',\n",
       " 'bert.encoder.layer.7.attention.self.query.bias',\n",
       " 'bert.encoder.layer.7.attention.self.key.weight',\n",
       " 'bert.encoder.layer.7.attention.self.key.bias',\n",
       " 'bert.encoder.layer.7.attention.self.value.weight',\n",
       " 'bert.encoder.layer.7.attention.self.value.bias',\n",
       " 'bert.encoder.layer.7.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.7.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.7.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.7.output.dense.weight',\n",
       " 'bert.encoder.layer.7.output.dense.bias',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.attention.self.query.weight',\n",
       " 'bert.encoder.layer.8.attention.self.query.bias',\n",
       " 'bert.encoder.layer.8.attention.self.key.weight',\n",
       " 'bert.encoder.layer.8.attention.self.key.bias',\n",
       " 'bert.encoder.layer.8.attention.self.value.weight',\n",
       " 'bert.encoder.layer.8.attention.self.value.bias',\n",
       " 'bert.encoder.layer.8.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.8.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.8.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.8.output.dense.weight',\n",
       " 'bert.encoder.layer.8.output.dense.bias',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.attention.self.query.weight',\n",
       " 'bert.encoder.layer.9.attention.self.query.bias',\n",
       " 'bert.encoder.layer.9.attention.self.key.weight',\n",
       " 'bert.encoder.layer.9.attention.self.key.bias',\n",
       " 'bert.encoder.layer.9.attention.self.value.weight',\n",
       " 'bert.encoder.layer.9.attention.self.value.bias',\n",
       " 'bert.encoder.layer.9.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.9.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.9.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.9.output.dense.weight',\n",
       " 'bert.encoder.layer.9.output.dense.bias',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.attention.self.query.weight',\n",
       " 'bert.encoder.layer.10.attention.self.query.bias',\n",
       " 'bert.encoder.layer.10.attention.self.key.weight',\n",
       " 'bert.encoder.layer.10.attention.self.key.bias',\n",
       " 'bert.encoder.layer.10.attention.self.value.weight',\n",
       " 'bert.encoder.layer.10.attention.self.value.bias',\n",
       " 'bert.encoder.layer.10.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.10.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.10.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.10.output.dense.weight',\n",
       " 'bert.encoder.layer.10.output.dense.bias',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.attention.self.query.weight',\n",
       " 'bert.encoder.layer.11.attention.self.query.bias',\n",
       " 'bert.encoder.layer.11.attention.self.key.weight',\n",
       " 'bert.encoder.layer.11.attention.self.key.bias',\n",
       " 'bert.encoder.layer.11.attention.self.value.weight',\n",
       " 'bert.encoder.layer.11.attention.self.value.bias',\n",
       " 'bert.encoder.layer.11.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.11.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.11.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.11.output.dense.weight',\n",
       " 'bert.encoder.layer.11.output.dense.bias',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.bias',\n",
       " 'classifier.weight',\n",
       " 'classifier.bias']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, params in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:07.769209Z",
     "iopub.status.busy": "2025-06-13T22:58:07.768932Z",
     "iopub.status.idle": "2025-06-13T22:58:08.202893Z",
     "shell.execute_reply": "2025-06-13T22:58:08.202291Z",
     "shell.execute_reply.started": "2025-06-13T22:58:07.769186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# dataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n",
    "\n",
    "\n",
    "# 模型创建\n",
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n",
    "                                                        num_labels=21,\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model.to('cuda')\n",
    "\n",
    "# 模型参数分组\n",
    "param_optimizer = list(model.named_parameters())\n",
    "bert_params, classifier_params = [],[]\n",
    "\n",
    "for name,params in param_optimizer:\n",
    "    if 'bert' in name:\n",
    "        bert_params.append(params)\n",
    "    else:\n",
    "        classifier_params.append(params)\n",
    "\n",
    "param_groups = [\n",
    "    {'params':bert_params, 'lr':1e-5},\n",
    "    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n",
    "]\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.AdamW(param_groups) # 优化器\n",
    "\n",
    "# 学习率调度器\n",
    "train_steps = len(train_dl) * 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=100,\n",
    "                                            num_training_steps=train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:08.203737Z",
     "iopub.status.busy": "2025-06-13T22:58:08.203543Z",
     "iopub.status.idle": "2025-06-13T22:58:08.207069Z",
     "shell.execute_reply": "2025-06-13T22:58:08.206311Z",
     "shell.execute_reply.started": "2025-06-13T22:58:08.203722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for item in train_dl:\n",
    "#     print(item['input_ids'].shape,\n",
    "#           item['token_type_ids'].shape,\n",
    "#           item['attention_mask'].shape,\n",
    "#           item['labels'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:08.207826Z",
     "iopub.status.busy": "2025-06-13T22:58:08.207671Z",
     "iopub.status.idle": "2025-06-13T22:58:08.217588Z",
     "shell.execute_reply": "2025-06-13T22:58:08.216783Z",
     "shell.execute_reply.started": "2025-06-13T22:58:08.207813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE='cuda'\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     model.train()\n",
    "#     tpbar = tqdm(train_dl)\n",
    "#     for items in tpbar:\n",
    "#         items = {k:v.to(DEVICE) for k,v in items.items()}\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(**items)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         tpbar.set_description(f'Epoch:{epoch+1} ' +\n",
    "#                           f'bert_lr:{scheduler.get_lr()[0]} ' +\n",
    "#                           f'classifier_lr:{scheduler.get_lr()[1]} '+\n",
    "#                           f'Loss:{loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持混合精度训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:08.218529Z",
     "iopub.status.busy": "2025-06-13T22:58:08.218348Z",
     "iopub.status.idle": "2025-06-13T22:58:08.705387Z",
     "shell.execute_reply": "2025-06-13T22:58:08.704839Z",
     "shell.execute_reply.started": "2025-06-13T22:58:08.218514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# dataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n",
    "\n",
    "\n",
    "# 模型创建\n",
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n",
    "                                                        num_labels=21,\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model.to('cuda')\n",
    "\n",
    "# 模型参数分组\n",
    "param_optimizer = list(model.named_parameters())\n",
    "bert_params, classifier_params = [],[]\n",
    "\n",
    "for name,params in param_optimizer:\n",
    "    if 'bert' in name:\n",
    "        bert_params.append(params)\n",
    "    else:\n",
    "        classifier_params.append(params)\n",
    "\n",
    "param_groups = [\n",
    "    {'params':bert_params, 'lr':1e-5},\n",
    "    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n",
    "]\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.AdamW(param_groups) # 优化器\n",
    "\n",
    "# 学习率调度器\n",
    "train_steps = len(train_dl) * 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=100,\n",
    "                                            num_training_steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:08.706398Z",
     "iopub.status.busy": "2025-06-13T22:58:08.706164Z",
     "iopub.status.idle": "2025-06-13T22:58:08.710166Z",
     "shell.execute_reply": "2025-06-13T22:58:08.709562Z",
     "shell.execute_reply.started": "2025-06-13T22:58:08.706382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "# DEVICE='cuda'\n",
    "\n",
    "# # 梯度计算缩放器\n",
    "# scaler = torch.GradScaler()\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     model.train()\n",
    "#     tpbar = tqdm(train_dl)\n",
    "#     for items in tpbar:\n",
    "#         items = {k:v.to(DEVICE) for k,v in items.items()}\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         with torch.autocast(device_type='cuda'):\n",
    "#             outputs = model(**items)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         # 缩放loss后，调用backward\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         tpbar.set_description(f'Epoch:{epoch+1} ' +\n",
    "#                           f'bert_lr:{scheduler.get_lr()[0]} ' +\n",
    "#                           f'classifier_lr:{scheduler.get_lr()[1]} '+\n",
    "#                           f'Loss:{loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持分布式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:08.711159Z",
     "iopub.status.busy": "2025-06-13T22:58:08.710907Z",
     "iopub.status.idle": "2025-06-13T22:58:08.725833Z",
     "shell.execute_reply": "2025-06-13T22:58:08.725274Z",
     "shell.execute_reply.started": "2025-06-13T22:58:08.711140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_simple.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_simple.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# 定义训练循环\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # 定义模型并将其移动到对应的 GPU 设备端\n",
    "    model = models.resnet50().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # 损失函数及优化器\n",
    "    criterion = nn.CrossEntropyLoss().to(rank)\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)\n",
    "\n",
    "    # 定义数据集Dataset的转换和图像增强\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    # 分布式训练采样器\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "\n",
    "    # 在训练开始时创建一次\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        ddp_model.train()\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(rank), labels.to(rank)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = ddp_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(ddp_model.parameters(), 1)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:08.726773Z",
     "iopub.status.busy": "2025-06-13T22:58:08.726590Z",
     "iopub.status.idle": "2025-06-13T22:58:08.739419Z",
     "shell.execute_reply": "2025-06-13T22:58:08.738798Z",
     "shell.execute_reply.started": "2025-06-13T22:58:08.726751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !python ddp_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer模型训练\n",
    "### TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:08.740490Z",
     "iopub.status.busy": "2025-06-13T22:58:08.740217Z",
     "iopub.status.idle": "2025-06-13T22:58:08.751639Z",
     "shell.execute_reply": "2025-06-13T22:58:08.750885Z",
     "shell.execute_reply.started": "2025-06-13T22:58:08.740466Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ner_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ner_ddp.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 数据集\n",
    "    ds = load_dataset('nlhappy/CLUE-NER')\n",
    "    # entity_index\n",
    "    entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "               'company', 'scene', 'book', 'organization', 'government'})\n",
    "    tags = ['O']\n",
    "    for entity in entites[1:]:\n",
    "        tags.append('B-' + entity.upper())\n",
    "        tags.append('I-' + entity.upper())\n",
    "\n",
    "    entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "    def entity_tags_proc(item):\n",
    "        # item即是dataset中记录\n",
    "        text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "        tags = [0] * text_len    # 初始值为‘O’\n",
    "        # 遍历实体列表，所有实体类别标记填入tags\n",
    "        entites = item['ents']\n",
    "        for ent in entites:\n",
    "            indices = ent['indices']  # 实体索引\n",
    "            label = ent['label']   # 实体名\n",
    "            tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "            for idx in indices[1:]:\n",
    "                tags[idx] = entity_index[label] * 2\n",
    "        return {'ent_tag': tags}\n",
    "\n",
    "    # 使用自定义回调函数处理数据集记录\n",
    "    ds1 = ds.map(entity_tags_proc)\n",
    "\n",
    "    def data_input_proc(item):\n",
    "        # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "        batch_texts = [list(text) for text in item['text']]\n",
    "        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "        input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512,\n",
    "                               is_split_into_words=True, padding='max_length')\n",
    "        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "        return input_data\n",
    "\n",
    "\n",
    "    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n",
    "\n",
    "\n",
    "    local_rank = rank\n",
    "\n",
    "    id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "    lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n",
    "                                                            num_labels=21,\n",
    "                                                            id2label=id2lbl,\n",
    "                                                            label2id=lbl2id)\n",
    "    model.to(local_rank)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "        num_train_epochs = 3,    # 训练 epoch\n",
    "        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "        per_device_train_batch_size=8,  # 训练批次\n",
    "        per_device_eval_batch_size=8,\n",
    "        report_to='tensorboard',  # 训练输出记录\n",
    "        eval_strategy=\"epoch\",\n",
    "        local_rank=local_rank,   # 当前进程 RANK\n",
    "        fp16=True,               # 使用混合精度\n",
    "        lr_scheduler_type='linear',  # 动态学习率\n",
    "        warmup_steps=100,        # 预热步数\n",
    "        ddp_find_unused_parameters=False  # 优化DDP性能\n",
    "    )\n",
    "\n",
    "    def compute_metric(result):\n",
    "        # result 是一个tuple (predicts, labels)\n",
    "\n",
    "        # 获取评估对象\n",
    "        seqeval = evaluate.load('seqeval')\n",
    "        predicts,labels = result\n",
    "        predicts = np.argmax(predicts, axis=2)\n",
    "\n",
    "        # 准备评估数据\n",
    "        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                     for ps,ls in zip(predicts,labels)]\n",
    "        labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                     for ps,ls in zip(predicts,labels)]\n",
    "        results = seqeval.compute(predictions=predicts, references=labels)\n",
    "\n",
    "        return results\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=ds2['train'],\n",
    "        eval_dataset=ds2['validation'],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metric\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T22:58:08.752512Z",
     "iopub.status.busy": "2025-06-13T22:58:08.752332Z",
     "iopub.status.idle": "2025-06-14T00:05:56.139840Z",
     "shell.execute_reply": "2025-06-14T00:05:56.138872Z",
     "shell.execute_reply.started": "2025-06-13T22:58:08.752497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:750: UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\AI\\pytorch_class\\week12\\week12_codes\\ner_ddp.py\", line 131, in <module>\n",
      "    main()\n",
      "  File \"e:\\AI\\pytorch_class\\week12\\week12_codes\\ner_ddp.py\", line 128, in main\n",
      "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py\", line 340, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py\", line 296, in start_processes\n",
      "    while not context.join():\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py\", line 215, in join\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py\", line 90, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"e:\\AI\\pytorch_class\\week12\\week12_codes\\ner_ddp.py\", line 25, in train\n",
      "    setup(rank, world_size)\n",
      "  File \"e:\\AI\\pytorch_class\\week12\\week12_codes\\ner_ddp.py\", line 17, in setup\n",
      "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\distributed\\c10d_logger.py\", line 81, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\distributed\\c10d_logger.py\", line 95, in wrapper\n",
      "    func_return = func(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py\", line 1714, in init_process_group\n",
      "    store, rank, world_size = next(rendezvous_iterator)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\distributed\\rendezvous.py\", line 274, in _env_rendezvous_handler\n",
      "    store = _create_c10d_store(\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\anaconda\\envs\\py311\\Lib\\site-packages\\torch\\distributed\\rendezvous.py\", line 194, in _create_c10d_store\n",
      "    return TCPStore(\n",
      "           ^^^^^^^^^\n",
      "RuntimeError: use_libuv was requested but PyTorch was build without libuv support\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ner_ddp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
