{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:52:50.188826Z","iopub.execute_input":"2025-07-11T03:52:50.189152Z","iopub.status.idle":"2025-07-11T03:52:50.193893Z","shell.execute_reply.started":"2025-07-11T03:52:50.189128Z","shell.execute_reply":"2025-07-11T03:52:50.193129Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"!pip -q install evaluate seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:52:53.241356Z","iopub.execute_input":"2025-07-11T03:52:53.241694Z","iopub.status.idle":"2025-07-11T03:52:56.543081Z","shell.execute_reply.started":"2025-07-11T03:52:53.241672Z","shell.execute_reply":"2025-07-11T03:52:56.542198Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# 导包\nfrom transformers import AutoModelForTokenClassification , AutoTokenizer \nfrom transformers import DataCollatorForTokenClassification , TrainingArguments , Trainer\nfrom datasets import load_dataset\nimport numpy as np\nimport evaluate   # pip install evaluate\nimport seqeval  # pip install seqeval\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\nimport torch\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:12.604244Z","iopub.execute_input":"2025-07-11T04:33:12.604519Z","iopub.status.idle":"2025-07-11T04:33:12.609963Z","shell.execute_reply.started":"2025-07-11T04:33:12.604500Z","shell.execute_reply":"2025-07-11T04:33:12.609276Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def load_and_prepare_data():\n    \"\"\"加载并预处理数据集\"\"\"\n    # 加载数据集\n    ds = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n\n    # 直接从数据集中提取已有标签（避免与原数据不一致）\n    tags = [\n        'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'\n    ]\n    # 提取实体类型 PER/ORG/LOC 及 O\n    entity_types = ['O'] + sorted(list(set(tag.split('-')[-1] for tag in tags if tag != 'O')))\n\n    entity_index = {entity: i for i, entity in enumerate(entity_types)}\n    \n    return ds, tags, entity_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:53:03.106289Z","iopub.execute_input":"2025-07-11T03:53:03.106993Z","iopub.status.idle":"2025-07-11T03:53:03.111965Z","shell.execute_reply.started":"2025-07-11T03:53:03.106966Z","shell.execute_reply":"2025-07-11T03:53:03.110984Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# 加载数据\nds = load_dataset('doushabao4766/msra_ner_k_V3')\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n\nfor items in ds['train']:\n    print(items['tokens'])\n    print(items['ner_tags'])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:25.087007Z","iopub.execute_input":"2025-07-11T04:33:25.087476Z","iopub.status.idle":"2025-07-11T04:33:26.981898Z","shell.execute_reply.started":"2025-07-11T04:33:25.087454Z","shell.execute_reply":"2025-07-11T04:33:26.981299Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f9548a09824490dad1aecda6a4c228b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4075ddfe1b492ab82488876615c00c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f61a0b59283a4f49a9957c61c9333f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2f4f0e37354a2193cc352739fb93a8"}},"metadata":{}},{"name":"stdout","text":"['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# 查看tag标签数量\ntags_id = set()\nfor tags in ds['train']:\n    tags_id.update(tags['ner_tags'])\n\ntags_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:38.002719Z","iopub.execute_input":"2025-07-11T04:33:38.003257Z","iopub.status.idle":"2025-07-11T04:33:42.938877Z","shell.execute_reply.started":"2025-07-11T04:33:38.003235Z","shell.execute_reply":"2025-07-11T04:33:42.938146Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"{0, 1, 2, 3, 4, 5, 6}"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"# 构建映射标签\nentites = list({'per' , 'loc' , 'org'})\ntags = ['O']\nfor entity in entites:\n    tags.append('B-' + entity.upper())  # upper()方法是转换为大写\n    tags.append('I-' + entity.upper())\ntags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:48.643263Z","iopub.execute_input":"2025-07-11T04:33:48.643954Z","iopub.status.idle":"2025-07-11T04:33:48.649125Z","shell.execute_reply.started":"2025-07-11T04:33:48.643928Z","shell.execute_reply":"2025-07-11T04:33:48.648465Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"\n# 创建构建方法 [tag + [0] * (512 - len(tag)) for tag in item['ner_tags']]\ndef data_input_proc(item):\n    input_data = tokenizer(item['tokens'],\n                          truncation = True ,  # 超过最大长度允许截断防止溢出\n                          max_length = 512 ,   #最大512\n                          add_special_tokens = False ,  # 禁止添加特殊标记  确保标签对其\n                          is_split_into_words = True) # 因为该数据集已经按照字符划分，所以用id_split_into_words = True 表明一个字符一个字符的传入\n    # 设置标签映射（超过512 截断）\n    labels = [lbl[:512] for lbl in item['ner_tags']]\n    input_data['labels'] = labels\n    return input_data\nds1 = ds.map(data_input_proc , batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:56.903526Z","iopub.execute_input":"2025-07-11T04:33:56.904194Z","iopub.status.idle":"2025-07-11T04:34:08.124203Z","shell.execute_reply.started":"2025-07-11T04:33:56.904171Z","shell.execute_reply":"2025-07-11T04:34:08.123463Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca554c08e10c42f595ddcb34b775ab51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70a754c958c94958b8f0444151445022"}},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"for item in ds1['train']:\n    print(item['tokens'])\n    print(item['ner_tags'])\n    print(item['knowledge'])\n    print(item['input_ids'])\n    print(item['token_type_ids'])\n    print(item['attention_mask'])\n    print(item['labels'])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:10.019631Z","iopub.execute_input":"2025-07-11T04:34:10.019909Z","iopub.status.idle":"2025-07-11T04:34:10.025816Z","shell.execute_reply.started":"2025-07-11T04:34:10.019888Z","shell.execute_reply":"2025-07-11T04:34:10.025118Z"}},"outputs":[{"name":"stdout","text":"['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n[2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"# 选择模型需要输入的列 将其转换为 torch张量类型\nds1.set_format('torch' , columns = ['input_ids' ,  # token 索引序列\n                                    'token_type_ids' ,  # 段落标记\n                                    'attention_mask' ,  # 注意力掩码\n                                    'labels']) # NER标签序列\nfor item in ds1['train']:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:21.677378Z","iopub.execute_input":"2025-07-11T04:34:21.677652Z","iopub.status.idle":"2025-07-11T04:34:21.692598Z","shell.execute_reply.started":"2025-07-11T04:34:21.677632Z","shell.execute_reply":"2025-07-11T04:34:21.692000Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n        1159, 8013]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])}\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# 构建模型初始化可读标签参数，\nid2lbl = {i:tag for i,tag in enumerate(tags)}\nlbl2id = {tag:i for i,tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"bert-base-chinese\" , # 预训练模型\n                                                       num_labels = len(tags) ,  # 输出的分类数量\n                                                       id2label = id2lbl , \n                                                       label2id = lbl2id)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:30.750940Z","iopub.execute_input":"2025-07-11T04:34:30.751284Z","iopub.status.idle":"2025-07-11T04:34:33.718485Z","shell.execute_reply.started":"2025-07-11T04:34:30.751256Z","shell.execute_reply":"2025-07-11T04:34:33.717755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ceb7175ade44949818d0fb56a7eada"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"# 自动填充对其\ndata_collator = DataCollatorForTokenClassification(tokenizer = tokenizer , padding = True)\n# 在DataLoader中使用\ntrain_dl = DataLoader(\n    ds1['train'], \n    batch_size=16,\n    shuffle = True,\n    collate_fn = data_collator\n)\n\n\nmodel.to('cuda')\n\n# 模型参数分组\n\n# 获取模型参数\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    # 获取预训练模型\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},  # 预训练模型的学习率较低 保持稳定性\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3} # 新的分类层学习率较高 更好的学习，'weight_decay':0.1 使用正则化L2\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\n\n# 步长 从初始设置值到0 衰减需要的步长\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            # 预热 从0到初始设置值的步长\n                                            num_warmup_steps=100, \n                                            # 衰减 从初始设置值到0 衰减需要的步长\n                                            num_training_steps=train_steps)\n\n\nfor item in train_dl:\n    print(item['input_ids'].shape, \n          item['token_type_ids'].shape, \n          item['attention_mask'].shape, \n          item['labels'].shape)\n    break\n\nDEVICE='cuda'\n\nfor epoch in range(5):\n    model.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        # 张量移动到指定的设备商\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        # 数据传入模型\n        outputs = model(**items)\n        # 计算损失\n        loss = outputs.loss\n        # 反向传播计算梯度\n        loss.backward()\n        # 更新模型参数的梯度\n        optimizer.step()\n        # 更新学习率\n        scheduler.step()\n        # 模型参数的梯度清零\n        optimizer.zero_grad()\n    \n        tpbar.set_description(f'Epoch:{epoch+1} ' + \n                          f'bert_lr:{scheduler.get_lr()[0]} ' + \n                          f'classifier_lr:{scheduler.get_lr()[1]} '+\n                          f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:50.779389Z","iopub.execute_input":"2025-07-11T04:34:50.780154Z","iopub.status.idle":"2025-07-11T05:57:56.561393Z","shell.execute_reply.started":"2025-07-11T04:34:50.780124Z","shell.execute_reply":"2025-07-11T05:57:56.560768Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 90]) torch.Size([16, 90]) torch.Size([16, 90]) torch.Size([16, 90])\n","output_type":"stream"},{"name":"stderr","text":"Epoch:1 bert_lr:8.057286072323668e-06 classifier_lr:0.0008057286072323666 Loss:0.0498: 100%|██████████| 2813/2813 [16:35<00:00,  2.83it/s]\nEpoch:2 bert_lr:6.042964554242751e-06 classifier_lr:0.000604296455424275 Loss:0.0105: 100%|██████████| 2813/2813 [16:37<00:00,  2.82it/s]  \nEpoch:3 bert_lr:4.028643036161834e-06 classifier_lr:0.0004028643036161833 Loss:0.0121: 100%|██████████| 2813/2813 [16:38<00:00,  2.82it/s]  \nEpoch:4 bert_lr:2.014321518080917e-06 classifier_lr:0.00020143215180809166 Loss:0.0001: 100%|██████████| 2813/2813 [16:33<00:00,  2.83it/s] \nEpoch:5 bert_lr:0.0 classifier_lr:0.0 Loss:0.0001: 100%|██████████| 2813/2813 [16:41<00:00,  2.81it/s]                                      \n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"def compute_metric(result):\n    # 传入的result是一个元祖 (predicts,labels)\n\n    # 加载序列标注评估指标库\n    seqeval = evaluate.load('seqeval')\n    # 解构模型输出的结果\n    predicts,labels = result\n    # 沿着axis = 2 的维度 取最大值索引 然后将predicts转换为预测标签ID\n    predicts = np.argmax(predicts , axis = 2)\n    # 准备评估数据 将数字ID转换为文本标签 并且过滤填充数值-100\n    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    results = seqeval.compute(predictions = predicts , references = labels)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:52.359981Z","iopub.execute_input":"2025-07-11T06:36:52.360592Z","iopub.status.idle":"2025-07-11T06:36:52.365753Z","shell.execute_reply.started":"2025-07-11T06:36:52.360568Z","shell.execute_reply":"2025-07-11T06:36:52.364957Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer , padding = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:55.953626Z","iopub.execute_input":"2025-07-11T06:36:55.953883Z","iopub.status.idle":"2025-07-11T06:36:55.957721Z","shell.execute_reply.started":"2025-07-11T06:36:55.953862Z","shell.execute_reply":"2025-07-11T06:36:55.956898Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir = 'ner_train' , # 设置模型输出目录\n    num_train_epochs = 3 , # 训练轮数\n    #save_safetensor = False # 模型禁止保存safe格式 可以用troch.load加载\n    per_device_train_batch_size = 32 , # 训练批次\n    per_device_eval_batch_size = 32 ,  # 评估批次\n    report_to = 'tensorboard' , # 设置训练输出记录为tensorboard\n    eval_strategy = 'epoch'  # 每轮评估一次\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:58.449836Z","iopub.execute_input":"2025-07-11T06:36:58.450070Z","iopub.status.idle":"2025-07-11T06:36:58.484696Z","shell.execute_reply.started":"2025-07-11T06:36:58.450053Z","shell.execute_reply":"2025-07-11T06:36:58.484042Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"trainer = Trainer(\n    model = model ,  # 指定模型\n    args = args , # 指定设置参数\n    train_dataset = ds1['train'] ,  # 输入训练数据\n    eval_dataset = ds1['test'] ,  # 输入评估数据\n    compute_metrics = compute_metric , # 指定评估函数\n    data_collator = data_collator  # 指定数据收集器\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:37:00.324017Z","iopub.execute_input":"2025-07-11T06:37:00.324243Z","iopub.status.idle":"2025-07-11T06:37:00.601153Z","shell.execute_reply.started":"2025-07-11T06:37:00.324227Z","shell.execute_reply":"2025-07-11T06:37:00.600605Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:37:03.217858Z","iopub.execute_input":"2025-07-11T06:37:03.218114Z","iopub.status.idle":"2025-07-11T07:15:47.910873Z","shell.execute_reply.started":"2025-07-11T06:37:03.218094Z","shell.execute_reply":"2025-07-11T07:15:47.910342Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2112' max='2112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2112/2112 38:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Loc</th>\n      <th>Org</th>\n      <th>Per</th>\n      <th>Overall Precision</th>\n      <th>Overall Recall</th>\n      <th>Overall F1</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.014400</td>\n      <td>0.033448</td>\n      <td>{'precision': 0.9479092841956059, 'recall': 0.9379382889200562, 'f1': 0.9428974268593585, 'number': 2852}</td>\n      <td>{'precision': 0.8187456926257753, 'recall': 0.9, 'f1': 0.8574521833273187, 'number': 1320}</td>\n      <td>{'precision': 0.9413298565840938, 'recall': 0.9607451763140386, 'f1': 0.9509384260783669, 'number': 1503}</td>\n      <td>0.913897</td>\n      <td>0.935154</td>\n      <td>0.924403</td>\n      <td>0.991583</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.010200</td>\n      <td>0.031717</td>\n      <td>{'precision': 0.9583184894905593, 'recall': 0.9431977559607293, 'f1': 0.950698003180774, 'number': 2852}</td>\n      <td>{'precision': 0.8731778425655977, 'recall': 0.9075757575757576, 'f1': 0.8900445765230313, 'number': 1320}</td>\n      <td>{'precision': 0.9519104084321476, 'recall': 0.9614105123087159, 'f1': 0.9566368752068851, 'number': 1503}</td>\n      <td>0.936107</td>\n      <td>0.939736</td>\n      <td>0.937918</td>\n      <td>0.993087</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.003000</td>\n      <td>0.038207</td>\n      <td>{'precision': 0.9593292900463789, 'recall': 0.9428471248246845, 'f1': 0.9510167992926614, 'number': 2852}</td>\n      <td>{'precision': 0.8776097912167027, 'recall': 0.9234848484848485, 'f1': 0.8999630860095975, 'number': 1320}</td>\n      <td>{'precision': 0.9572086899275839, 'recall': 0.9673985362608117, 'f1': 0.9622766379880874, 'number': 1503}</td>\n      <td>0.938890</td>\n      <td>0.944846</td>\n      <td>0.941858</td>\n      <td>0.993504</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a0cedcd9a404e53a6d18bfd84adb77a"}},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'precision': 0.9479092841956059, 'recall': 0.9379382889200562, 'f1': 0.9428974268593585, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8187456926257753, 'recall': 0.9, 'f1': 0.8574521833273187, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9413298565840938, 'recall': 0.9607451763140386, 'f1': 0.9509384260783669, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.9583184894905593, 'recall': 0.9431977559607293, 'f1': 0.950698003180774, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8731778425655977, 'recall': 0.9075757575757576, 'f1': 0.8900445765230313, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9519104084321476, 'recall': 0.9614105123087159, 'f1': 0.9566368752068851, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.9593292900463789, 'recall': 0.9428471248246845, 'f1': 0.9510167992926614, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8776097912167027, 'recall': 0.9234848484848485, 'f1': 0.8999630860095975, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9572086899275839, 'recall': 0.9673985362608117, 'f1': 0.9622766379880874, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2112, training_loss=0.008169087835333565, metrics={'train_runtime': 2324.0708, 'train_samples_per_second': 58.089, 'train_steps_per_second': 0.909, 'total_flos': 1.180990200098808e+16, 'train_loss': 0.008169087835333565, 'epoch': 3.0})"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"from transformers import pipeline\n\npipeline = pipeline('token-classification', 'ner_train/checkpoint-2112')\n\ntext = pipeline('双方确定了今后发展中美关系的指导方针')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:51.301395Z","iopub.execute_input":"2025-07-11T07:18:51.302135Z","iopub.status.idle":"2025-07-11T07:18:51.547399Z","shell.execute_reply.started":"2025-07-11T07:18:51.302110Z","shell.execute_reply":"2025-07-11T07:18:51.546822Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":81}]}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile ddp.py\n\nimport subprocess\nimport sys\n\nsubprocess.run(\n    [sys.executable, \"-m\", \"pip\", \"install\", \"evaluate\", \"-q\"],\n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.DEVNULL\n)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom tqdm import tqdm\nimport json\n\nfrom transformers import (\n    AutoModelForTokenClassification, \n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    TrainingArguments, \n    Trainer,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset\nimport evaluate\n\n# ===== 数据处理模块 =====\ndef load_and_prepare_data():\n    \"\"\"加载并预处理数据集\"\"\"\n    # 加载数据集\n    ds = load_dataset('nlhappy/CLUE-NER')\n    \n    # 定义实体类别和标签\n    entities = ['O'] + list({\n        'movie', 'name', 'game', 'address', 'position', \n        'company', 'scene', 'book', 'organization', 'government'\n    })\n    \n    tags = ['O']\n    for entity in entities[1:]:\n        tags.append('B-' + entity.upper())\n        tags.append('I-' + entity.upper())\n    \n    entity_index = {entity: i for i, entity in enumerate(entities)}\n    \n    return ds, tags, entity_index\n\ndef process_entity_tags(ds, entity_index):\n    \"\"\"处理实体标签\"\"\"\n    def entity_tags_proc(item):\n        text_len = len(item['text'])\n        tags = [0] * text_len\n        entities = item['ents']\n        for ent in entities:\n            indices = ent['indices']\n            label = ent['label']\n            tags[indices[0]] = entity_index[label] * 2 - 1\n            for idx in indices[1:]:\n                tags[idx] = entity_index[label] * 2\n        return {'ent_tag': tags}\n    \n    return ds.map(entity_tags_proc)\n\ndef tokenize_data(ds, tokenizer):\n    \"\"\"分词处理\"\"\"\n    def data_input_proc(item):\n        batch_texts = [list(text) for text in item['text']]\n        input_data = tokenizer(\n            batch_texts, \n            truncation=True, \n            add_special_tokens=False, \n            max_length=512,\n            is_split_into_words=True, \n            padding='max_length'\n        )\n        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n        return input_data\n    \n    return ds.map(data_input_proc, batched=True)\n\n# ===== 模型训练模块 =====\ndef create_model_and_optimizer(tags, learning_rates=None):\n    \"\"\"创建模型和优化器\"\"\"\n    if learning_rates is None:\n        learning_rates = {'bert': 1e-5, 'classifier': 1e-3}\n    \n    # 创建标签映射\n    id2lbl = {i: tag for i, tag in enumerate(tags)}\n    lbl2id = {tag: i for i, tag in enumerate(tags)}\n    \n    # 加载模型\n    model = AutoModelForTokenClassification.from_pretrained(\n        'google-bert/bert-base-chinese',\n        num_labels=len(tags),\n        id2label=id2lbl,\n        label2id=lbl2id\n    )\n    \n    # 参数分组\n    param_optimizer = list(model.named_parameters())\n    bert_params = [params for name, params in param_optimizer if 'bert' in name]\n    classifier_params = [params for name, params in param_optimizer if 'bert' not in name]\n    \n    param_groups = [\n        {'params': bert_params, 'lr': learning_rates['bert']},\n        {'params': classifier_params, 'weight_decay': 0.1, 'lr': learning_rates['classifier']}\n    ]\n    \n    optimizer = optim.AdamW(param_groups)\n    return model, optimizer\n\ndef setup_distributed(rank, world_size):\n    \"\"\"设置分布式训练环境\"\"\"\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\ndef cleanup_distributed():\n    \"\"\"清理分布式训练环境\"\"\"\n    dist.destroy_process_group()\n\ndef train_ddp(rank, world_size, train_dataset, tags, epochs=3, batch_size=16, use_amp=False):\n    \"\"\"分布式DDP训练函数\"\"\"\n    print(f\"Running DDP on rank {rank}\")\n    setup_distributed(rank, world_size)\n    \n    # 创建模型和优化器\n    model, optimizer = create_model_and_optimizer(tags)\n    model = model.to(rank)\n    model = DDP(model, device_ids=[rank])\n    \n    # 创建分布式数据加载器\n    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size,\n        sampler=train_sampler,\n        pin_memory=True\n    )\n    \n    # 学习率调度器\n    train_steps = len(train_loader) * epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=100,\n        num_training_steps=train_steps\n    )\n    \n    # 混合精度训练\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n    \n    # 训练循环\n    for epoch in range(epochs):\n        train_sampler.set_epoch(epoch)\n        model.train()\n        \n        if rank == 0:\n            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n        else:\n            pbar = train_loader\n            \n        total_loss = 0\n        for batch_idx, items in enumerate(pbar):\n            items = {k: v.to(rank) for k, v in items.items()}\n            optimizer.zero_grad()\n            \n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    outputs = model(**items)\n                loss = outputs.loss\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(**items)\n                loss = outputs.loss\n                loss.backward()\n                optimizer.step()\n            \n            scheduler.step()\n            total_loss += loss.item()\n            \n            if rank == 0 and isinstance(pbar, tqdm):\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        \n        if rank == 0:\n            avg_loss = total_loss / len(train_loader)\n            print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n    \n    # 保存模型 (只在rank 0保存)\n    if rank == 0:\n        save_model(model.module, tags, \"/kaggle/working/ner_ddp_model\")\n        print(\"DDP训练完成，模型已保存！\")\n    \n    cleanup_distributed()\n\ndef save_model(model, tags, save_dir):\n    \"\"\"保存模型和配置\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # 保存模型权重\n    torch.save(model.state_dict(), os.path.join(save_dir, \"/kaggle/working/model_weights.pth\"))\n    \n    # 保存标签映射\n    label_config = {\n        'tags': tags,\n        'id2label': {i: tag for i, tag in enumerate(tags)},\n        'label2id': {tag: i for i, tag in enumerate(tags)}\n    }\n    with open(os.path.join(save_dir, \"/kaggle/working/label_config.json\"), 'w', encoding='utf-8') as f:\n        json.dump(label_config, f, indent=2, ensure_ascii=False)\n    \n    print(f\"模型已保存到: {save_dir}\")\n\ndef load_model(save_dir):\n    \"\"\"加载保存的模型\"\"\"\n    # 加载标签配置\n    with open(os.path.join(save_dir, \"/kaggle/working/label_config.json\"), 'r', encoding='utf-8') as f:\n        label_config = json.load(f)\n    \n    tags = label_config['tags']\n    id2label = label_config['id2label']\n    label2id = label_config['label2id']\n    \n    # 创建模型\n    model = AutoModelForTokenClassification.from_pretrained(\n        'google-bert/bert-base-chinese',\n        num_labels=len(tags),\n        id2label=id2label,\n        label2id=label2id\n    )\n    \n    # 加载权重\n    model.load_state_dict(torch.load(os.path.join(save_dir, \"/kaggle/working/model_weights.pth\")))\n    \n    # 加载分词器\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    \n    return model, tokenizer, tags\n\ndef predict_entities(model, tokenizer, tags, text):\n    \"\"\"推理函数\"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    \n    # 处理输入文本\n    chars = list(text)\n    inputs = tokenizer(\n        chars,\n        return_tensors='pt',\n        is_split_into_words=True,\n        padding=True,\n        truncation=True,\n        max_length=512\n    )\n    \n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # 预测\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n    \n    # 解析结果\n    predicted_labels = [tags[pred.item()] for pred in predictions[0]]\n    \n    # 提取实体\n    entities = []\n    current_entity = None\n    \n    for i, (char, label) in enumerate(zip(chars, predicted_labels)):\n        if label.startswith('B-'):\n            if current_entity:\n                entities.append(current_entity)\n            current_entity = {\n                'text': char,\n                'label': label[2:],\n                'start': i,\n                'end': i + 1\n            }\n        elif label.startswith('I-') and current_entity and label[2:] == current_entity['label']:\n            current_entity['text'] += char\n            current_entity['end'] = i + 1\n        else:\n            if current_entity:\n                entities.append(current_entity)\n                current_entity = None\n    \n    if current_entity:\n        entities.append(current_entity)\n    \n    return entities\n\ndef compute_metrics(eval_pred):\n    \"\"\"计算评估指标\"\"\"\n    seqeval = evaluate.load('seqeval')\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=2)\n    \n    # 获取标签映射\n    _, tags, _ = load_and_prepare_data()\n    \n    # 准备评估数据\n    true_predictions = [\n        [tags[p] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [tags[l] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    return seqeval.compute(predictions=true_predictions, references=true_labels)\n\ndef inference_demo():\n    \"\"\"推理演示\"\"\"\n    print(\"加载模型进行推理演示...\")\n    try:\n        model, tokenizer, tags = load_model(\"/kaggle/working/ner_ddp_model\")\n        model.eval()\n        \n        # 测试文本\n        test_texts = [\n            \"我在北京大学学习人工智能\",\n            \"张三在腾讯公司工作\",\n            \"他喜欢看电影《阿甘正传》\"\n        ]\n        \n        for text in test_texts:\n            print(f\"\\n输入文本: {text}\")\n            entities = predict_entities(model, tokenizer, tags, text)\n            print(\"识别的实体:\")\n            for entity in entities:\n                print(f\"  - {entity['text']} ({entity['label']}) [{entity['start']}:{entity['end']}]\")\n    \n    except FileNotFoundError:\n        print(\"未找到保存的模型，请先进行训练！\")\n\ndef main():\n    \n\n    ds, tags, entity_index = load_and_prepare_data()\n    \n \n    ds1 = process_entity_tags(ds, entity_index)\n    \n   \n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    ds2 = tokenize_data(ds1, tokenizer)\n    \n\n    model, optimizer = create_model_and_optimizer(tags)\n    \n\n    world_size = torch.cuda.device_count()\n    if world_size < 2:\n        print(\"警告: 只检测到一个GPU\")\n        world_size = 1\n    \n    print(f\"使用 {world_size} 个GPU进行分布式训练...\")\n    ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n    \n    if world_size > 1:\n        mp.spawn(train_ddp, args=(world_size, ds2['train'], tags), nprocs=world_size, join=True)\n    else:\n        # 单GPU情况下直接训练\n        train_ddp(0, 1, ds2['train'], tags)\n\n    print(\"训练完成！\")\n\n    inference_demo()\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:48:15.282332Z","iopub.execute_input":"2025-07-12T16:48:15.283219Z","iopub.status.idle":"2025-07-12T16:48:15.294683Z","shell.execute_reply.started":"2025-07-12T16:48:15.283185Z","shell.execute_reply":"2025-07-12T16:48:15.294058Z"}},"outputs":[{"name":"stdout","text":"Writing ddp.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"! python ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:48:43.923417Z","iopub.execute_input":"2025-07-12T16:48:43.924150Z"}},"outputs":[{"name":"stdout","text":"2025-07-12 16:48:50.719703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752338930.752033     141 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752338930.759022     141 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nREADME.md: 100%|██████████████████████████████| 21.0/21.0 [00:00<00:00, 177kB/s]\ndataset_infos.json: 100%|██████████████████████| 970/970 [00:00<00:00, 8.67MB/s]\n(…)-00000-of-00001-a33d0e4276aef9b4.parquet: 100%|█| 1.30M/1.30M [00:00<00:00, 3\n(…)-00000-of-00001-07f476b71c5edde6.parquet: 100%|█| 178k/178k [00:00<00:00, 92.\nGenerating train split: 100%|██| 10748/10748 [00:00<00:00, 248962.46 examples/s]\nGenerating validation split: 100%|█| 1343/1343 [00:00<00:00, 358763.79 examples/\nMap: 100%|██████████████████████| 10748/10748 [00:01<00:00, 10379.60 examples/s]\nMap: 100%|████████████████████████| 1343/1343 [00:00<00:00, 10123.59 examples/s]\ntokenizer_config.json: 100%|██████████████████| 49.0/49.0 [00:00<00:00, 347kB/s]\nconfig.json: 100%|█████████████████████████████| 624/624 [00:00<00:00, 5.63MB/s]\nvocab.txt: 110kB [00:00, 14.1MB/s]\ntokenizer.json: 269kB [00:00, 61.9MB/s]\nMap: 100%|███████████████████████| 10748/10748 [00:05<00:00, 2083.85 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 2336.85 examples/s]\nmodel.safetensors: 100%|██████████████████████| 412M/412M [00:02<00:00, 203MB/s]\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n使用 2 个GPU进行分布式训练...\n2025-07-12 16:49:16.798006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-07-12 16:49:16.803220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752338956.820519     188 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752338956.824875     187 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752338956.827589     188 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1752338956.831650     187 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nRunning DDP on rank 0\nRunning DDP on rank 1\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/3:  17%|███               | 57/336 [01:28<07:50,  1.69s/it, loss=0.1884]","output_type":"stream"}],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:52:50.188826Z","iopub.execute_input":"2025-07-11T03:52:50.189152Z","iopub.status.idle":"2025-07-11T03:52:50.193893Z","shell.execute_reply.started":"2025-07-11T03:52:50.189128Z","shell.execute_reply":"2025-07-11T03:52:50.193129Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"!pip -q install evaluate seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:52:53.241356Z","iopub.execute_input":"2025-07-11T03:52:53.241694Z","iopub.status.idle":"2025-07-11T03:52:56.543081Z","shell.execute_reply.started":"2025-07-11T03:52:53.241672Z","shell.execute_reply":"2025-07-11T03:52:56.542198Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# 导包\nfrom transformers import AutoModelForTokenClassification , AutoTokenizer \nfrom transformers import DataCollatorForTokenClassification , TrainingArguments , Trainer\nfrom datasets import load_dataset\nimport numpy as np\nimport evaluate   # pip install evaluate\nimport seqeval  # pip install seqeval\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\nimport torch\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:12.604244Z","iopub.execute_input":"2025-07-11T04:33:12.604519Z","iopub.status.idle":"2025-07-11T04:33:12.609963Z","shell.execute_reply.started":"2025-07-11T04:33:12.604500Z","shell.execute_reply":"2025-07-11T04:33:12.609276Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def load_and_prepare_data():\n    \"\"\"加载并预处理数据集\"\"\"\n    # 加载数据集\n    ds = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n\n    # 直接从数据集中提取已有标签（避免与原数据不一致）\n    tags = [\n        'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'\n    ]\n    # 提取实体类型 PER/ORG/LOC 及 O\n    entity_types = ['O'] + sorted(list(set(tag.split('-')[-1] for tag in tags if tag != 'O')))\n\n    entity_index = {entity: i for i, entity in enumerate(entity_types)}\n    \n    return ds, tags, entity_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T03:53:03.106289Z","iopub.execute_input":"2025-07-11T03:53:03.106993Z","iopub.status.idle":"2025-07-11T03:53:03.111965Z","shell.execute_reply.started":"2025-07-11T03:53:03.106966Z","shell.execute_reply":"2025-07-11T03:53:03.110984Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# 加载数据\nds = load_dataset('doushabao4766/msra_ner_k_V3')\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n\nfor items in ds['train']:\n    print(items['tokens'])\n    print(items['ner_tags'])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:25.087007Z","iopub.execute_input":"2025-07-11T04:33:25.087476Z","iopub.status.idle":"2025-07-11T04:33:26.981898Z","shell.execute_reply.started":"2025-07-11T04:33:25.087454Z","shell.execute_reply":"2025-07-11T04:33:26.981299Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f9548a09824490dad1aecda6a4c228b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4075ddfe1b492ab82488876615c00c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f61a0b59283a4f49a9957c61c9333f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2f4f0e37354a2193cc352739fb93a8"}},"metadata":{}},{"name":"stdout","text":"['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"# 查看tag标签数量\ntags_id = set()\nfor tags in ds['train']:\n    tags_id.update(tags['ner_tags'])\n\ntags_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:38.002719Z","iopub.execute_input":"2025-07-11T04:33:38.003257Z","iopub.status.idle":"2025-07-11T04:33:42.938877Z","shell.execute_reply.started":"2025-07-11T04:33:38.003235Z","shell.execute_reply":"2025-07-11T04:33:42.938146Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"{0, 1, 2, 3, 4, 5, 6}"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"# 构建映射标签\nentites = list({'per' , 'loc' , 'org'})\ntags = ['O']\nfor entity in entites:\n    tags.append('B-' + entity.upper())  # upper()方法是转换为大写\n    tags.append('I-' + entity.upper())\ntags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:48.643263Z","iopub.execute_input":"2025-07-11T04:33:48.643954Z","iopub.status.idle":"2025-07-11T04:33:48.649125Z","shell.execute_reply.started":"2025-07-11T04:33:48.643928Z","shell.execute_reply":"2025-07-11T04:33:48.648465Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"\n# 创建构建方法 [tag + [0] * (512 - len(tag)) for tag in item['ner_tags']]\ndef data_input_proc(item):\n    input_data = tokenizer(item['tokens'],\n                          truncation = True ,  # 超过最大长度允许截断防止溢出\n                          max_length = 512 ,   #最大512\n                          add_special_tokens = False ,  # 禁止添加特殊标记  确保标签对其\n                          is_split_into_words = True) # 因为该数据集已经按照字符划分，所以用id_split_into_words = True 表明一个字符一个字符的传入\n    # 设置标签映射（超过512 截断）\n    labels = [lbl[:512] for lbl in item['ner_tags']]\n    input_data['labels'] = labels\n    return input_data\nds1 = ds.map(data_input_proc , batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:33:56.903526Z","iopub.execute_input":"2025-07-11T04:33:56.904194Z","iopub.status.idle":"2025-07-11T04:34:08.124203Z","shell.execute_reply.started":"2025-07-11T04:33:56.904171Z","shell.execute_reply":"2025-07-11T04:34:08.123463Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca554c08e10c42f595ddcb34b775ab51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70a754c958c94958b8f0444151445022"}},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"for item in ds1['train']:\n    print(item['tokens'])\n    print(item['ner_tags'])\n    print(item['knowledge'])\n    print(item['input_ids'])\n    print(item['token_type_ids'])\n    print(item['attention_mask'])\n    print(item['labels'])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:10.019631Z","iopub.execute_input":"2025-07-11T04:34:10.019909Z","iopub.status.idle":"2025-07-11T04:34:10.025816Z","shell.execute_reply.started":"2025-07-11T04:34:10.019888Z","shell.execute_reply":"2025-07-11T04:34:10.025118Z"}},"outputs":[{"name":"stdout","text":"['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n[2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"# 选择模型需要输入的列 将其转换为 torch张量类型\nds1.set_format('torch' , columns = ['input_ids' ,  # token 索引序列\n                                    'token_type_ids' ,  # 段落标记\n                                    'attention_mask' ,  # 注意力掩码\n                                    'labels']) # NER标签序列\nfor item in ds1['train']:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:21.677378Z","iopub.execute_input":"2025-07-11T04:34:21.677652Z","iopub.status.idle":"2025-07-11T04:34:21.692598Z","shell.execute_reply.started":"2025-07-11T04:34:21.677632Z","shell.execute_reply":"2025-07-11T04:34:21.692000Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n        1159, 8013]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])}\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# 构建模型初始化可读标签参数，\nid2lbl = {i:tag for i,tag in enumerate(tags)}\nlbl2id = {tag:i for i,tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"bert-base-chinese\" , # 预训练模型\n                                                       num_labels = len(tags) ,  # 输出的分类数量\n                                                       id2label = id2lbl , \n                                                       label2id = lbl2id)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:30.750940Z","iopub.execute_input":"2025-07-11T04:34:30.751284Z","iopub.status.idle":"2025-07-11T04:34:33.718485Z","shell.execute_reply.started":"2025-07-11T04:34:30.751256Z","shell.execute_reply":"2025-07-11T04:34:33.717755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ceb7175ade44949818d0fb56a7eada"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"# 自动填充对其\ndata_collator = DataCollatorForTokenClassification(tokenizer = tokenizer , padding = True)\n# 在DataLoader中使用\ntrain_dl = DataLoader(\n    ds1['train'], \n    batch_size=16,\n    shuffle = True,\n    collate_fn = data_collator\n)\n\n\nmodel.to('cuda')\n\n# 模型参数分组\n\n# 获取模型参数\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    # 获取预训练模型\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},  # 预训练模型的学习率较低 保持稳定性\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3} # 新的分类层学习率较高 更好的学习，'weight_decay':0.1 使用正则化L2\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\n\n# 步长 从初始设置值到0 衰减需要的步长\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            # 预热 从0到初始设置值的步长\n                                            num_warmup_steps=100, \n                                            # 衰减 从初始设置值到0 衰减需要的步长\n                                            num_training_steps=train_steps)\n\n\nfor item in train_dl:\n    print(item['input_ids'].shape, \n          item['token_type_ids'].shape, \n          item['attention_mask'].shape, \n          item['labels'].shape)\n    break\n\nDEVICE='cuda'\n\nfor epoch in range(5):\n    model.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        # 张量移动到指定的设备商\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        # 数据传入模型\n        outputs = model(**items)\n        # 计算损失\n        loss = outputs.loss\n        # 反向传播计算梯度\n        loss.backward()\n        # 更新模型参数的梯度\n        optimizer.step()\n        # 更新学习率\n        scheduler.step()\n        # 模型参数的梯度清零\n        optimizer.zero_grad()\n    \n        tpbar.set_description(f'Epoch:{epoch+1} ' + \n                          f'bert_lr:{scheduler.get_lr()[0]} ' + \n                          f'classifier_lr:{scheduler.get_lr()[1]} '+\n                          f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T04:34:50.779389Z","iopub.execute_input":"2025-07-11T04:34:50.780154Z","iopub.status.idle":"2025-07-11T05:57:56.561393Z","shell.execute_reply.started":"2025-07-11T04:34:50.780124Z","shell.execute_reply":"2025-07-11T05:57:56.560768Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 90]) torch.Size([16, 90]) torch.Size([16, 90]) torch.Size([16, 90])\n","output_type":"stream"},{"name":"stderr","text":"Epoch:1 bert_lr:8.057286072323668e-06 classifier_lr:0.0008057286072323666 Loss:0.0498: 100%|██████████| 2813/2813 [16:35<00:00,  2.83it/s]\nEpoch:2 bert_lr:6.042964554242751e-06 classifier_lr:0.000604296455424275 Loss:0.0105: 100%|██████████| 2813/2813 [16:37<00:00,  2.82it/s]  \nEpoch:3 bert_lr:4.028643036161834e-06 classifier_lr:0.0004028643036161833 Loss:0.0121: 100%|██████████| 2813/2813 [16:38<00:00,  2.82it/s]  \nEpoch:4 bert_lr:2.014321518080917e-06 classifier_lr:0.00020143215180809166 Loss:0.0001: 100%|██████████| 2813/2813 [16:33<00:00,  2.83it/s] \nEpoch:5 bert_lr:0.0 classifier_lr:0.0 Loss:0.0001: 100%|██████████| 2813/2813 [16:41<00:00,  2.81it/s]                                      \n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"def compute_metric(result):\n    # 传入的result是一个元祖 (predicts,labels)\n\n    # 加载序列标注评估指标库\n    seqeval = evaluate.load('seqeval')\n    # 解构模型输出的结果\n    predicts,labels = result\n    # 沿着axis = 2 的维度 取最大值索引 然后将predicts转换为预测标签ID\n    predicts = np.argmax(predicts , axis = 2)\n    # 准备评估数据 将数字ID转换为文本标签 并且过滤填充数值-100\n    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    results = seqeval.compute(predictions = predicts , references = labels)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:52.359981Z","iopub.execute_input":"2025-07-11T06:36:52.360592Z","iopub.status.idle":"2025-07-11T06:36:52.365753Z","shell.execute_reply.started":"2025-07-11T06:36:52.360568Z","shell.execute_reply":"2025-07-11T06:36:52.364957Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer , padding = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:55.953626Z","iopub.execute_input":"2025-07-11T06:36:55.953883Z","iopub.status.idle":"2025-07-11T06:36:55.957721Z","shell.execute_reply.started":"2025-07-11T06:36:55.953862Z","shell.execute_reply":"2025-07-11T06:36:55.956898Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir = 'ner_train' , # 设置模型输出目录\n    num_train_epochs = 3 , # 训练轮数\n    #save_safetensor = False # 模型禁止保存safe格式 可以用troch.load加载\n    per_device_train_batch_size = 32 , # 训练批次\n    per_device_eval_batch_size = 32 ,  # 评估批次\n    report_to = 'tensorboard' , # 设置训练输出记录为tensorboard\n    eval_strategy = 'epoch'  # 每轮评估一次\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:36:58.449836Z","iopub.execute_input":"2025-07-11T06:36:58.450070Z","iopub.status.idle":"2025-07-11T06:36:58.484696Z","shell.execute_reply.started":"2025-07-11T06:36:58.450053Z","shell.execute_reply":"2025-07-11T06:36:58.484042Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"trainer = Trainer(\n    model = model ,  # 指定模型\n    args = args , # 指定设置参数\n    train_dataset = ds1['train'] ,  # 输入训练数据\n    eval_dataset = ds1['test'] ,  # 输入评估数据\n    compute_metrics = compute_metric , # 指定评估函数\n    data_collator = data_collator  # 指定数据收集器\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:37:00.324017Z","iopub.execute_input":"2025-07-11T06:37:00.324243Z","iopub.status.idle":"2025-07-11T06:37:00.601153Z","shell.execute_reply.started":"2025-07-11T06:37:00.324227Z","shell.execute_reply":"2025-07-11T06:37:00.600605Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T06:37:03.217858Z","iopub.execute_input":"2025-07-11T06:37:03.218114Z","iopub.status.idle":"2025-07-11T07:15:47.910873Z","shell.execute_reply.started":"2025-07-11T06:37:03.218094Z","shell.execute_reply":"2025-07-11T07:15:47.910342Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2112' max='2112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2112/2112 38:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Loc</th>\n      <th>Org</th>\n      <th>Per</th>\n      <th>Overall Precision</th>\n      <th>Overall Recall</th>\n      <th>Overall F1</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.014400</td>\n      <td>0.033448</td>\n      <td>{'precision': 0.9479092841956059, 'recall': 0.9379382889200562, 'f1': 0.9428974268593585, 'number': 2852}</td>\n      <td>{'precision': 0.8187456926257753, 'recall': 0.9, 'f1': 0.8574521833273187, 'number': 1320}</td>\n      <td>{'precision': 0.9413298565840938, 'recall': 0.9607451763140386, 'f1': 0.9509384260783669, 'number': 1503}</td>\n      <td>0.913897</td>\n      <td>0.935154</td>\n      <td>0.924403</td>\n      <td>0.991583</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.010200</td>\n      <td>0.031717</td>\n      <td>{'precision': 0.9583184894905593, 'recall': 0.9431977559607293, 'f1': 0.950698003180774, 'number': 2852}</td>\n      <td>{'precision': 0.8731778425655977, 'recall': 0.9075757575757576, 'f1': 0.8900445765230313, 'number': 1320}</td>\n      <td>{'precision': 0.9519104084321476, 'recall': 0.9614105123087159, 'f1': 0.9566368752068851, 'number': 1503}</td>\n      <td>0.936107</td>\n      <td>0.939736</td>\n      <td>0.937918</td>\n      <td>0.993087</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.003000</td>\n      <td>0.038207</td>\n      <td>{'precision': 0.9593292900463789, 'recall': 0.9428471248246845, 'f1': 0.9510167992926614, 'number': 2852}</td>\n      <td>{'precision': 0.8776097912167027, 'recall': 0.9234848484848485, 'f1': 0.8999630860095975, 'number': 1320}</td>\n      <td>{'precision': 0.9572086899275839, 'recall': 0.9673985362608117, 'f1': 0.9622766379880874, 'number': 1503}</td>\n      <td>0.938890</td>\n      <td>0.944846</td>\n      <td>0.941858</td>\n      <td>0.993504</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a0cedcd9a404e53a6d18bfd84adb77a"}},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'precision': 0.9479092841956059, 'recall': 0.9379382889200562, 'f1': 0.9428974268593585, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8187456926257753, 'recall': 0.9, 'f1': 0.8574521833273187, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9413298565840938, 'recall': 0.9607451763140386, 'f1': 0.9509384260783669, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.9583184894905593, 'recall': 0.9431977559607293, 'f1': 0.950698003180774, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8731778425655977, 'recall': 0.9075757575757576, 'f1': 0.8900445765230313, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9519104084321476, 'recall': 0.9614105123087159, 'f1': 0.9566368752068851, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.9593292900463789, 'recall': 0.9428471248246845, 'f1': 0.9510167992926614, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8776097912167027, 'recall': 0.9234848484848485, 'f1': 0.8999630860095975, 'number': 1320}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9572086899275839, 'recall': 0.9673985362608117, 'f1': 0.9622766379880874, 'number': 1503}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2112, training_loss=0.008169087835333565, metrics={'train_runtime': 2324.0708, 'train_samples_per_second': 58.089, 'train_steps_per_second': 0.909, 'total_flos': 1.180990200098808e+16, 'train_loss': 0.008169087835333565, 'epoch': 3.0})"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"from transformers import pipeline\n\npipeline = pipeline('token-classification', 'ner_train/checkpoint-2112')\n\ntext = pipeline('双方确定了今后发展中美关系的指导方针')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T07:18:51.301395Z","iopub.execute_input":"2025-07-11T07:18:51.302135Z","iopub.status.idle":"2025-07-11T07:18:51.547399Z","shell.execute_reply.started":"2025-07-11T07:18:51.302110Z","shell.execute_reply":"2025-07-11T07:18:51.546822Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":81}]}