{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile ddp.py\n\nimport subprocess\nimport sys\n\nsubprocess.run(\n    [sys.executable, \"-m\", \"pip\", \"install\", \"evaluate\", \"-q\"],\n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.DEVNULL\n)\n\nimport os\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom tqdm import tqdm\nimport json\n\nfrom transformers import (\n    AutoModelForTokenClassification, \n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    TrainingArguments, \n    Trainer,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset\nimport evaluate\n\n# ===== 数据处理模块 =====\ndef load_and_prepare_data():\n    \"\"\"加载并预处理数据集\"\"\"\n    # 加载数据集\n    ds = load_dataset('nlhappy/CLUE-NER')\n    \n    # 定义实体类别和标签\n    entities = ['O'] + list({\n        'movie', 'name', 'game', 'address', 'position', \n        'company', 'scene', 'book', 'organization', 'government'\n    })\n    \n    tags = ['O']\n    for entity in entities[1:]:\n        tags.append('B-' + entity.upper())\n        tags.append('I-' + entity.upper())\n    \n    entity_index = {entity: i for i, entity in enumerate(entities)}\n    \n    return ds, tags, entity_index\n\ndef process_entity_tags(ds, entity_index):\n    \"\"\"处理实体标签\"\"\"\n    def entity_tags_proc(item):\n        text_len = len(item['text'])\n        tags = [0] * text_len\n        entities = item['ents']\n        for ent in entities:\n            indices = ent['indices']\n            label = ent['label']\n            tags[indices[0]] = entity_index[label] * 2 - 1\n            for idx in indices[1:]:\n                tags[idx] = entity_index[label] * 2\n        return {'ent_tag': tags}\n    \n    return ds.map(entity_tags_proc)\n\ndef tokenize_data(ds, tokenizer):\n    \"\"\"分词处理\"\"\"\n    def data_input_proc(item):\n        batch_texts = [list(text) for text in item['text']]\n        input_data = tokenizer(\n            batch_texts, \n            truncation=True, \n            add_special_tokens=False, \n            max_length=512,\n            is_split_into_words=True, \n            padding='max_length'\n        )\n        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n        return input_data\n    \n    return ds.map(data_input_proc, batched=True)\n\n# ===== 模型训练模块 =====\ndef create_model_and_optimizer(tags, learning_rates=None):\n    \"\"\"创建模型和优化器\"\"\"\n    if learning_rates is None:\n        learning_rates = {'bert': 1e-5, 'classifier': 1e-3}\n    \n    # 创建标签映射\n    id2lbl = {i: tag for i, tag in enumerate(tags)}\n    lbl2id = {tag: i for i, tag in enumerate(tags)}\n    \n    # 加载模型\n    model = AutoModelForTokenClassification.from_pretrained(\n        'google-bert/bert-base-chinese',\n        num_labels=len(tags),\n        id2label=id2lbl,\n        label2id=lbl2id\n    )\n    \n    # 参数分组\n    param_optimizer = list(model.named_parameters())\n    bert_params = [params for name, params in param_optimizer if 'bert' in name]\n    classifier_params = [params for name, params in param_optimizer if 'bert' not in name]\n    \n    param_groups = [\n        {'params': bert_params, 'lr': learning_rates['bert']},\n        {'params': classifier_params, 'weight_decay': 0.1, 'lr': learning_rates['classifier']}\n    ]\n    \n    optimizer = optim.AdamW(param_groups)\n    return model, optimizer\n\ndef setup_distributed(rank, world_size):\n    \"\"\"设置分布式训练环境\"\"\"\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\ndef cleanup_distributed():\n    \"\"\"清理分布式训练环境\"\"\"\n    dist.destroy_process_group()\n\ndef train_ddp(rank, world_size, train_dataset, tags, epochs=3, batch_size=16, use_amp=False):\n    \"\"\"分布式DDP训练函数\"\"\"\n    print(f\"Running DDP on rank {rank}\")\n    setup_distributed(rank, world_size)\n    \n    # 创建模型和优化器\n    model, optimizer = create_model_and_optimizer(tags)\n    model = model.to(rank)\n    model = DDP(model, device_ids=[rank])\n    \n    # 创建分布式数据加载器\n    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size,\n        sampler=train_sampler,\n        pin_memory=True\n    )\n    \n    # 学习率调度器\n    train_steps = len(train_loader) * epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=100,\n        num_training_steps=train_steps\n    )\n    \n    # 混合精度训练\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n    \n    # 训练循环\n    for epoch in range(epochs):\n        train_sampler.set_epoch(epoch)\n        model.train()\n        \n        if rank == 0:\n            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n        else:\n            pbar = train_loader\n            \n        total_loss = 0\n        for batch_idx, items in enumerate(pbar):\n            items = {k: v.to(rank) for k, v in items.items()}\n            optimizer.zero_grad()\n            \n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    outputs = model(**items)\n                loss = outputs.loss\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(**items)\n                loss = outputs.loss\n                loss.backward()\n                optimizer.step()\n            \n            scheduler.step()\n            total_loss += loss.item()\n            \n            if rank == 0 and isinstance(pbar, tqdm):\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        \n        if rank == 0:\n            avg_loss = total_loss / len(train_loader)\n            print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n    \n    # 保存模型 (只在rank 0保存)\n    if rank == 0:\n        save_model(model.module, tags, \"/kaggle/working/ner_ddp_model\")\n        print(\"DDP训练完成，模型已保存！\")\n    \n    cleanup_distributed()\n\ndef save_model(model, tags, save_dir):\n    \"\"\"保存模型和配置\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # 保存模型权重\n    torch.save(model.state_dict(), os.path.join(save_dir, \"/kaggle/working/model_weights.pth\"))\n    \n    # 保存标签映射\n    label_config = {\n        'tags': tags,\n        'id2label': {i: tag for i, tag in enumerate(tags)},\n        'label2id': {tag: i for i, tag in enumerate(tags)}\n    }\n    with open(os.path.join(save_dir, \"/kaggle/working/label_config.json\"), 'w', encoding='utf-8') as f:\n        json.dump(label_config, f, indent=2, ensure_ascii=False)\n    \n    print(f\"模型已保存到: {save_dir}\")\n\ndef load_model(save_dir):\n    \"\"\"加载保存的模型\"\"\"\n    # 加载标签配置\n    with open(os.path.join(save_dir, \"/kaggle/working/label_config.json\"), 'r', encoding='utf-8') as f:\n        label_config = json.load(f)\n    \n    tags = label_config['tags']\n    id2label = label_config['id2label']\n    label2id = label_config['label2id']\n    \n    # 创建模型\n    model = AutoModelForTokenClassification.from_pretrained(\n        'google-bert/bert-base-chinese',\n        num_labels=len(tags),\n        id2label=id2label,\n        label2id=label2id\n    )\n    \n    # 加载权重\n    model.load_state_dict(torch.load(os.path.join(save_dir, \"/kaggle/working/model_weights.pth\")))\n    \n    # 加载分词器\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    \n    return model, tokenizer, tags\n\ndef predict_entities(model, tokenizer, tags, text):\n    \"\"\"推理函数\"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    \n    # 处理输入文本\n    chars = list(text)\n    inputs = tokenizer(\n        chars,\n        return_tensors='pt',\n        is_split_into_words=True,\n        padding=True,\n        truncation=True,\n        max_length=512\n    )\n    \n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # 预测\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n    \n    # 解析结果\n    predicted_labels = [tags[pred.item()] for pred in predictions[0]]\n    \n    # 提取实体\n    entities = []\n    current_entity = None\n    \n    for i, (char, label) in enumerate(zip(chars, predicted_labels)):\n        if label.startswith('B-'):\n            if current_entity:\n                entities.append(current_entity)\n            current_entity = {\n                'text': char,\n                'label': label[2:],\n                'start': i,\n                'end': i + 1\n            }\n        elif label.startswith('I-') and current_entity and label[2:] == current_entity['label']:\n            current_entity['text'] += char\n            current_entity['end'] = i + 1\n        else:\n            if current_entity:\n                entities.append(current_entity)\n                current_entity = None\n    \n    if current_entity:\n        entities.append(current_entity)\n    \n    return entities\n\ndef compute_metrics(eval_pred):\n    \"\"\"计算评估指标\"\"\"\n    seqeval = evaluate.load('seqeval')\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=2)\n    \n    # 获取标签映射\n    _, tags, _ = load_and_prepare_data()\n    \n    # 准备评估数据\n    true_predictions = [\n        [tags[p] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [tags[l] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    return seqeval.compute(predictions=true_predictions, references=true_labels)\n\ndef inference_demo():\n    \"\"\"推理演示\"\"\"\n    print(\"加载模型进行推理演示...\")\n    try:\n        model, tokenizer, tags = load_model(\"/kaggle/working/ner_ddp_model\")\n        model.eval()\n        \n        # 测试文本\n        test_texts = [\n            \"我在北京大学学习人工智能\",\n            \"张三在腾讯公司工作\",\n            \"他喜欢看电影《阿甘正传》\"\n        ]\n        \n        for text in test_texts:\n            print(f\"\\n输入文本: {text}\")\n            entities = predict_entities(model, tokenizer, tags, text)\n            print(\"识别的实体:\")\n            for entity in entities:\n                print(f\"  - {entity['text']} ({entity['label']}) [{entity['start']}:{entity['end']}]\")\n    \n    except FileNotFoundError:\n        print(\"未找到保存的模型，请先进行训练！\")\n\ndef main():\n    \n\n    ds, tags, entity_index = load_and_prepare_data()\n    \n \n    ds1 = process_entity_tags(ds, entity_index)\n    \n   \n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    ds2 = tokenize_data(ds1, tokenizer)\n    \n\n    model, optimizer = create_model_and_optimizer(tags)\n    \n\n    world_size = torch.cuda.device_count()\n    if world_size < 2:\n        print(\"警告: 只检测到一个GPU\")\n        world_size = 1\n    \n    print(f\"使用 {world_size} 个GPU进行分布式训练...\")\n    ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n    \n    if world_size > 1:\n        mp.spawn(train_ddp, args=(world_size, ds2['train'], tags), nprocs=world_size, join=True)\n    else:\n        # 单GPU情况下直接训练\n        train_ddp(0, 1, ds2['train'], tags)\n\n    print(\"训练完成！\")\n\n    inference_demo()\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:48:15.282332Z","iopub.execute_input":"2025-07-12T16:48:15.283219Z","iopub.status.idle":"2025-07-12T16:48:15.294683Z","shell.execute_reply.started":"2025-07-12T16:48:15.283185Z","shell.execute_reply":"2025-07-12T16:48:15.294058Z"}},"outputs":[{"name":"stdout","text":"Writing ddp.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"! python ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:48:43.923417Z","iopub.execute_input":"2025-07-12T16:48:43.924150Z"}},"outputs":[{"name":"stdout","text":"2025-07-12 16:48:50.719703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752338930.752033     141 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752338930.759022     141 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nREADME.md: 100%|██████████████████████████████| 21.0/21.0 [00:00<00:00, 177kB/s]\ndataset_infos.json: 100%|██████████████████████| 970/970 [00:00<00:00, 8.67MB/s]\n(…)-00000-of-00001-a33d0e4276aef9b4.parquet: 100%|█| 1.30M/1.30M [00:00<00:00, 3\n(…)-00000-of-00001-07f476b71c5edde6.parquet: 100%|█| 178k/178k [00:00<00:00, 92.\nGenerating train split: 100%|██| 10748/10748 [00:00<00:00, 248962.46 examples/s]\nGenerating validation split: 100%|█| 1343/1343 [00:00<00:00, 358763.79 examples/\nMap: 100%|██████████████████████| 10748/10748 [00:01<00:00, 10379.60 examples/s]\nMap: 100%|████████████████████████| 1343/1343 [00:00<00:00, 10123.59 examples/s]\ntokenizer_config.json: 100%|██████████████████| 49.0/49.0 [00:00<00:00, 347kB/s]\nconfig.json: 100%|█████████████████████████████| 624/624 [00:00<00:00, 5.63MB/s]\nvocab.txt: 110kB [00:00, 14.1MB/s]\ntokenizer.json: 269kB [00:00, 61.9MB/s]\nMap: 100%|███████████████████████| 10748/10748 [00:05<00:00, 2083.85 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 2336.85 examples/s]\nmodel.safetensors: 100%|██████████████████████| 412M/412M [00:02<00:00, 203MB/s]\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n使用 2 个GPU进行分布式训练...\n2025-07-12 16:49:16.798006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-07-12 16:49:16.803220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752338956.820519     188 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752338956.824875     187 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752338956.827589     188 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1752338956.831650     187 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nRunning DDP on rank 0\nRunning DDP on rank 1\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/3:  17%|███               | 57/336 [01:28<07:50,  1.69s/it, loss=0.1884]","output_type":"stream"}],"execution_count":null}]}
