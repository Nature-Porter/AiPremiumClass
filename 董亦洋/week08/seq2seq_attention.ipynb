{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11545133,"sourceType":"datasetVersion","datasetId":7240138},{"sourceId":11546953,"sourceType":"datasetVersion","datasetId":7241242}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:05:35.944422Z","iopub.execute_input":"2025-05-10T17:05:35.945302Z","iopub.status.idle":"2025-05-10T17:05:35.973925Z","shell.execute_reply.started":"2025-05-10T17:05:35.945270Z","shell.execute_reply":"2025-05-10T17:05:35.973344Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/testdate/in.txt\n/kaggle/input/couplet/vocab.bin\n/kaggle/input/couplet/encoder.json\n/kaggle/input/couplet/decoder.json\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# 编码器\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, dropout):\n        super(Encoder, self).__init__()\n        # 定义嵌入层\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        # 定义GRU层\n        self.rnn = nn.GRU(emb_dim, hidden_dim,dropout=dropout, \n                          batch_first=True, bidirectional=True)\n\n    def forward(self, token_seq):\n        # token_seq: [batch_size, seq_len]\n        # embedded: [batch_size, seq_len, emb_dim]\n        embedded = self.embedding(token_seq)\n\n        # outputs: [batch_size, seq_len, hidden_dim * 2]\n        # hidden: [2, batch_size, hidden_dim]\n        outputs, hidden = self.rnn(embedded)\n\n        # 返回，Encoder最后一个时间步的隐藏状态(拼接)\n        # return outputs[:, -1, :]\n        # 返回最后一个时间步的隐藏状态(拼接)\n        return torch.cat((hidden[0], hidden[1]), dim=1), outputs\n        # 返回最后一个时间步的隐状态（相加）\n        # return hidden.sum(dim=0)\n\n# Attention层\nclass Attention(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, enc_output, dec_output):\n        # a_t = h_t @ h_s  \n        a_t = torch.bmm(enc_output, dec_output.permute(0, 2, 1))\n        # 1.计算 结合解码token和编码token，关联的权重\n        a_t = torch.softmax(a_t, dim=1)\n        # 2.计算 关联权重和编码token 贡献值\n        c_t = torch.bmm(a_t.permute(0, 2, 1), enc_output)\n        return c_t\n\n# 解码器\nclass Decoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, dropout):\n        super(Decoder, self).__init__()\n        # 定义嵌入层\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        # 定义GRU层\n        self.rnn = nn.GRU(emb_dim, hidden_dim*2 , dropout=dropout,\n                          batch_first=True)\n        # 定义线性层\n        self.fc = nn.Linear(hidden_dim*2 , input_dim)  # 解码词典中词汇概率\n        # attention层\n        self.atteniton = Attention()\n        # attention结果转换线性层\n        self.atteniton_fc = nn.Linear(hidden_dim * 4, hidden_dim*2)\n\n    def forward(self, token_seq, hidden_state, enc_output):\n        # token_seq: [batch_size, seq_len]\n        # embedded: [batch_size, seq_len, emb_dim]\n        embedded = self.embedding(token_seq)\n\n        # outputs: [batch_size, seq_len, hidden_dim * 2]\n        # hidden: [1, batch_size, hidden_dim * 2]\n        dec_output, hidden = self.rnn(embedded, hidden_state.unsqueeze(0))\n\n        # attention运算\n        c_t = self.atteniton(enc_output, dec_output)\n        # [attention, dec_output]\n        cat_output = torch.cat((c_t, dec_output), dim=-1)\n        # 线性运算\n        out = torch.tanh(self.atteniton_fc(cat_output))\n\n        # logits: [batch_size, seq_len, input_dim]\n        logits = self.fc(out)\n        return logits, hidden\n\nclass Seq2Seq(nn.Module):\n\n    def __init__(self,\n                 enc_emb_size, \n                 dec_emb_size,\n                 emb_dim,\n                 hidden_size,\n                 dropout=0.5,\n                 ):\n        \n        super().__init__()\n\n        # encoder\n        self.encoder = Encoder(enc_emb_size, emb_dim, hidden_size, dropout=dropout)\n        # decoder\n        self.decoder = Decoder(dec_emb_size, emb_dim, hidden_size, dropout=dropout)\n\n\n    def forward(self, enc_input, dec_input):\n        # encoder last hidden state\n        encoder_state, enc_output = self.encoder(enc_input)\n        output,hidden = self.decoder(dec_input, encoder_state, enc_output)\n\n        return output,hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:05:35.975050Z","iopub.execute_input":"2025-05-10T17:05:35.975252Z","iopub.status.idle":"2025-05-10T17:05:35.985919Z","shell.execute_reply.started":"2025-05-10T17:05:35.975236Z","shell.execute_reply":"2025-05-10T17:05:35.985242Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\ndef get_proc(enc_voc, dec_voc):\n\n    # 嵌套函数定义\n    # 外部函数变量生命周期会延续到内部函数调用结束 （闭包）\n\n    def batch_proc(data):\n        \"\"\"\n        批次数据处理并返回\n        \"\"\"\n        enc_ids, dec_ids, labels = [],[],[]\n        for enc,dec in data:\n            # token -> token index\n            enc_idx = [enc_voc[tk] for tk in enc]\n            dec_idx = [dec_voc[tk] for tk in dec]\n\n            # encoder_input\n            enc_ids.append(torch.tensor(enc_idx))\n            # decoder_input\n            dec_ids.append(torch.tensor(dec_idx[:-1]))\n            # label\n            labels.append(torch.tensor(dec_idx[1:]))\n\n        \n        # 数据转换张量 [batch, max_token_len]\n        # 用批次中最长token序列构建张量\n        enc_input = pad_sequence(enc_ids, batch_first=True)\n        dec_input = pad_sequence(dec_ids, batch_first=True)\n        targets = pad_sequence(labels, batch_first=True)\n\n        # 返回数据都是模型训练和推理的需要\n        return enc_input, dec_input, targets\n\n    # 返回回调函数\n    return batch_proc   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:05:35.986518Z","iopub.execute_input":"2025-05-10T17:05:35.986774Z","iopub.status.idle":"2025-05-10T17:05:36.011283Z","shell.execute_reply.started":"2025-05-10T17:05:35.986755Z","shell.execute_reply":"2025-05-10T17:05:36.010741Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import pickle\nimport torch\nimport json\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda')\n\n# 加载训练数据\nwith open('/kaggle/input/couplet/vocab.bin','rb') as f:\n    evoc,dvoc = pickle.load(f)\n\nwith open('/kaggle/input/couplet/encoder.json') as f:\n    enc_data = json.load(f)\nwith open('/kaggle/input/couplet/decoder.json') as f:\n    dec_data = json.load(f)\n\nds = list(zip(enc_data,dec_data))\ndl = DataLoader(ds, batch_size=256, shuffle=True, collate_fn=get_proc(evoc, dvoc))\n\n# 构建训练模型\n# 模型构建\nmodel = Seq2Seq(\n    enc_emb_size=len(evoc),\n    dec_emb_size=len(dvoc),\n    emb_dim=100,\n    hidden_size=120,\n    dropout=0.5,\n)\nmodel.to(device)\n\n# 优化器、损失\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# 训练\nfor epoch in range(20):\n    model.train()\n    tpbar = tqdm(dl)\n    for enc_input, dec_input, targets in tpbar:\n        enc_input = enc_input.to(device)\n        dec_input = dec_input.to(device)\n        targets = targets.to(device)\n\n        # 前向传播 \n        logits, _ = model(enc_input, dec_input)\n\n        # 计算损失\n        # CrossEntropyLoss需要将logits和targets展平\n        # logits: [batch_size, seq_len, vocab_size]\n        # targets: [batch_size, seq_len]\n        # 展平为 [batch_size * seq_len, vocab_size] 和 [batch_size * seq_len]\n        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tpbar.set_description(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n\ntorch.save(model.state_dict(), '/kaggle/working/seq2seq_state_add.bin')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:05:36.012656Z","iopub.execute_input":"2025-05-10T17:05:36.012877Z","iopub.status.idle":"2025-05-10T17:58:45.770047Z","shell.execute_reply.started":"2025-05-10T17:05:36.012863Z","shell.execute_reply":"2025-05-10T17:58:45.769455Z"}},"outputs":[{"name":"stderr","text":"Epoch 1, Loss: 1.8048: 100%|██████████| 3010/3010 [02:38<00:00, 18.96it/s]\nEpoch 2, Loss: 1.3345: 100%|██████████| 3010/3010 [02:38<00:00, 18.96it/s]\nEpoch 3, Loss: 1.3399: 100%|██████████| 3010/3010 [02:38<00:00, 18.94it/s]\nEpoch 4, Loss: 1.3400: 100%|██████████| 3010/3010 [02:39<00:00, 18.84it/s]\nEpoch 5, Loss: 1.3258: 100%|██████████| 3010/3010 [02:38<00:00, 18.95it/s]\nEpoch 6, Loss: 1.3453: 100%|██████████| 3010/3010 [02:38<00:00, 18.93it/s]\nEpoch 7, Loss: 1.4967: 100%|██████████| 3010/3010 [02:39<00:00, 18.91it/s]\nEpoch 8, Loss: 1.2877: 100%|██████████| 3010/3010 [02:39<00:00, 18.83it/s]\nEpoch 9, Loss: 1.2859: 100%|██████████| 3010/3010 [02:38<00:00, 18.96it/s]\nEpoch 10, Loss: 1.4259: 100%|██████████| 3010/3010 [02:38<00:00, 18.95it/s]\nEpoch 11, Loss: 1.3544: 100%|██████████| 3010/3010 [02:39<00:00, 18.84it/s]\nEpoch 12, Loss: 1.3631: 100%|██████████| 3010/3010 [02:39<00:00, 18.89it/s]\nEpoch 13, Loss: 1.5249: 100%|██████████| 3010/3010 [02:39<00:00, 18.93it/s]\nEpoch 14, Loss: 1.3131: 100%|██████████| 3010/3010 [02:39<00:00, 18.90it/s]\nEpoch 15, Loss: 1.2132: 100%|██████████| 3010/3010 [02:39<00:00, 18.84it/s]\nEpoch 16, Loss: 1.2097: 100%|██████████| 3010/3010 [02:39<00:00, 18.92it/s]\nEpoch 17, Loss: 1.2895: 100%|██████████| 3010/3010 [02:39<00:00, 18.92it/s]\nEpoch 19, Loss: 1.3187: 100%|██████████| 3010/3010 [02:39<00:00, 18.90it/s]\nEpoch 20, Loss: 1.2622: 100%|██████████| 3010/3010 [02:39<00:00, 18.91it/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport pickle\n\n# 加载训练好的模型和词典\nstate_dict = torch.load('/kaggle/working/seq2seq_state_add.bin')\nwith open('/kaggle/input/couplet/vocab.bin','rb') as f:\n    evoc,dvoc = pickle.load(f)\n\nmodel = Seq2Seq(\n    enc_emb_size=len(evoc),\n    dec_emb_size=len(dvoc),\n    emb_dim=100,\n    hidden_size=120,\n    dropout=0.5,\n)\nmodel.load_state_dict(state_dict)\n\n# 创建解码器反向字典\ndvoc_inv = {v:k for k,v in dvoc.items()}\n\ndef test(in_file):\n    with open(in_file,'r',encoding='utf-8') as f, open('/kaggle/working/test_out_add.txt','w',encoding='utf-8') as w:\n        lines = f.read().split('\\n')\n        for line in lines:\n            #空数据排除\n            if line == '':\n                continue\n            # 用户输入\n            enc_idx = torch.tensor([[evoc.get(tk, evoc['UNK']) for tk in line.split(' ')[:-1]]])\n\n            # 推理\n            # 最大解码长度=输入长度\n            max_dec_len = len(line.split(' ')[:-1])\n\n            model.eval()\n            with torch.no_grad():\n                # 编码器\n                hidden_state, enc_outputs = model.encoder(enc_idx)\n\n                # 解码器输入 shape [1,1]\n                dec_input = torch.tensor([[dvoc['BOS']]])\n\n                # 循环decoder\n                dec_tokens = []\n                while True:\n                    if len(dec_tokens) >= max_dec_len:\n                        break\n                    # 解码器 \n                    # logits: [1,1,dec_voc_size]\n                    # logits,hidden_state = model.decoder(dec_input, hidden_state)\n                    logits,hidden_state = model.decoder(dec_input, hidden_state, enc_outputs)\n                    \n                    # 下个token index\n                    next_token = torch.argmax(logits, dim=-1)\n\n                    if dvoc_inv[next_token.squeeze().item()] == 'EOS':\n                        break\n                    # 收集每次token_index 【解码集合】\n                    dec_tokens.append(next_token.squeeze().item())\n                    # decoder的下一个输入 = token_index\n                    dec_input = next_token\n                    hidden_state = hidden_state.view(1, -1)\n\n            # 输出解码结果\n            w.write(''.join([dvoc_inv[tk] for tk in dec_tokens]))\n            w.write('\\n')\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:58:45.771480Z","iopub.execute_input":"2025-05-10T17:58:45.771696Z","iopub.status.idle":"2025-05-10T17:58:45.837824Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3848950086.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load('/kaggle/working/seq2seq_state_add.bin')\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"test('/kaggle/input/testdate/in.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:58:45.843683Z","iopub.execute_input":"2025-05-10T17:58:45.843970Z","iopub.status.idle":"2025-05-10T17:59:13.327395Z"}},"outputs":[],"execution_count":null}]}