{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":318737,"sourceType":"datasetVersion","datasetId":134082},{"sourceId":11933630,"sourceType":"datasetVersion","datasetId":7502701}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:44:45.154483Z","iopub.execute_input":"2025-05-27T06:44:45.154692Z","iopub.status.idle":"2025-05-27T06:44:45.434679Z","shell.execute_reply.started":"2025-05-27T06:44:45.154676Z","shell.execute_reply":"2025-05-27T06:44:45.433704Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jd_comment_with_label/jd_comment_data.xlsx\n/kaggle/input/jd-comment-data-csv/jd_comment_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### 利用huggingface中预训练模型，实现文本分类模型定制和微调\n\n1. 加载预训练模型定制输出端任务\n2. 原始数据进行清洗转换\n   - 清理停用词或非法字符\n3. 构建Dataset和DataLoader\n   - DataLoader的collate_fn参数，在回调函数中使用tokenizer转换模型输入数据\n5. 创建模型，损失函数、优化器\n6. 训练模型\n7. 观察损失调参迭代\n8. 模型保存","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:19:30.920217Z","iopub.execute_input":"2025-05-27T07:19:30.920494Z","iopub.status.idle":"2025-05-27T07:19:30.924624Z","shell.execute_reply.started":"2025-05-27T07:19:30.920473Z","shell.execute_reply":"2025-05-27T07:19:30.923936Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#数据预处理，只保留写了评价的数据\nimport csv\n\n# 用户评论数据集\nds_comments = []\n\n# 1. Read the CSV file\nwith open('/kaggle/input/jd-comment-data-csv/jd_comment_data.csv', 'r') as file:\n    reader = csv.DictReader(file)\n    for row in reader:\n        vote = int(row['评分（总分5分）(score)'])\n        content = row['评价内容(content)']\n        if content != '此用户未填写评价内容':\n            ds_comments.append([content, vote])  # 1 for positive, 0 for negative\n\nlen(ds_comments)\n\ntrain_comments = ds_comments[:round(0.8*len(ds_comments))]  # Display the first 80% of the dataset\ntest_comments = ds_comments[round(0.8*len(ds_comments)):]  # Display the last 20% of the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:19:33.777843Z","iopub.execute_input":"2025-05-27T07:19:33.778156Z","iopub.status.idle":"2025-05-27T07:19:34.211647Z","shell.execute_reply.started":"2025-05-27T07:19:33.778133Z","shell.execute_reply":"2025-05-27T07:19:34.211070Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 加载词典创建分词器\nfrom transformers import AutoTokenizer\ntokenizer=  AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:19:40.537034Z","iopub.execute_input":"2025-05-27T07:19:40.537312Z","iopub.status.idle":"2025-05-27T07:19:40.993085Z","shell.execute_reply.started":"2025-05-27T07:19:40.537291Z","shell.execute_reply":"2025-05-27T07:19:40.992261Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ndef build_collate(tokenizer):\n    def collate_fn(batch):\n        # 文本分类语料：输入语句，类别标签\n        sentents,labels = zip(*batch)\n    \n        # tokenizer转换\n        model_inputs = tokenizer(sentents, return_tensors='pt', padding = True,  truncation = True)\n        labels = torch.tensor(labels)\n\n        return model_inputs, labels\n    return collate_fn\n\n# DataLoader\ndl = DataLoader(train_comments, batch_size=20, shuffle=True, collate_fn=build_collate(tokenizer))\n\n\n\n# 使用预训练bert模型时，学习率不能太大!!! 推荐1e-4或1e-5 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:19:50.911517Z","iopub.execute_input":"2025-05-27T07:19:50.912029Z","iopub.status.idle":"2025-05-27T07:19:50.916846Z","shell.execute_reply.started":"2025-05-27T07:19:50.912005Z","shell.execute_reply":"2025-05-27T07:19:50.916018Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# 定制模型输出\nfrom transformers import AutoModelForSequenceClassification, AutoModelForMaskedLM\n\n# 完成文本分类任务(5个类别)\nmodel = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=5)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:44:58.486111Z","iopub.execute_input":"2025-05-27T06:44:58.486399Z","iopub.status.idle":"2025-05-27T06:45:14.392030Z","shell.execute_reply.started":"2025-05-27T06:44:58.486372Z","shell.execute_reply":"2025-05-27T06:45:14.391373Z"}},"outputs":[{"name":"stderr","text":"2025-05-27 06:45:00.748466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748328300.929140      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748328300.983906      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c8f1b081294ec490d1875a9467301b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 优化器、损失\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\n# 训练\n\nfor epoch in range(1):\n    model.train()\n    tpbar = tqdm(dl)\n    for model_inputs, labels in tpbar:\n        model_inputs, labels = model_inputs.to(device), labels.to(device)\n        # 前向传播\n        logits = model(model_inputs.input_ids).logits\n        #print(logits,labels)\n        # 计算损失\n        loss = criterion(logits.view(-1, 5), labels.view(-1)-1)\n\n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tpbar.set_description(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n\ntorch.save(model.state_dict(), '/kaggle/working/bert_classification_JD_Comments.bin')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:45:14.392848Z","iopub.execute_input":"2025-05-27T06:45:14.393448Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1777 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\nEpoch 1, Loss: 0.2697:  12%|█▏        | 218/1777 [01:00<06:49,  3.81it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:20:32.858121Z","iopub.execute_input":"2025-05-27T07:20:32.858404Z","iopub.status.idle":"2025-05-27T07:20:32.864955Z","shell.execute_reply.started":"2025-05-27T07:20:32.858384Z","shell.execute_reply":"2025-05-27T07:20:32.864079Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"dl_test = DataLoader(test_comments, batch_size=20, shuffle=True, collate_fn=build_collate(tokenizer))\nmodel.eval()\ncorrect,total = 0,0\ntpbar_test = tqdm(dl_test)\nwith torch.no_grad():\n    for inputs,labels in tpbar_test:\n        inputs, labels = inputs.to(device), labels.to(device)\n        out = model(inputs.input_ids).logits\n        #print(model_inputs.input_ids.shape,labels.shape,out.shape)\n        _,predicted = torch.max(out,1) \n        #print(predicted,labels)\n        correct += (predicted == (labels-1)).sum().item()\n        total += labels.shape[0]\nprint(f'{correct},{total},准确率:{correct/total*100}%')\n\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:30:15.863802Z","iopub.execute_input":"2025-05-27T07:30:15.864103Z","iopub.status.idle":"2025-05-27T07:30:44.890455Z","shell.execute_reply.started":"2025-05-27T07:30:15.864082Z","shell.execute_reply":"2025-05-27T07:30:44.889646Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 445/445 [00:29<00:00, 15.34it/s]","output_type":"stream"},{"name":"stdout","text":"8403,8884,准确率:94.58577217469607%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27}]}
