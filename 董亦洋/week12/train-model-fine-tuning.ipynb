{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T07:22:00.908597Z","iopub.execute_input":"2025-06-14T07:22:00.908763Z","iopub.status.idle":"2025-06-14T07:22:01.188979Z","shell.execute_reply.started":"2025-06-14T07:22:00.908747Z","shell.execute_reply":"2025-06-14T07:22:01.188256Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip -q install evaluate\n!pip -q install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:09:41.932574Z","iopub.execute_input":"2025-06-14T09:09:41.932872Z","iopub.status.idle":"2025-06-14T09:09:48.105405Z","shell.execute_reply.started":"2025-06-14T09:09:41.932852Z","shell.execute_reply":"2025-06-14T09:09:48.104550Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:09:50.570952Z","iopub.execute_input":"2025-06-14T09:09:50.571258Z","iopub.status.idle":"2025-06-14T09:09:50.576360Z","shell.execute_reply.started":"2025-06-14T09:09:50.571231Z","shell.execute_reply":"2025-06-14T09:09:50.575455Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:09:53.073046Z","iopub.execute_input":"2025-06-14T09:09:53.073609Z","iopub.status.idle":"2025-06-14T09:09:53.402350Z","shell.execute_reply.started":"2025-06-14T09:09:53.073586Z","shell.execute_reply":"2025-06-14T09:09:53.401569Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"# 加载hf中dataset\nds = load_dataset('doushabao4766/msra_ner_k_V3')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:49:59.089978Z","iopub.execute_input":"2025-06-14T09:49:59.090266Z","iopub.status.idle":"2025-06-14T09:49:59.930425Z","shell.execute_reply.started":"2025-06-14T09:49:59.090247Z","shell.execute_reply":"2025-06-14T09:49:59.929696Z"}},"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":136},{"cell_type":"code","source":"def data_input_proc(item):\n    # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n    input_data = tokenizer(item['tokens'], truncation=True, add_special_tokens=False, max_length=512, \n                           is_split_into_words=True, padding='max_length')\n    input_data['labels'] = [(tag + [0] * (512 - len(tag)))[:512] for tag in item['ner_tags']]\n    return input_data\n    \n\nds2 = ds.map(data_input_proc, batched=True)  # batched 每次传入自定义方法样本数量多个\nds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:50:01.706757Z","iopub.execute_input":"2025-06-14T09:50:01.707332Z","iopub.status.idle":"2025-06-14T09:50:27.490127Z","shell.execute_reply.started":"2025-06-14T09:50:01.707313Z","shell.execute_reply":"2025-06-14T09:50:27.489327Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e50df7f736448885986cede91b7a84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3d357fe4044a1aad7eebd3711c3b25"}},"metadata":{}}],"execution_count":137},{"cell_type":"code","source":"id2lbl = {'0': 'O','1': 'B-PER','2': 'I-PER','3': 'B-ORG','4': 'I-ORG','5': 'B-LOC','6': 'I-LOC'}\nlbl2id = {id2lbl[tag]:tag for tag in id2lbl}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:28:46.094500Z","iopub.execute_input":"2025-06-14T09:28:46.095031Z","iopub.status.idle":"2025-06-14T09:28:46.098640Z","shell.execute_reply.started":"2025-06-14T09:28:46.095005Z","shell.execute_reply":"2025-06-14T09:28:46.097940Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                        num_labels=len(id2lbl),\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel.to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:28:47.379869Z","iopub.execute_input":"2025-06-14T09:28:47.380393Z","iopub.status.idle":"2025-06-14T09:28:47.687680Z","shell.execute_reply.started":"2025-06-14T09:28:47.380372Z","shell.execute_reply":"2025-06-14T09:28:47.687144Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":107},{"cell_type":"markdown","source":"## 动态学习率","metadata":{}},{"cell_type":"code","source":"# dataLoader\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\n\ntrain_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n\n# 模型参数分组\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=100, \n                                            num_training_steps=train_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:51:19.234383Z","iopub.execute_input":"2025-06-14T09:51:19.234966Z","iopub.status.idle":"2025-06-14T09:51:19.275348Z","shell.execute_reply.started":"2025-06-14T09:51:19.234944Z","shell.execute_reply":"2025-06-14T09:51:19.274512Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"from tqdm import tqdm\n\nDEVICE='cuda'\n\nfor epoch in range(5):\n    model.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        optimizer.zero_grad()\n        outputs = model(**items)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    \n        tpbar.set_description(f'Epoch:{epoch+1} ' + \n                          f'bert_lr:{scheduler.get_lr()[0]} ' + \n                          f'classifier_lr:{scheduler.get_lr()[1]} '+\n                          f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T08:27:19.340696Z","iopub.execute_input":"2025-06-14T08:27:19.341671Z","iopub.status.idle":"2025-06-14T08:27:34.878440Z","shell.execute_reply.started":"2025-06-14T08:27:19.341637Z","shell.execute_reply":"2025-06-14T08:27:34.877469Z"}},"outputs":[{"name":"stderr","text":"Epoch:1 bert_lr:9.000000000000001e-07 classifier_lr:8.999999999999999e-05 Loss:1.8119:   0%|          | 9/2813 [00:15<1:20:12,  1.72s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3988290948.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                           \u001b[0;34mf'bert_lr:{scheduler.get_lr()[0]} '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                           \u001b[0;34mf'classifier_lr:{scheduler.get_lr()[1]} '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                           f'Loss:{loss.item():.4f}')\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":"## 混合精度","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\nDEVICE='cuda'\n\n# 梯度计算缩放器\nscaler = torch.GradScaler()\n\nfor epoch in range(5):\n    model.train()\n    tpbar = tqdm(train_dl)\n    for items in tpbar:\n        items = {k:v.to(DEVICE) for k,v in items.items()}\n        optimizer.zero_grad()\n\n        with torch.autocast(device_type='cuda'):\n            outputs = model(**items)\n        loss = outputs.loss\n\n        # 缩放loss后，调用backward\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n    \n        tpbar.set_description(f'Epoch:{epoch+1} ' + \n                          f'bert_lr:{scheduler.get_lr()[0]} ' + \n                          f'classifier_lr:{scheduler.get_lr()[1]} '+\n                          f'Loss:{loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:51:41.930301Z","iopub.execute_input":"2025-06-14T09:51:41.930991Z","iopub.status.idle":"2025-06-14T10:06:25.813913Z","shell.execute_reply.started":"2025-06-14T09:51:41.930969Z","shell.execute_reply":"2025-06-14T10:06:25.812664Z"}},"outputs":[{"name":"stderr","text":"Epoch:1 bert_lr:8.627998567848193e-06 classifier_lr:0.0008627998567848192 Loss:0.0029:  72%|███████▏  | 2016/2813 [14:43<05:49,  2.28it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/707467825.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 缩放loss后，调用backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":142},{"cell_type":"markdown","source":"## 分布式","metadata":{}},{"cell_type":"code","source":"%%writefile ner_ddp.py\n\nimport os\nimport numpy as np\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n# 设置分布式环境\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# 清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n    \n\ndef train(rank, world_size):\n    setup(rank, world_size)\n    # 数据集\n    ds = load_dataset('doushabao4766/msra_ner_k_V3')\n\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    \n    def data_input_proc(item):\n        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n        input_data = tokenizer(item['tokens'], truncation=True, add_special_tokens=False, max_length=512, \n                               is_split_into_words=True, padding='max_length')\n        input_data['labels'] = [(tag + [0] * (512 - len(tag)))[:512] for tag in item['ner_tags']]\n        return input_data\n        \n    ds2 = ds.map(data_input_proc, batched=True)  # batched 每次传入自定义方法样本数量多个\n    ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n    \n    id2lbl = {'0': 'O','1': 'B-PER','2': 'I-PER','3': 'B-ORG','4': 'I-ORG','5': 'B-LOC','6': 'I-LOC'}\n    lbl2id = {id2lbl[tag]:tag for tag in id2lbl}\n    \n    local_rank = rank\n    \n    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                            num_labels=len(id2lbl),\n                                                            id2label=id2lbl,\n                                                            label2id=lbl2id)\n    model.to(local_rank)\n    \n    args = TrainingArguments(\n        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n        num_train_epochs = 3,    # 训练 epoch\n        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n        per_device_train_batch_size=16,  # 训练批次\n        per_device_eval_batch_size=16,\n        report_to='tensorboard',  # 训练输出记录\n        eval_strategy=\"epoch\",\n        local_rank=local_rank,   # 当前进程 RANK\n        fp16=True,               # 使用混合精度\n        lr_scheduler_type='linear',  # 动态学习率\n        warmup_steps=100,        # 预热步数\n        ddp_find_unused_parameters=False  # 优化DDP性能\n    )\n    \n    def compute_metric(result):\n        # result 是一个tuple (predicts, labels)\n        \n        # 获取评估对象\n        seqeval = evaluate.load('seqeval')\n        predicts,labels = result\n        predicts = np.argmax(predicts, axis=2)\n        \n        # 准备评估数据\n        predicts = [[id2lbl[str(p)] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        labels = [[id2lbl[str(l)] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        results = seqeval.compute(predictions=predicts, references=labels)\n    \n        return results\n    \n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n    \n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=ds2['train'],\n        eval_dataset=ds2['test'],\n        data_collator=data_collator,\n        compute_metrics=compute_metric\n    )\n    \n    trainer.train()\n\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:22:30.943197Z","iopub.execute_input":"2025-06-14T10:22:30.943537Z","iopub.status.idle":"2025-06-14T10:22:30.953254Z","shell.execute_reply.started":"2025-06-14T10:22:30.943497Z","shell.execute_reply":"2025-06-14T10:22:30.952447Z"}},"outputs":[{"name":"stdout","text":"Overwriting ner_ddp.py\n","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"!python ner_ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T10:22:50.736869Z","iopub.execute_input":"2025-06-14T10:22:50.737229Z","iopub.status.idle":"2025-06-14T10:23:18.329178Z","shell.execute_reply.started":"2025-06-14T10:22:50.737209Z","shell.execute_reply":"2025-06-14T10:23:18.328212Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"2025-06-14 10:22:55.881491: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749896575.905887     539 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749896575.913490     539 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 10:23:05.150857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749896585.174502     553 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749896585.181345     553 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 10:23:05.274944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749896585.298400     554 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749896585.305589     554 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n  0%|                                                  | 0/4221 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n  0%|                                                  | 0/4221 [00:02<?, ?it/s]\n[rank0]:[W614 10:23:15.365239163 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n  0%|                                        | 1/4221 [00:03<3:55:33,  3.35s/it]\nW0614 10:23:16.332000 539 torch/multiprocessing/spawn.py:169] Terminating process 554 via signal SIGTERM\nTraceback (most recent call last):\n  File \"/kaggle/working/ner_ddp.py\", line 102, in <module>\n    main()\n  File \"/kaggle/working/ner_ddp.py\", line 99, in main\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 340, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 296, in start_processes\n    while not context.join():\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 215, in join\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\ntorch.multiprocessing.spawn.ProcessRaisedException: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/kaggle/working/ner_ddp.py\", line 95, in train\n    trainer.train()\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n    return inner_training_loop(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n    outputs = model(**inputs)\n              ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\", line 193, in forward\n    outputs = self.parallel_apply(replicas, inputs, module_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\", line 212, in parallel_apply\n    return parallel_apply(\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 126, in parallel_apply\n    output.reraise()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n    raise exception\ntorch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1869, in forward\n    outputs = self.bert(\n              ^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n    encoder_outputs = self.encoder(\n                      ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n    layer_outputs = layer_module(\n                    ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 627, in forward\n    layer_output = apply_chunking_to_forward(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n    return forward_fn(*input_tensors)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 639, in feed_forward_chunk\n    intermediate_output = self.intermediate(attention_output)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 539, in forward\n    hidden_states = self.dense(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 90.12 MiB is free. Process 3403 has 7.24 GiB memory in use. Process 53737 has 2.01 GiB memory in use. Process 53738 has 5.40 GiB memory in use. Of the allocated memory 1.76 GiB is allocated by PyTorch, and 58.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n","output_type":"stream"}],"execution_count":152}]}
