{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:45:33.920355Z","iopub.execute_input":"2025-05-30T15:45:33.920651Z","iopub.status.idle":"2025-05-30T15:45:33.924555Z","shell.execute_reply.started":"2025-05-30T15:45:33.920630Z","shell.execute_reply":"2025-05-30T15:45:33.923826Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:45:38.485007Z","iopub.execute_input":"2025-05-30T15:45:38.485310Z","iopub.status.idle":"2025-05-30T15:45:41.069072Z","shell.execute_reply.started":"2025-05-30T15:45:38.485290Z","shell.execute_reply":"2025-05-30T15:45:41.068512Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9dae516658c4a56b4724571c45fdb6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61cc8ab6abd84a9e8e32ba8d099cd93e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:45:43.548036Z","iopub.execute_input":"2025-05-30T15:45:43.548707Z","iopub.status.idle":"2025-05-30T15:45:43.554486Z","shell.execute_reply.started":"2025-05-30T15:45:43.548681Z","shell.execute_reply":"2025-05-30T15:45:43.553662Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:45:47.379437Z","iopub.execute_input":"2025-05-30T15:45:47.379978Z","iopub.status.idle":"2025-05-30T15:45:47.975023Z","shell.execute_reply.started":"2025-05-30T15:45:47.379956Z","shell.execute_reply":"2025-05-30T15:45:47.974447Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5fd29a75d0b4590ad9c8e16d4841bdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56b4e2cadc14b02bf625878f7881478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d7fa9a6753c49b487bbcff2f5d70672"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 模型测试\nmessage= \"命名实体识别\"\nlabel = torch.tensor([0,1,0,2,5,4])\n\nmodel_input = tokenizer([message], return_tensors='pt')\nresult = model(**model_input)\n\nprint(result.loss)\nprint(result.logits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:45:50.113375Z","iopub.execute_input":"2025-05-30T15:45:50.113993Z","iopub.status.idle":"2025-05-30T15:45:50.295697Z","shell.execute_reply.started":"2025-05-30T15:45:50.113969Z","shell.execute_reply":"2025-05-30T15:45:50.294799Z"}},"outputs":[{"name":"stdout","text":"None\ntensor([[[ 0.3389, -0.1038,  0.7628, -0.0923,  0.2169,  0.1954,  0.2398],\n         [ 0.8772,  0.0736,  0.5088,  0.4887,  0.2392,  0.3507,  0.0748],\n         [-0.0131,  0.2049, -0.5644, -0.2755, -0.7436,  0.0763,  0.3688],\n         [ 0.4493,  0.1104, -0.8357,  0.0921, -0.9937,  0.2044,  0.2632],\n         [ 0.2270,  0.2171, -0.6591, -0.2837, -0.3419,  0.0233,  0.1571],\n         [-0.1314, -0.0120, -0.2281, -0.2134, -0.2069, -0.1293,  0.5736],\n         [-0.3765, -0.4535, -0.5427, -0.4635, -0.6766, -0.2555,  0.9607],\n         [ 0.5152,  0.0869, -0.5296,  0.0581,  0.2645, -0.0101, -0.0743]]],\n       grad_fn=<ViewBackward0>)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 加载hf中dataset\nds = load_dataset('doushabao4766/msra_ner_k_V3')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:45:53.374206Z","iopub.execute_input":"2025-05-30T15:45:53.374751Z","iopub.status.idle":"2025-05-30T15:45:55.027532Z","shell.execute_reply.started":"2025-05-30T15:45:53.374728Z","shell.execute_reply":"2025-05-30T15:45:55.026678Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de27568087844c56b8d52a8e3c29c972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-42717a92413393f9.parquet:   0%|          | 0.00/13.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ca8cd711854bc49ded670f5bb9b08d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-8899cab5fdab45bc.parquet:   0%|          | 0.00/946k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7551aeece3534f17914211a6f1fdab1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"235ee359ec114b539cd1129a7305aa95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba56650a72d4093be14e12f79ce9f76"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"ds[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T13:48:36.608933Z","iopub.execute_input":"2025-05-30T13:48:36.609273Z","iopub.status.idle":"2025-05-30T13:48:36.615964Z","shell.execute_reply.started":"2025-05-30T13:48:36.609249Z","shell.execute_reply":"2025-05-30T13:48:36.615002Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n    num_rows: 45001\n})"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"## 实体映射数据集词典准备","metadata":{}},{"cell_type":"code","source":"# entity_index\nentites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n           'company', 'scene', 'book', 'organization', 'government'})\ntags = ['O']\nfor entity in entites[1:]:\n    tags.append('B-' + entity.upper())\n    tags.append('I-' + entity.upper())\n\nentity_index = {entity:i for i, entity in enumerate(entites)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:45:59.093294Z","iopub.execute_input":"2025-05-30T15:45:59.093588Z","iopub.status.idle":"2025-05-30T15:45:59.098638Z","shell.execute_reply.started":"2025-05-30T15:45:59.093566Z","shell.execute_reply":"2025-05-30T15:45:59.097952Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"entity_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T12:04:25.651865Z","iopub.execute_input":"2025-05-30T12:04:25.652560Z","iopub.status.idle":"2025-05-30T12:04:25.657978Z","shell.execute_reply.started":"2025-05-30T12:04:25.652518Z","shell.execute_reply":"2025-05-30T12:04:25.657120Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'O': 0,\n 'company': 1,\n 'game': 2,\n 'government': 3,\n 'scene': 4,\n 'book': 5,\n 'name': 6,\n 'organization': 7,\n 'address': 8,\n 'position': 9,\n 'movie': 10}"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"tags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T12:04:28.013777Z","iopub.execute_input":"2025-05-30T12:04:28.014743Z","iopub.status.idle":"2025-05-30T12:04:28.020411Z","shell.execute_reply.started":"2025-05-30T12:04:28.014688Z","shell.execute_reply":"2025-05-30T12:04:28.019377Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['O',\n 'B-COMPANY',\n 'I-COMPANY',\n 'B-GAME',\n 'I-GAME',\n 'B-GOVERNMENT',\n 'I-GOVERNMENT',\n 'B-SCENE',\n 'I-SCENE',\n 'B-BOOK',\n 'I-BOOK',\n 'B-NAME',\n 'I-NAME',\n 'B-ORGANIZATION',\n 'I-ORGANIZATION',\n 'B-ADDRESS',\n 'I-ADDRESS',\n 'B-POSITION',\n 'I-POSITION',\n 'B-MOVIE',\n 'I-MOVIE']"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"def entity_tags_proc(item):\n    # item即是dataset中记录\n    # text_len = len(item['text'])  # 根据文本长度生成tags列表\n    # tags = [0] * text_len    # 初始值为‘O’\n    # 遍历实体列表，所有实体类别标记填入tags\n    tags = item['ents']\n    # for ent in entites:\n    #     indices = ent['indices']  # 实体索引\n    #     label = ent['label']   # 实体名\n    #     tags[indices[0]] = entity_index[label] * 2 - 1\n    #     for idx in indices[1:]:\n    #         tags[idx] = entity_index[label] * 2\n    return {'ent_tag': tags}\n\n# 使用自定义回调函数处理数据集记录\nds1 = ds.map(entity_tags_proc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:46:06.898255Z","iopub.execute_input":"2025-05-30T15:46:06.898597Z","iopub.status.idle":"2025-05-30T15:46:07.028826Z","shell.execute_reply.started":"2025-05-30T15:46:06.898573Z","shell.execute_reply":"2025-05-30T15:46:07.027606Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d034491f11274e998c3363504614eb57"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/790390686.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 使用自定义回调函数处理数据集记录\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mds1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_tags_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             dataset_dict[split] = dataset.map(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3077\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3079\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3080\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3499\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3501\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3502\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3503\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3473\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3474\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3475\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3396\u001b[0m             \u001b[0;34m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3397\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3398\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3399\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/790390686.py\u001b[0m in \u001b[0;36mentity_tags_proc\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# tags = [0] * text_len    # 初始值为‘O’\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 遍历实体列表，所有实体类别标记填入tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# for ent in entites:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#     indices = ent['indices']  # 实体索引\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys_to_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'ents'"],"ename":"KeyError","evalue":"'ents'","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"# 训练集\nfor row in ds['train']:\n    print(row)\n    print(row['tokens'])\n    # print(row['tokens'])\n    # print(row['tokens'])\n    # print(row['ner_tags'])\n    # print(row['ent_tag'])\n    \n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:07:07.366399Z","iopub.execute_input":"2025-05-30T14:07:07.367374Z","iopub.status.idle":"2025-05-30T14:07:07.374087Z","shell.execute_reply.started":"2025-05-30T14:07:07.367340Z","shell.execute_reply":"2025-05-30T14:07:07.373152Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'knowledge': ''}\n['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"ds1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:28:08.209163Z","iopub.execute_input":"2025-05-30T10:28:08.210278Z","iopub.status.idle":"2025-05-30T10:28:08.216190Z","shell.execute_reply.started":"2025-05-30T10:28:08.210242Z","shell.execute_reply":"2025-05-30T10:28:08.215149Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'ent_tag'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'ent_tag'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"中文bert分词在日期时间和英文转换token过程中，出现合并。影响ner标注准确性。","metadata":{}},{"cell_type":"code","source":"token_index = tokenizer.encode('2000年2月add', add_special_tokens=False)\nprint(token_index)\ntokens = tokenizer.decode(token_index)\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:46:34.927832Z","iopub.execute_input":"2025-05-30T15:46:34.928100Z","iopub.status.idle":"2025-05-30T15:46:34.933306Z","shell.execute_reply.started":"2025-05-30T15:46:34.928080Z","shell.execute_reply":"2025-05-30T15:46:34.932422Z"}},"outputs":[{"name":"stdout","text":"[8202, 2399, 123, 3299, 10253]\n2000 年 2 月 add\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"input_data = tokenizer(['2000年2月add'], add_special_tokens=False, truncation=True)\nprint(input_data)\n\ninput_data.word_ids(0) # 返回批次中指定token对应原始文本的索引映射","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:46:37.663936Z","iopub.execute_input":"2025-05-30T15:46:37.664734Z","iopub.status.idle":"2025-05-30T15:46:37.670683Z","shell.execute_reply.started":"2025-05-30T15:46:37.664698Z","shell.execute_reply":"2025-05-30T15:46:37.669974Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[8202, 2399, 123, 3299, 10253]], 'token_type_ids': [[0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[0, 1, 2, 3, 4]"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def data_input_proc(item):\n    # 处理批处理数据（batched=True时item是字典，每个值都是列表）\n    texts = []\n    for tokens in item['tokens']:\n        if len(tokens) == 0:\n            texts.append('')\n            continue\n        # 确保每个样本的token列表被正确转换为字符串\n        if isinstance(tokens[0], list):\n            tokens = [t for sublist in tokens for t in sublist]\n        texts.append(''.join(tokens))\n    input_data = tokenizer(texts, truncation=True, add_special_tokens=False, max_length=512)\n    \n    # 获取word_ids映射（批处理）\n    # word_ids_batch = [input_data.word_ids(batch_index=i) for i in range(len(texts))]\n    \n    # 调整标签序列\n    adjust_labels = []\n    # 上一步骤生成ner_tag中索引和token对齐\n    for k in range(len(input_data['input_ids'])):\n        # 每条记录token对应word_ids\n        word_ids = input_data.word_ids(k)\n        # 批次ner_tag长度和token长度对齐\n        tags = item['ner_tags'][k]\n        \n        adjusted_label_ids = []\n        i, prev_wid = -1,-1\n        for wid in word_ids:\n            if (wid != prev_wid):   #  word_ids [1,1,1,2,3,4,5] -> [0,1,2,3,4,5,6]\n                i += 1 # token对应检索位置+1\n                prev_wid = wid\n            adjusted_label_ids.append(tags[i])\n        adjust_labels.append(adjusted_label_ids)                \n    # 修正后label添加到input_data\n    input_data['labels'] = adjust_labels\n    return input_data\n\n# 应用处理函数\nds2 = ds.map(data_input_proc, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:47:23.257884Z","iopub.execute_input":"2025-05-30T15:47:23.258349Z","iopub.status.idle":"2025-05-30T15:47:32.279489Z","shell.execute_reply.started":"2025-05-30T15:47:23.258324Z","shell.execute_reply":"2025-05-30T15:47:32.278723Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3185e4fdef894d4ca64f327dd24c5438"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3604a14f1a04f4aa1abcf9f64fa12a1"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"ds2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:47:39.408650Z","iopub.execute_input":"2025-05-30T15:47:39.408923Z","iopub.status.idle":"2025-05-30T15:47:39.414004Z","shell.execute_reply.started":"2025-05-30T15:47:39.408903Z","shell.execute_reply":"2025-05-30T15:47:39.413299Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# 训练集\nfor row in ds2['train']:\n    print(row)\n    \n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:47:42.709879Z","iopub.execute_input":"2025-05-30T15:47:42.710150Z","iopub.status.idle":"2025-05-30T15:47:42.715388Z","shell.execute_reply.started":"2025-05-30T15:47:42.710129Z","shell.execute_reply":"2025-05-30T15:47:42.714673Z"}},"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'knowledge': '', 'input_ids': [2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 训练集\nfor row in ds2['train']:\n    print(row['id'])\n    print(row['tokens'])\n    print(row['ner_tags'])\n    print(row['input_ids'])\n    # print(row['token_type_ids'])\n    print(row['attention_mask'])\n    print(row['labels'])\n    print()\n    print(len(row['tokens']))\n    print(len(row['ner_tags']))\n    print(len(row['input_ids']))\n    # print(row['token_type_ids'])\n    print(len(row['attention_mask']))\n    print(len(row['labels']))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:47:47.068288Z","iopub.execute_input":"2025-05-30T15:47:47.068688Z","iopub.status.idle":"2025-05-30T15:47:47.075346Z","shell.execute_reply.started":"2025-05-30T15:47:47.068666Z","shell.execute_reply":"2025-05-30T15:47:47.074644Z"}},"outputs":[{"name":"stdout","text":"0\n['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n50\n50\n50\n50\n50\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 记录转换为pytorch\nds2.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n# ds_new = ds2.with_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:47:50.856335Z","iopub.execute_input":"2025-05-30T15:47:50.856898Z","iopub.status.idle":"2025-05-30T15:47:50.861442Z","shell.execute_reply.started":"2025-05-30T15:47:50.856872Z","shell.execute_reply":"2025-05-30T15:47:50.860714Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"for item in ds2['train']:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:47:54.502646Z","iopub.execute_input":"2025-05-30T15:47:54.502915Z","iopub.status.idle":"2025-05-30T15:47:54.509646Z","shell.execute_reply.started":"2025-05-30T15:47:54.502894Z","shell.execute_reply":"2025-05-30T15:47:54.509001Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n        1159, 8013]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 模型训练\n### TrainingArguments\n","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n    num_train_epochs = 3,    # 训练 epoch\n    save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n    per_device_train_batch_size=32,  # 训练批次\n    per_device_eval_batch_size=32,\n    report_to='tensorboard',  # 训练输出记录\n    eval_strategy=\"epoch\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:49:22.290972Z","iopub.execute_input":"2025-05-30T15:49:22.291220Z","iopub.status.idle":"2025-05-30T15:49:22.323689Z","shell.execute_reply.started":"2025-05-30T15:49:22.291202Z","shell.execute_reply":"2025-05-30T15:49:22.322969Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"id2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                        num_labels=21,\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:48:04.956811Z","iopub.execute_input":"2025-05-30T15:48:04.957489Z","iopub.status.idle":"2025-05-30T15:48:05.091613Z","shell.execute_reply.started":"2025-05-30T15:48:04.957463Z","shell.execute_reply":"2025-05-30T15:48:05.090770Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=21, bias=True)\n)"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Trainer","metadata":{}},{"cell_type":"code","source":"# metric 方法\ndef compute_metric(result):\n    # result 是一个tuple (predicts, labels)\n    \n    # 获取评估对象\n    seqeval = evaluate.load('seqeval')\n    predicts,labels = result\n    predicts = np.argmax(predicts, axis=2)\n    \n    # 准备评估数据\n    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                 for ps,ls in zip(predicts,labels)]\n    results = seqeval.compute(predictions=predicts, references=labels)\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:48:11.065413Z","iopub.execute_input":"2025-05-30T15:48:11.066114Z","iopub.status.idle":"2025-05-30T15:48:11.070787Z","shell.execute_reply.started":"2025-05-30T15:48:11.066091Z","shell.execute_reply":"2025-05-30T15:48:11.070160Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=ds2['train'],\n    eval_dataset=ds2['test'],\n    data_collator=data_collator,\n    compute_metrics=compute_metric\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:49:32.016241Z","iopub.execute_input":"2025-05-30T15:49:32.016568Z","iopub.status.idle":"2025-05-30T15:49:32.030544Z","shell.execute_reply.started":"2025-05-30T15:49:32.016541Z","shell.execute_reply":"2025-05-30T15:49:32.029856Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"模型训练","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:49:37.153659Z","iopub.execute_input":"2025-05-30T15:49:37.154005Z","iopub.status.idle":"2025-05-30T16:27:57.156968Z","shell.execute_reply.started":"2025-05-30T15:49:37.153977Z","shell.execute_reply":"2025-05-30T16:27:57.156311Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2112' max='2112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2112/2112 38:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Address</th>\n      <th>Game</th>\n      <th>Position</th>\n      <th>Overall Precision</th>\n      <th>Overall Recall</th>\n      <th>Overall F1</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.086700</td>\n      <td>0.044454</td>\n      <td>{'precision': 0.8665943600867679, 'recall': 0.8419388830347735, 'f1': 0.854088722608231, 'number': 2847}</td>\n      <td>{'precision': 0.7631384159881569, 'recall': 0.7816527672479151, 'f1': 0.7722846441947565, 'number': 1319}</td>\n      <td>{'precision': 0.8803475935828877, 'recall': 0.873921698739217, 'f1': 0.8771228771228772, 'number': 1507}</td>\n      <td>0.845359</td>\n      <td>0.836418</td>\n      <td>0.840865</td>\n      <td>0.985045</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.043700</td>\n      <td>0.044853</td>\n      <td>{'precision': 0.8779685787358421, 'recall': 0.8440463645943098, 'f1': 0.86067335243553, 'number': 2847}</td>\n      <td>{'precision': 0.821319018404908, 'recall': 0.8119787717968158, 'f1': 0.8166221883339687, 'number': 1319}</td>\n      <td>{'precision': 0.8977572559366754, 'recall': 0.9031187790311878, 'f1': 0.9004300363876943, 'number': 1507}</td>\n      <td>0.870074</td>\n      <td>0.852283</td>\n      <td>0.861086</td>\n      <td>0.986635</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.021800</td>\n      <td>0.044692</td>\n      <td>{'precision': 0.8959308606409795, 'recall': 0.8739023533544081, 'f1': 0.8847795163584636, 'number': 2847}</td>\n      <td>{'precision': 0.8123167155425219, 'recall': 0.8400303260045489, 'f1': 0.8259411106969808, 'number': 1319}</td>\n      <td>{'precision': 0.9081967213114754, 'recall': 0.9190444591904446, 'f1': 0.9135883905013195, 'number': 1507}</td>\n      <td>0.879103</td>\n      <td>0.878019</td>\n      <td>0.878561</td>\n      <td>0.987760</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40acbcd3875849ca90b02587db4776a4"}},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'precision': 0.8665943600867679, 'recall': 0.8419388830347735, 'f1': 0.854088722608231, 'number': 2847}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7631384159881569, 'recall': 0.7816527672479151, 'f1': 0.7722846441947565, 'number': 1319}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8803475935828877, 'recall': 0.873921698739217, 'f1': 0.8771228771228772, 'number': 1507}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.8779685787358421, 'recall': 0.8440463645943098, 'f1': 0.86067335243553, 'number': 2847}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.821319018404908, 'recall': 0.8119787717968158, 'f1': 0.8166221883339687, 'number': 1319}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8977572559366754, 'recall': 0.9031187790311878, 'f1': 0.9004300363876943, 'number': 1507}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.8959308606409795, 'recall': 0.8739023533544081, 'f1': 0.8847795163584636, 'number': 2847}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8123167155425219, 'recall': 0.8400303260045489, 'f1': 0.8259411106969808, 'number': 1319}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.9081967213114754, 'recall': 0.9190444591904446, 'f1': 0.9135883905013195, 'number': 1507}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2112, training_loss=0.0441932672578277, metrics={'train_runtime': 2298.9743, 'train_samples_per_second': 58.723, 'train_steps_per_second': 0.919, 'total_flos': 1.1735178399679266e+16, 'train_loss': 0.0441932672578277, 'epoch': 3.0})"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"模型推理","metadata":{}},{"cell_type":"code","source":"result = trainer.predict(ds2['test'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:28:05.108895Z","iopub.execute_input":"2025-05-30T16:28:05.109192Z","iopub.status.idle":"2025-05-30T16:28:30.886962Z","shell.execute_reply.started":"2025-05-30T16:28:05.109172Z","shell.execute_reply":"2025-05-30T16:28:30.886400Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"print(ds['test'][10]['tokens'])\nprint(ds2['test'][10]['labels'])\nprint(result.label_ids[10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:28:40.593022Z","iopub.execute_input":"2025-05-30T16:28:40.593686Z","iopub.status.idle":"2025-05-30T16:28:40.601604Z","shell.execute_reply.started":"2025-05-30T16:28:40.593663Z","shell.execute_reply":"2025-05-30T16:28:40.600937Z"}},"outputs":[{"name":"stdout","text":"['在', '跨', '世', '纪', '的', '征', '途', '上', '，', '在', '中', '国', '共', '产', '党', '领', '导', '下', '，', '我', '们', '要', '努', '力', '实', '现', '包', '括', '各', '民', '主', '党', '派', '、', '各', '人', '民', '团', '体', '、', '无', '党', '派', '人', '士', '在', '内', '的', '全', '体', '中', '国', '人', '民', '的', '大', '团', '结', '，', '实', '现', '包', '括', '大', '陆', '同', '胞', '、', '台', '港', '澳', '同', '胞', '和', '海', '外', '侨', '胞', '在', '内', '的', '所', '有', '爱', '国', '的', '中', '华', '儿', '女', '的', '大', '团', '结', '，', '从', '而', '战', '胜', '各', '种', '艰', '难', '险', '阻', '，', '实', '现', '跨', '世', '纪', '的', '宏', '伟', '蓝', '图', '。']\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 5, 5, 5, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n[   0    0    0    0    0    0    0    0    0    0    3    4    4    4\n    4    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    5    6    0    0    0    0\n    0    0    0    0    0    0    0    5    6    0    0    0    5    5\n    5    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    5    6    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n -100 -100 -100 -100 -100 -100 -100 -100]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":" [tags[p] for p,l in zip(result.label_ids[10],ds2['test'][10]['labels'])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:28:48.888132Z","iopub.execute_input":"2025-05-30T16:28:48.888847Z","iopub.status.idle":"2025-05-30T16:28:48.895924Z","shell.execute_reply.started":"2025-05-30T16:28:48.888821Z","shell.execute_reply":"2025-05-30T16:28:48.895351Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"['O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-GAME',\n 'I-GAME',\n 'I-GAME',\n 'I-GAME',\n 'I-GAME',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-ADDRESS',\n 'I-ADDRESS',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-ADDRESS',\n 'I-ADDRESS',\n 'O',\n 'O',\n 'O',\n 'B-ADDRESS',\n 'B-ADDRESS',\n 'B-ADDRESS',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-ADDRESS',\n 'I-ADDRESS',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O']"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"[tags[l] for p,l in zip(result.label_ids[10],ds2['test'][10]['labels'])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T16:28:55.109700Z","iopub.execute_input":"2025-05-30T16:28:55.110210Z","iopub.status.idle":"2025-05-30T16:28:55.118605Z","shell.execute_reply.started":"2025-05-30T16:28:55.110189Z","shell.execute_reply":"2025-05-30T16:28:55.117822Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"['O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-GAME',\n 'I-GAME',\n 'I-GAME',\n 'I-GAME',\n 'I-GAME',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-ADDRESS',\n 'I-ADDRESS',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-ADDRESS',\n 'I-ADDRESS',\n 'O',\n 'O',\n 'O',\n 'B-ADDRESS',\n 'B-ADDRESS',\n 'B-ADDRESS',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-ADDRESS',\n 'I-ADDRESS',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O']"},"metadata":{}}],"execution_count":34}]}