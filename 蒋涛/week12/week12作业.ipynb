{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install evaluate seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:36:46.575308Z","iopub.execute_input":"2025-06-18T06:36:46.575963Z","iopub.status.idle":"2025-06-18T06:36:49.683583Z","shell.execute_reply.started":"2025-06-18T06:36:46.575939Z","shell.execute_reply":"2025-06-18T06:36:49.682758Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:37:05.936889Z","iopub.execute_input":"2025-06-18T06:37:05.937842Z","iopub.status.idle":"2025-06-18T06:37:05.941970Z","shell.execute_reply.started":"2025-06-18T06:37:05.937811Z","shell.execute_reply":"2025-06-18T06:37:05.941277Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:37:09.716121Z","iopub.execute_input":"2025-06-18T06:37:09.716709Z","iopub.status.idle":"2025-06-18T06:37:12.318311Z","shell.execute_reply.started":"2025-06-18T06:37:09.716685Z","shell.execute_reply":"2025-06-18T06:37:12.317607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4053f322de0d436188eafdc598e707dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9558ebdca3b6452f9a832f6eb10b2faa"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:37:20.871642Z","iopub.execute_input":"2025-06-18T06:37:20.871935Z","iopub.status.idle":"2025-06-18T06:37:20.878126Z","shell.execute_reply.started":"2025-06-18T06:37:20.871915Z","shell.execute_reply":"2025-06-18T06:37:20.877331Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:37:38.446430Z","iopub.execute_input":"2025-06-18T06:37:38.447182Z","iopub.status.idle":"2025-06-18T06:37:38.964932Z","shell.execute_reply.started":"2025-06-18T06:37:38.447159Z","shell.execute_reply":"2025-06-18T06:37:38.964350Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2278fd75e9b47e98b23c6f24746b446"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1fd563e0214e6d91a021224a9cb21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14290a5c7e9b4bbaa7e84adf7eeb2955"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# 模型测试\nmessage= \"命名实体识别\"\nlabel = torch.tensor([0,1,0,2,5,4])\n\nmodel_input = tokenizer([message], return_tensors='pt')\nresult = model(**model_input)\n\nprint(result.loss)\nprint(result.logits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:37:44.503915Z","iopub.execute_input":"2025-06-18T06:37:44.504428Z","iopub.status.idle":"2025-06-18T06:37:44.660486Z","shell.execute_reply.started":"2025-06-18T06:37:44.504406Z","shell.execute_reply":"2025-06-18T06:37:44.659872Z"}},"outputs":[{"name":"stdout","text":"None\ntensor([[[ 0.3328, -0.2662, -0.1428, -0.1978, -0.4868, -1.4934,  0.7539],\n         [ 0.4074, -0.7107,  0.4805, -0.3104, -0.3438, -1.0272,  0.6406],\n         [-0.0743, -0.1020,  0.4508, -0.0212, -0.3985, -0.9073,  0.0806],\n         [-0.3371, -0.3284,  0.4692, -0.1262, -0.3711, -0.8790, -0.3829],\n         [-0.1737, -0.2531,  0.5247, -0.2002, -0.1514, -1.2913,  0.0605],\n         [-0.6175,  0.0764,  0.6844, -0.2436, -0.0042, -1.0078, -0.1176],\n         [-0.0885, -0.2327, -0.2008, -1.1142, -0.4992, -1.4460, -0.0302],\n         [-0.3209,  0.0764,  0.8272, -0.0095, -0.0070, -0.9031,  0.3057]]],\n       grad_fn=<ViewBackward0>)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# 加载hf中dataset\nds = load_dataset('doushabao4766/msra_ner_k_V3')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:37:48.907781Z","iopub.execute_input":"2025-06-18T06:37:48.908349Z","iopub.status.idle":"2025-06-18T06:37:50.560255Z","shell.execute_reply.started":"2025-06-18T06:37:48.908328Z","shell.execute_reply":"2025-06-18T06:37:50.559337Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6df9bea13b1e48a3b19d52a1249453cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-42717a92413393f9.parquet:   0%|          | 0.00/13.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c0b3a207a8f4237a3e64b172ee09b23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-8899cab5fdab45bc.parquet:   0%|          | 0.00/946k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"628fa107e1e64a3bb1d82e6b165907be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fb57adc7deb41128428ac70f06a3193"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31fe968f71fc497e9c8d877383a613ac"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"ds[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:37:54.720998Z","iopub.execute_input":"2025-06-18T06:37:54.721480Z","iopub.status.idle":"2025-06-18T06:37:54.726431Z","shell.execute_reply.started":"2025-06-18T06:37:54.721459Z","shell.execute_reply":"2025-06-18T06:37:54.725851Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n    num_rows: 45001\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## 实体映射数据集词典准备","metadata":{}},{"cell_type":"code","source":"# entity_index\nentites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n           'company', 'scene', 'book', 'organization', 'government'})\ntags = ['O']\nfor entity in entites[1:]:\n    tags.append('B-' + entity.upper())\n    tags.append('I-' + entity.upper())\n\nentity_index = {entity:i for i, entity in enumerate(entites)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:37:57.595927Z","iopub.execute_input":"2025-06-18T06:37:57.596209Z","iopub.status.idle":"2025-06-18T06:37:57.601007Z","shell.execute_reply.started":"2025-06-18T06:37:57.596190Z","shell.execute_reply":"2025-06-18T06:37:57.600382Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"entity_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:38:01.725762Z","iopub.execute_input":"2025-06-18T06:38:01.726320Z","iopub.status.idle":"2025-06-18T06:38:01.730897Z","shell.execute_reply.started":"2025-06-18T06:38:01.726300Z","shell.execute_reply":"2025-06-18T06:38:01.730158Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'O': 0,\n 'organization': 1,\n 'company': 2,\n 'government': 3,\n 'book': 4,\n 'position': 5,\n 'movie': 6,\n 'game': 7,\n 'name': 8,\n 'scene': 9,\n 'address': 10}"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"tags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:39:03.905326Z","iopub.execute_input":"2025-06-18T06:39:03.905657Z","iopub.status.idle":"2025-06-18T06:39:03.910409Z","shell.execute_reply.started":"2025-06-18T06:39:03.905637Z","shell.execute_reply":"2025-06-18T06:39:03.909711Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['O',\n 'B-ORGANIZATION',\n 'I-ORGANIZATION',\n 'B-COMPANY',\n 'I-COMPANY',\n 'B-GOVERNMENT',\n 'I-GOVERNMENT',\n 'B-BOOK',\n 'I-BOOK',\n 'B-POSITION',\n 'I-POSITION',\n 'B-MOVIE',\n 'I-MOVIE',\n 'B-GAME',\n 'I-GAME',\n 'B-NAME',\n 'I-NAME',\n 'B-SCENE',\n 'I-SCENE',\n 'B-ADDRESS',\n 'I-ADDRESS']"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# 训练集\nfor row in ds['train']:\n    print(row)\n    print(row['tokens'])\n    # print(row['tokens'])\n    # print(row['tokens'])\n    # print(row['ner_tags'])\n    # print(row['ent_tag'])\n    \n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:07:07.366399Z","iopub.execute_input":"2025-05-30T14:07:07.367374Z","iopub.status.idle":"2025-05-30T14:07:07.374087Z","shell.execute_reply.started":"2025-05-30T14:07:07.367340Z","shell.execute_reply":"2025-05-30T14:07:07.373152Z"}},"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'knowledge': ''}\n['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"ds1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:28:08.209163Z","iopub.execute_input":"2025-05-30T10:28:08.210278Z","iopub.status.idle":"2025-05-30T10:28:08.216190Z","shell.execute_reply.started":"2025-05-30T10:28:08.210242Z","shell.execute_reply":"2025-05-30T10:28:08.215149Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'ent_tag'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'ent_tag'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"中文bert分词在日期时间和英文转换token过程中，出现合并。影响ner标注准确性。","metadata":{}},{"cell_type":"code","source":"token_index = tokenizer.encode('2000年2月add', add_special_tokens=False)\nprint(token_index)\ntokens = tokenizer.decode(token_index)\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:39:27.417114Z","iopub.execute_input":"2025-06-18T06:39:27.417648Z","iopub.status.idle":"2025-06-18T06:39:27.422889Z","shell.execute_reply.started":"2025-06-18T06:39:27.417622Z","shell.execute_reply":"2025-06-18T06:39:27.422126Z"}},"outputs":[{"name":"stdout","text":"[8202, 2399, 123, 3299, 10253]\n2000 年 2 月 add\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"input_data = tokenizer(['2000年2月add'], add_special_tokens=False, truncation=True)\nprint(input_data)\n\ninput_data.word_ids(0) # 返回批次中指定token对应原始文本的索引映射","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:39:31.887067Z","iopub.execute_input":"2025-06-18T06:39:31.887580Z","iopub.status.idle":"2025-06-18T06:39:31.893526Z","shell.execute_reply.started":"2025-06-18T06:39:31.887530Z","shell.execute_reply":"2025-06-18T06:39:31.892852Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[8202, 2399, 123, 3299, 10253]], 'token_type_ids': [[0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[0, 1, 2, 3, 4]"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def data_input_proc(item):\n    # 处理批处理数据（batched=True时item是字典，每个值都是列表）\n    texts = []\n    for tokens in item['tokens']:\n        if len(tokens) == 0:\n            texts.append('')\n            continue\n        # 确保每个样本的token列表被正确转换为字符串\n        if isinstance(tokens[0], list):\n            tokens = [t for sublist in tokens for t in sublist]\n        texts.append(''.join(tokens))\n    input_data = tokenizer(texts, truncation=True, add_special_tokens=False, max_length=512)\n    \n    # 获取word_ids映射（批处理）\n    # word_ids_batch = [input_data.word_ids(batch_index=i) for i in range(len(texts))]\n    \n    # 调整标签序列\n    adjust_labels = []\n    # 上一步骤生成ner_tag中索引和token对齐\n    for k in range(len(input_data['input_ids'])):\n        # 每条记录token对应word_ids\n        word_ids = input_data.word_ids(k)\n        # 批次ner_tag长度和token长度对齐\n        tags = item['ner_tags'][k]\n        \n        adjusted_label_ids = []\n        i, prev_wid = -1,-1\n        for wid in word_ids:\n            if (wid != prev_wid):   #  word_ids [1,1,1,2,3,4,5] -> [0,1,2,3,4,5,6]\n                i += 1 # token对应检索位置+1\n                prev_wid = wid\n            adjusted_label_ids.append(tags[i])\n        adjust_labels.append(adjusted_label_ids)                \n    # 修正后label添加到input_data\n    input_data['labels'] = adjust_labels\n    return input_data\n\n# 应用处理函数\nds2 = ds.map(data_input_proc, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:39:37.313180Z","iopub.execute_input":"2025-06-18T06:39:37.313802Z","iopub.status.idle":"2025-06-18T06:39:46.470753Z","shell.execute_reply.started":"2025-06-18T06:39:37.313773Z","shell.execute_reply":"2025-06-18T06:39:46.469995Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17d0b68a1764b5aa58a2b1c37c39dfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aad16ffd89a44ee856246e4aa21d1cc"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"ds2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:39:51.243789Z","iopub.execute_input":"2025-06-18T06:39:51.244613Z","iopub.status.idle":"2025-06-18T06:39:51.248871Z","shell.execute_reply.started":"2025-06-18T06:39:51.244581Z","shell.execute_reply":"2025-06-18T06:39:51.248278Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# 训练集\nfor row in ds2['train']:\n    print(row)\n    \n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:39:55.310951Z","iopub.execute_input":"2025-06-18T06:39:55.311214Z","iopub.status.idle":"2025-06-18T06:39:55.316441Z","shell.execute_reply.started":"2025-06-18T06:39:55.311194Z","shell.execute_reply":"2025-06-18T06:39:55.315805Z"}},"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'knowledge': '', 'input_ids': [2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# 训练集\nfor row in ds2['train']:\n    print(row['id'])\n    print(row['tokens'])\n    print(row['ner_tags'])\n    print(row['input_ids'])\n    # print(row['token_type_ids'])\n    print(row['attention_mask'])\n    print(row['labels'])\n    print()\n    print(len(row['tokens']))\n    print(len(row['ner_tags']))\n    print(len(row['input_ids']))\n    # print(row['token_type_ids'])\n    print(len(row['attention_mask']))\n    print(len(row['labels']))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:39:57.991240Z","iopub.execute_input":"2025-06-18T06:39:57.991953Z","iopub.status.idle":"2025-06-18T06:39:57.997407Z","shell.execute_reply.started":"2025-06-18T06:39:57.991928Z","shell.execute_reply":"2025-06-18T06:39:57.996741Z"}},"outputs":[{"name":"stdout","text":"0\n['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n50\n50\n50\n50\n50\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# 记录转换为pytorch\nds2.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n# ds_new = ds2.with_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:40:02.880251Z","iopub.execute_input":"2025-06-18T06:40:02.880603Z","iopub.status.idle":"2025-06-18T06:40:02.885669Z","shell.execute_reply.started":"2025-06-18T06:40:02.880578Z","shell.execute_reply":"2025-06-18T06:40:02.884876Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"for item in ds2['train']:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:40:06.106041Z","iopub.execute_input":"2025-06-18T06:40:06.106501Z","iopub.status.idle":"2025-06-18T06:40:06.112690Z","shell.execute_reply.started":"2025-06-18T06:40:06.106476Z","shell.execute_reply":"2025-06-18T06:40:06.112082Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n        1159, 8013]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## 模型手动训练\n","metadata":{}},{"cell_type":"code","source":"# 废弃\nargs = TrainingArguments(\n    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n    num_train_epochs = 3,    # 训练 epoch\n    save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n    per_device_train_batch_size=32,  # 训练批次\n    per_device_eval_batch_size=32,\n    report_to='tensorboard',  # 训练输出记录\n    eval_strategy=\"epoch\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T15:49:22.290972Z","iopub.execute_input":"2025-05-30T15:49:22.291220Z","iopub.status.idle":"2025-05-30T15:49:22.323689Z","shell.execute_reply.started":"2025-05-30T15:49:22.291202Z","shell.execute_reply":"2025-05-30T15:49:22.322969Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"[name for name, params in model.named_parameters()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:40:10.540428Z","iopub.execute_input":"2025-06-18T06:40:10.540998Z","iopub.status.idle":"2025-06-18T06:40:10.547279Z","shell.execute_reply.started":"2025-06-18T06:40:10.540973Z","shell.execute_reply":"2025-06-18T06:40:10.546571Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['bert.embeddings.word_embeddings.weight',\n 'bert.embeddings.position_embeddings.weight',\n 'bert.embeddings.token_type_embeddings.weight',\n 'bert.embeddings.LayerNorm.weight',\n 'bert.embeddings.LayerNorm.bias',\n 'bert.encoder.layer.0.attention.self.query.weight',\n 'bert.encoder.layer.0.attention.self.query.bias',\n 'bert.encoder.layer.0.attention.self.key.weight',\n 'bert.encoder.layer.0.attention.self.key.bias',\n 'bert.encoder.layer.0.attention.self.value.weight',\n 'bert.encoder.layer.0.attention.self.value.bias',\n 'bert.encoder.layer.0.attention.output.dense.weight',\n 'bert.encoder.layer.0.attention.output.dense.bias',\n 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.0.intermediate.dense.weight',\n 'bert.encoder.layer.0.intermediate.dense.bias',\n 'bert.encoder.layer.0.output.dense.weight',\n 'bert.encoder.layer.0.output.dense.bias',\n 'bert.encoder.layer.0.output.LayerNorm.weight',\n 'bert.encoder.layer.0.output.LayerNorm.bias',\n 'bert.encoder.layer.1.attention.self.query.weight',\n 'bert.encoder.layer.1.attention.self.query.bias',\n 'bert.encoder.layer.1.attention.self.key.weight',\n 'bert.encoder.layer.1.attention.self.key.bias',\n 'bert.encoder.layer.1.attention.self.value.weight',\n 'bert.encoder.layer.1.attention.self.value.bias',\n 'bert.encoder.layer.1.attention.output.dense.weight',\n 'bert.encoder.layer.1.attention.output.dense.bias',\n 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.1.intermediate.dense.weight',\n 'bert.encoder.layer.1.intermediate.dense.bias',\n 'bert.encoder.layer.1.output.dense.weight',\n 'bert.encoder.layer.1.output.dense.bias',\n 'bert.encoder.layer.1.output.LayerNorm.weight',\n 'bert.encoder.layer.1.output.LayerNorm.bias',\n 'bert.encoder.layer.2.attention.self.query.weight',\n 'bert.encoder.layer.2.attention.self.query.bias',\n 'bert.encoder.layer.2.attention.self.key.weight',\n 'bert.encoder.layer.2.attention.self.key.bias',\n 'bert.encoder.layer.2.attention.self.value.weight',\n 'bert.encoder.layer.2.attention.self.value.bias',\n 'bert.encoder.layer.2.attention.output.dense.weight',\n 'bert.encoder.layer.2.attention.output.dense.bias',\n 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.2.intermediate.dense.weight',\n 'bert.encoder.layer.2.intermediate.dense.bias',\n 'bert.encoder.layer.2.output.dense.weight',\n 'bert.encoder.layer.2.output.dense.bias',\n 'bert.encoder.layer.2.output.LayerNorm.weight',\n 'bert.encoder.layer.2.output.LayerNorm.bias',\n 'bert.encoder.layer.3.attention.self.query.weight',\n 'bert.encoder.layer.3.attention.self.query.bias',\n 'bert.encoder.layer.3.attention.self.key.weight',\n 'bert.encoder.layer.3.attention.self.key.bias',\n 'bert.encoder.layer.3.attention.self.value.weight',\n 'bert.encoder.layer.3.attention.self.value.bias',\n 'bert.encoder.layer.3.attention.output.dense.weight',\n 'bert.encoder.layer.3.attention.output.dense.bias',\n 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.3.intermediate.dense.weight',\n 'bert.encoder.layer.3.intermediate.dense.bias',\n 'bert.encoder.layer.3.output.dense.weight',\n 'bert.encoder.layer.3.output.dense.bias',\n 'bert.encoder.layer.3.output.LayerNorm.weight',\n 'bert.encoder.layer.3.output.LayerNorm.bias',\n 'bert.encoder.layer.4.attention.self.query.weight',\n 'bert.encoder.layer.4.attention.self.query.bias',\n 'bert.encoder.layer.4.attention.self.key.weight',\n 'bert.encoder.layer.4.attention.self.key.bias',\n 'bert.encoder.layer.4.attention.self.value.weight',\n 'bert.encoder.layer.4.attention.self.value.bias',\n 'bert.encoder.layer.4.attention.output.dense.weight',\n 'bert.encoder.layer.4.attention.output.dense.bias',\n 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.4.intermediate.dense.weight',\n 'bert.encoder.layer.4.intermediate.dense.bias',\n 'bert.encoder.layer.4.output.dense.weight',\n 'bert.encoder.layer.4.output.dense.bias',\n 'bert.encoder.layer.4.output.LayerNorm.weight',\n 'bert.encoder.layer.4.output.LayerNorm.bias',\n 'bert.encoder.layer.5.attention.self.query.weight',\n 'bert.encoder.layer.5.attention.self.query.bias',\n 'bert.encoder.layer.5.attention.self.key.weight',\n 'bert.encoder.layer.5.attention.self.key.bias',\n 'bert.encoder.layer.5.attention.self.value.weight',\n 'bert.encoder.layer.5.attention.self.value.bias',\n 'bert.encoder.layer.5.attention.output.dense.weight',\n 'bert.encoder.layer.5.attention.output.dense.bias',\n 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.5.intermediate.dense.weight',\n 'bert.encoder.layer.5.intermediate.dense.bias',\n 'bert.encoder.layer.5.output.dense.weight',\n 'bert.encoder.layer.5.output.dense.bias',\n 'bert.encoder.layer.5.output.LayerNorm.weight',\n 'bert.encoder.layer.5.output.LayerNorm.bias',\n 'bert.encoder.layer.6.attention.self.query.weight',\n 'bert.encoder.layer.6.attention.self.query.bias',\n 'bert.encoder.layer.6.attention.self.key.weight',\n 'bert.encoder.layer.6.attention.self.key.bias',\n 'bert.encoder.layer.6.attention.self.value.weight',\n 'bert.encoder.layer.6.attention.self.value.bias',\n 'bert.encoder.layer.6.attention.output.dense.weight',\n 'bert.encoder.layer.6.attention.output.dense.bias',\n 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.6.intermediate.dense.weight',\n 'bert.encoder.layer.6.intermediate.dense.bias',\n 'bert.encoder.layer.6.output.dense.weight',\n 'bert.encoder.layer.6.output.dense.bias',\n 'bert.encoder.layer.6.output.LayerNorm.weight',\n 'bert.encoder.layer.6.output.LayerNorm.bias',\n 'bert.encoder.layer.7.attention.self.query.weight',\n 'bert.encoder.layer.7.attention.self.query.bias',\n 'bert.encoder.layer.7.attention.self.key.weight',\n 'bert.encoder.layer.7.attention.self.key.bias',\n 'bert.encoder.layer.7.attention.self.value.weight',\n 'bert.encoder.layer.7.attention.self.value.bias',\n 'bert.encoder.layer.7.attention.output.dense.weight',\n 'bert.encoder.layer.7.attention.output.dense.bias',\n 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.7.intermediate.dense.weight',\n 'bert.encoder.layer.7.intermediate.dense.bias',\n 'bert.encoder.layer.7.output.dense.weight',\n 'bert.encoder.layer.7.output.dense.bias',\n 'bert.encoder.layer.7.output.LayerNorm.weight',\n 'bert.encoder.layer.7.output.LayerNorm.bias',\n 'bert.encoder.layer.8.attention.self.query.weight',\n 'bert.encoder.layer.8.attention.self.query.bias',\n 'bert.encoder.layer.8.attention.self.key.weight',\n 'bert.encoder.layer.8.attention.self.key.bias',\n 'bert.encoder.layer.8.attention.self.value.weight',\n 'bert.encoder.layer.8.attention.self.value.bias',\n 'bert.encoder.layer.8.attention.output.dense.weight',\n 'bert.encoder.layer.8.attention.output.dense.bias',\n 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.8.intermediate.dense.weight',\n 'bert.encoder.layer.8.intermediate.dense.bias',\n 'bert.encoder.layer.8.output.dense.weight',\n 'bert.encoder.layer.8.output.dense.bias',\n 'bert.encoder.layer.8.output.LayerNorm.weight',\n 'bert.encoder.layer.8.output.LayerNorm.bias',\n 'bert.encoder.layer.9.attention.self.query.weight',\n 'bert.encoder.layer.9.attention.self.query.bias',\n 'bert.encoder.layer.9.attention.self.key.weight',\n 'bert.encoder.layer.9.attention.self.key.bias',\n 'bert.encoder.layer.9.attention.self.value.weight',\n 'bert.encoder.layer.9.attention.self.value.bias',\n 'bert.encoder.layer.9.attention.output.dense.weight',\n 'bert.encoder.layer.9.attention.output.dense.bias',\n 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.9.intermediate.dense.weight',\n 'bert.encoder.layer.9.intermediate.dense.bias',\n 'bert.encoder.layer.9.output.dense.weight',\n 'bert.encoder.layer.9.output.dense.bias',\n 'bert.encoder.layer.9.output.LayerNorm.weight',\n 'bert.encoder.layer.9.output.LayerNorm.bias',\n 'bert.encoder.layer.10.attention.self.query.weight',\n 'bert.encoder.layer.10.attention.self.query.bias',\n 'bert.encoder.layer.10.attention.self.key.weight',\n 'bert.encoder.layer.10.attention.self.key.bias',\n 'bert.encoder.layer.10.attention.self.value.weight',\n 'bert.encoder.layer.10.attention.self.value.bias',\n 'bert.encoder.layer.10.attention.output.dense.weight',\n 'bert.encoder.layer.10.attention.output.dense.bias',\n 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.10.intermediate.dense.weight',\n 'bert.encoder.layer.10.intermediate.dense.bias',\n 'bert.encoder.layer.10.output.dense.weight',\n 'bert.encoder.layer.10.output.dense.bias',\n 'bert.encoder.layer.10.output.LayerNorm.weight',\n 'bert.encoder.layer.10.output.LayerNorm.bias',\n 'bert.encoder.layer.11.attention.self.query.weight',\n 'bert.encoder.layer.11.attention.self.query.bias',\n 'bert.encoder.layer.11.attention.self.key.weight',\n 'bert.encoder.layer.11.attention.self.key.bias',\n 'bert.encoder.layer.11.attention.self.value.weight',\n 'bert.encoder.layer.11.attention.self.value.bias',\n 'bert.encoder.layer.11.attention.output.dense.weight',\n 'bert.encoder.layer.11.attention.output.dense.bias',\n 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n 'bert.encoder.layer.11.intermediate.dense.weight',\n 'bert.encoder.layer.11.intermediate.dense.bias',\n 'bert.encoder.layer.11.output.dense.weight',\n 'bert.encoder.layer.11.output.dense.bias',\n 'bert.encoder.layer.11.output.LayerNorm.weight',\n 'bert.encoder.layer.11.output.LayerNorm.bias',\n 'classifier.weight',\n 'classifier.bias']"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"# dataLoader\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\n\ntrain_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n\n\n# 模型创建\nid2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                        num_labels=21,\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel.to('cuda')\n\n# 模型参数分组\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=100, \n                                            num_training_steps=train_steps)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:40:33.793359Z","iopub.execute_input":"2025-06-18T06:40:33.793734Z","iopub.status.idle":"2025-06-18T06:40:34.231674Z","shell.execute_reply.started":"2025-06-18T06:40:33.793711Z","shell.execute_reply":"2025-06-18T06:40:34.230846Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## 支持混合精度训练","metadata":{}},{"cell_type":"code","source":"# dataLoader\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.optim as optim\n\ntrain_dl = DataLoader(ds2['train'], shuffle=True, batch_size=16)\n\n\n# 模型创建\nid2lbl = {i:tag for i, tag in enumerate(tags)}\nlbl2id = {tag:i for i, tag in enumerate(tags)}\n\nmodel = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                        num_labels=21,\n                                                        id2label=id2lbl,\n                                                        label2id=lbl2id)\nmodel.to('cuda')\n\n# 模型参数分组\nparam_optimizer = list(model.named_parameters())\nbert_params, classifier_params = [],[]\n\nfor name,params in param_optimizer:\n    if 'bert' in name:\n        bert_params.append(params)\n    else:\n        classifier_params.append(params)\n\nparam_groups = [\n    {'params':bert_params, 'lr':1e-5},\n    {'params':classifier_params, 'weight_decay':0.1, 'lr':1e-3}\n]\n\n# optimizer\noptimizer = optim.AdamW(param_groups) # 优化器\n\n# 学习率调度器\ntrain_steps = len(train_dl) * 5\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=100, \n                                            num_training_steps=train_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:41:56.772574Z","iopub.execute_input":"2025-06-18T06:41:56.772875Z","iopub.status.idle":"2025-06-18T06:41:57.072376Z","shell.execute_reply.started":"2025-06-18T06:41:56.772856Z","shell.execute_reply":"2025-06-18T06:41:57.071475Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 支持分布式训练","metadata":{}},{"cell_type":"code","source":"%%writefile ddp_simple.py\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\n\n# 设置分布式环境\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# 清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n\n# 定义训练循环\ndef train(rank, world_size):\n    setup(rank, world_size)\n    \n    # 定义模型并将其移动到对应的 GPU 设备端\n    model = models.resnet50().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n    \n    # 损失函数及优化器\n    criterion = nn.CrossEntropyLoss().to(rank)\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)\n    \n    # 定义数据集Dataset的转换和图像增强\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    # 分布式训练采样器\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n    \n    # 在训练开始时创建一次\n    scaler = torch.cuda.amp.GradScaler()\n    \n    for epoch in range(10):\n        ddp_model.train()\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(rank), labels.to(rank)\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():\n                outputs = ddp_model(inputs)\n                loss = criterion(outputs, labels)\n            \n            scaler.scale(loss).backward()\n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(ddp_model.parameters(), 1)\n            \n            scaler.step(optimizer)\n            scaler.update()\n            \n#             loss.backward()\n#             optimizer.step()\n            print(f\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\")\n\n    cleanup()\n    \ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:42:37.936276Z","iopub.execute_input":"2025-06-18T06:42:37.936574Z","iopub.status.idle":"2025-06-18T06:42:37.942741Z","shell.execute_reply.started":"2025-06-18T06:42:37.936533Z","shell.execute_reply":"2025-06-18T06:42:37.941986Z"}},"outputs":[{"name":"stdout","text":"Writing ddp_simple.py\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!python ddp_simple.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T06:43:03.192330Z","iopub.execute_input":"2025-06-18T06:43:03.192630Z","iopub.status.idle":"2025-06-18T06:43:18.764476Z","shell.execute_reply.started":"2025-06-18T06:43:03.192610Z","shell.execute_reply":"2025-06-18T06:43:18.763773Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"100%|████████████████████████████████████████| 170M/170M [00:02<00:00, 66.7MB/s]\n100%|████████████████████████████████████████| 170M/170M [00:03<00:00, 46.9MB/s]\n/kaggle/working/ddp_simple.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n/kaggle/working/ddp_simple.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nW0618 06:43:17.944000 146 torch/multiprocessing/spawn.py:169] Terminating process 149 via signal SIGTERM\nTraceback (most recent call last):\n  File \"/kaggle/working/ddp_simple.py\", line 79, in <module>\n    main()\n  File \"/kaggle/working/ddp_simple.py\", line 76, in main\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 340, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 296, in start_processes\n    while not context.join():\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 215, in join\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\ntorch.multiprocessing.spawn.ProcessRaisedException: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/kaggle/working/ddp_simple.py\", line 44, in train\n    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/cifar.py\", line 83, in __init__\n    entry = pickle.load(f, encoding=\"latin1\")\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_pickle.UnpicklingError: pickle data was truncated\n\n","output_type":"stream"}],"execution_count":35}]}