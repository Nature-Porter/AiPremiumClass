{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install evaluate seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:52:59.043320Z","iopub.execute_input":"2025-06-13T08:52:59.043996Z","iopub.status.idle":"2025-06-13T08:53:06.641756Z","shell.execute_reply.started":"2025-06-13T08:52:59.043971Z","shell.execute_reply":"2025-06-13T08:53:06.641017Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile ner_ddp.py\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nimport evaluate  # pip install evaluate\nimport seqeval   # pip install seqeval\nfrom datasets import load_dataset\nimport os\nimport numpy as np\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n# 设置分布式环境\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# 清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n    \n\ndef train(rank, world_size):\n    setup(rank, world_size)\n    # 数据集\n    ds = load_dataset('nlhappy/CLUE-NER')\n    # entity_index\n    entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n               'company', 'scene', 'book', 'organization', 'government'})\n    tags = ['O']\n    for entity in entites[1:]:\n        tags.append('B-' + entity.upper())\n        tags.append('I-' + entity.upper())\n    \n    entity_index = {entity:i for i, entity in enumerate(entites)}\n\n    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n    \n    def entity_tags_proc(item):\n        # item即是dataset中记录\n        text_len = len(item['text'])  # 根据文本长度生成tags列表\n        tags = [0] * text_len    # 初始值为‘O’\n        # 遍历实体列表，所有实体类别标记填入tags\n        entites = item['ents']\n        for ent in entites:\n            indices = ent['indices']  # 实体索引\n            label = ent['label']   # 实体名\n            tags[indices[0]] = entity_index[label] * 2 - 1\n            for idx in indices[1:]:\n                tags[idx] = entity_index[label] * 2\n        return {'ent_tag': tags}\n    \n    # 使用自定义回调函数处理数据集记录\n    ds1 = ds.map(entity_tags_proc)\n    \n    def data_input_proc(item):\n        # 输入文本先拆分为字符，再转换为模型输入的token索引\n        batch_texts = [list(text) for text in item['text']]\n        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n        input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n                               is_split_into_words=True, padding='max_length')\n        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n        return input_data\n        \n    \n    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n    \n    \n    local_rank = rank\n    \n    id2lbl = {i:tag for i, tag in enumerate(tags)}\n    lbl2id = {tag:i for i, tag in enumerate(tags)}\n    \n    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n                                                            num_labels=21,\n                                                            id2label=id2lbl,\n                                                            label2id=lbl2id)\n    model.to(local_rank)\n    \n    args = TrainingArguments(\n        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n        num_train_epochs = 1,    # 训练 epoch\n        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n        per_device_train_batch_size=16,  # 训练批次\n        per_device_eval_batch_size=16,\n        report_to='tensorboard',  # 训练输出记录\n        eval_strategy=\"epoch\",\n        local_rank=local_rank,   # 当前进程 RANK\n        fp16=True,               # 使用混合精度\n        lr_scheduler_type='linear',  # 动态学习率\n        warmup_steps=100,        # 预热步数\n        ddp_find_unused_parameters=False  # 优化DDP性能\n    )\n    \n    def compute_metric(result):\n        # result 是一个tuple (predicts, labels)\n        \n        # 获取评估对象\n        seqeval = evaluate.load('seqeval')\n        predicts,labels = result\n        predicts = np.argmax(predicts, axis=2)\n        \n        # 准备评估数据\n        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n                     for ps,ls in zip(predicts,labels)]\n        results = seqeval.compute(predictions=predicts, references=labels)\n    \n        return results\n    \n    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n    \n    trainer = Trainer(\n        model,\n        args,\n        train_dataset=ds2['train'],\n        eval_dataset=ds2['validation'],\n        data_collator=data_collator,\n        compute_metrics=compute_metric\n    )\n    \n    trainer.train()\n\ndef main():\n    world_size = torch.cuda.device_count()\n    # 启动 world_size 个独立的 Python 进程，每个进程运行一次 train() 函数，并传入对应的进程编号（rank）作为第一个参数。\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:53:06.643491Z","iopub.execute_input":"2025-06-13T08:53:06.644156Z","iopub.status.idle":"2025-06-13T08:53:06.651389Z","shell.execute_reply.started":"2025-06-13T08:53:06.644130Z","shell.execute_reply":"2025-06-13T08:53:06.650497Z"}},"outputs":[{"name":"stdout","text":"Writing ner_ddp.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!python ner_ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:53:06.652131Z","iopub.execute_input":"2025-06-13T08:53:06.652347Z","iopub.status.idle":"2025-06-13T09:16:42.620272Z","shell.execute_reply.started":"2025-06-13T08:53:06.652332Z","shell.execute_reply":"2025-06-13T09:16:42.619390Z"}},"outputs":[{"name":"stdout","text":"2025-06-13 08:53:20.546201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749804800.770440     119 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749804800.839491     119 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-13 08:53:42.138282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-13 08:53:42.150816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749804822.161069     136 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749804822.168069     136 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749804822.173233     135 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749804822.180087     135 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nREADME.md: 100%|██████████████████████████████| 21.0/21.0 [00:00<00:00, 139kB/s]\ndataset_infos.json: 100%|██████████████████████| 970/970 [00:00<00:00, 7.56MB/s]\n(…)-00000-of-00001-a33d0e4276aef9b4.parquet: 100%|█| 1.30M/1.30M [00:00<00:00, 2\n(…)-00000-of-00001-07f476b71c5edde6.parquet: 100%|█| 178k/178k [00:00<00:00, 346\nGenerating train split: 100%|██| 10748/10748 [00:00<00:00, 174885.38 examples/s]\nGenerating validation split: 100%|█| 1343/1343 [00:00<00:00, 348207.35 examples/\ntokenizer_config.json: 100%|██████████████████| 49.0/49.0 [00:00<00:00, 322kB/s]\nconfig.json: 100%|█████████████████████████████| 624/624 [00:00<00:00, 6.98MB/s]\nvocab.txt: 100%|█████████████████████████████| 110k/110k [00:00<00:00, 4.70MB/s]\ntokenizer.json: 100%|████████████████████████| 269k/269k [00:00<00:00, 11.0MB/s]\nMap: 100%|███████████████████████| 10748/10748 [00:01<00:00, 9948.25 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:01<00:00, 9854.69 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 9485.32 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 9496.51 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:05<00:00, 1889.76 examples/s]\nMap: 100%|███████████████████████| 10748/10748 [00:05<00:00, 1876.07 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1985.20 examples/s]\nMap: 100%|█████████████████████████| 1343/1343 [00:00<00:00, 1994.97 examples/s]\nmodel.safetensors: 100%|██████████████████████| 412M/412M [00:02<00:00, 188MB/s]\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n  0%|                                                   | 0/336 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n100%|█████████████████████████████████████████| 336/336 [21:33<00:00,  3.72s/it]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n\n  0%|                                                    | 0/42 [00:00<?, ?it/s]\u001b[A\n  0%|                                                    | 0/42 [00:00<?, ?it/s]\u001b[A\n  5%|██                                          | 2/42 [00:01<00:23,  1.70it/s]\u001b[A\n  5%|██                                          | 2/42 [00:01<00:24,  1.65it/s]\u001b[A\n  7%|███▏                                        | 3/42 [00:02<00:32,  1.22it/s]\u001b[A\n  7%|███▏                                        | 3/42 [00:02<00:33,  1.16it/s]\u001b[A\n 10%|████▏                                       | 4/42 [00:03<00:35,  1.06it/s]\u001b[A\n 10%|████▏                                       | 4/42 [00:03<00:38,  1.01s/it]\u001b[A\n 12%|█████▏                                      | 5/42 [00:04<00:38,  1.04s/it]\u001b[A\n 12%|█████▏                                      | 5/42 [00:04<00:40,  1.09s/it]\u001b[A\n 14%|██████▎                                     | 6/42 [00:05<00:39,  1.10s/it]\u001b[A\n 14%|██████▎                                     | 6/42 [00:06<00:40,  1.13s/it]\u001b[A\n 17%|███████▎                                    | 7/42 [00:07<00:39,  1.14s/it]\u001b[A\n 17%|███████▎                                    | 7/42 [00:07<00:40,  1.17s/it]\u001b[A\n 19%|████████▍                                   | 8/42 [00:08<00:39,  1.16s/it]\u001b[A\n 19%|████████▍                                   | 8/42 [00:08<00:40,  1.19s/it]\u001b[A\n 21%|█████████▍                                  | 9/42 [00:09<00:38,  1.18s/it]\u001b[A\n 21%|█████████▍                                  | 9/42 [00:09<00:39,  1.20s/it]\u001b[A\n 24%|██████████▏                                | 10/42 [00:10<00:38,  1.20s/it]\u001b[A\n 24%|██████████▏                                | 10/42 [00:11<00:38,  1.21s/it]\u001b[A\n 26%|███████████▎                               | 11/42 [00:12<00:37,  1.21s/it]\u001b[A\n 26%|███████████▎                               | 11/42 [00:12<00:37,  1.22s/it]\u001b[A\n 29%|████████████▎                              | 12/42 [00:13<00:36,  1.22s/it]\u001b[A\n 29%|████████████▎                              | 12/42 [00:13<00:36,  1.22s/it]\u001b[A\n 31%|█████████████▎                             | 13/42 [00:14<00:35,  1.23s/it]\u001b[A\n 31%|█████████████▎                             | 13/42 [00:14<00:35,  1.23s/it]\u001b[A\n 33%|██████████████▎                            | 14/42 [00:15<00:34,  1.23s/it]\u001b[A\n 33%|██████████████▎                            | 14/42 [00:16<00:34,  1.23s/it]\u001b[A\n 36%|███████████████▎                           | 15/42 [00:16<00:33,  1.23s/it]\u001b[A\n 36%|███████████████▎                           | 15/42 [00:17<00:33,  1.24s/it]\u001b[A\n 38%|████████████████▍                          | 16/42 [00:18<00:31,  1.22s/it]\u001b[A\n 38%|████████████████▍                          | 16/42 [00:18<00:32,  1.27s/it]\u001b[A\n 40%|█████████████████▍                         | 17/42 [00:19<00:30,  1.22s/it]\u001b[A\n 40%|█████████████████▍                         | 17/42 [00:19<00:31,  1.26s/it]\u001b[A\n 43%|██████████████████▍                        | 18/42 [00:20<00:29,  1.23s/it]\u001b[A\n 43%|██████████████████▍                        | 18/42 [00:21<00:30,  1.25s/it]\u001b[A\n 45%|███████████████████▍                       | 19/42 [00:21<00:28,  1.23s/it]\u001b[A\n 45%|███████████████████▍                       | 19/42 [00:22<00:28,  1.25s/it]\u001b[A\n 48%|████████████████████▍                      | 20/42 [00:23<00:27,  1.23s/it]\u001b[A\n 48%|████████████████████▍                      | 20/42 [00:23<00:27,  1.25s/it]\u001b[A\n 50%|█████████████████████▌                     | 21/42 [00:24<00:26,  1.24s/it]\u001b[A\n 50%|█████████████████████▌                     | 21/42 [00:24<00:26,  1.25s/it]\u001b[A\n 52%|██████████████████████▌                    | 22/42 [00:25<00:24,  1.24s/it]\u001b[A\n 52%|██████████████████████▌                    | 22/42 [00:26<00:24,  1.24s/it]\u001b[A\n 55%|███████████████████████▌                   | 23/42 [00:26<00:23,  1.24s/it]\u001b[A\n 55%|███████████████████████▌                   | 23/42 [00:27<00:23,  1.24s/it]\u001b[A\n 57%|████████████████████████▌                  | 24/42 [00:28<00:22,  1.24s/it]\u001b[A\n 57%|████████████████████████▌                  | 24/42 [00:28<00:22,  1.23s/it]\u001b[A\n 60%|█████████████████████████▌                 | 25/42 [00:29<00:21,  1.24s/it]\u001b[A\n 60%|█████████████████████████▌                 | 25/42 [00:29<00:21,  1.24s/it]\u001b[A\n 62%|██████████████████████████▌                | 26/42 [00:30<00:18,  1.17s/it]\u001b[A\n 62%|██████████████████████████▌                | 26/42 [00:31<00:21,  1.31s/it]\u001b[A\n 64%|███████████████████████████▋               | 27/42 [00:31<00:17,  1.19s/it]\u001b[A\n 64%|███████████████████████████▋               | 27/42 [00:32<00:19,  1.29s/it]\u001b[A\n 67%|████████████████████████████▋              | 28/42 [00:32<00:16,  1.21s/it]\u001b[A\n 67%|████████████████████████████▋              | 28/42 [00:33<00:17,  1.28s/it]\u001b[A\n 69%|█████████████████████████████▋             | 29/42 [00:34<00:15,  1.23s/it]\u001b[A\n 69%|█████████████████████████████▋             | 29/42 [00:34<00:16,  1.27s/it]\u001b[A\n 71%|██████████████████████████████▋            | 30/42 [00:35<00:14,  1.23s/it]\u001b[A\n 71%|██████████████████████████████▋            | 30/42 [00:36<00:15,  1.25s/it]\u001b[A\n 74%|███████████████████████████████▋           | 31/42 [00:36<00:13,  1.24s/it]\u001b[A\n 74%|███████████████████████████████▋           | 31/42 [00:37<00:13,  1.25s/it]\u001b[A\n 76%|████████████████████████████████▊          | 32/42 [00:37<00:12,  1.24s/it]\u001b[A\n 76%|████████████████████████████████▊          | 32/42 [00:38<00:12,  1.24s/it]\u001b[A\n 79%|█████████████████████████████████▊         | 33/42 [00:39<00:11,  1.24s/it]\u001b[A\n 79%|█████████████████████████████████▊         | 33/42 [00:39<00:11,  1.24s/it]\u001b[A\n 81%|██████████████████████████████████▊        | 34/42 [00:40<00:09,  1.25s/it]\u001b[A\n 81%|██████████████████████████████████▊        | 34/42 [00:41<00:09,  1.25s/it]\u001b[A\n 83%|███████████████████████████████████▊       | 35/42 [00:41<00:08,  1.25s/it]\u001b[A\n 83%|███████████████████████████████████▊       | 35/42 [00:42<00:08,  1.24s/it]\u001b[A\n 86%|████████████████████████████████████▊      | 36/42 [00:42<00:07,  1.25s/it]\u001b[A\n 86%|████████████████████████████████████▊      | 36/42 [00:43<00:07,  1.24s/it]\u001b[A\n 88%|█████████████████████████████████████▉     | 37/42 [00:44<00:06,  1.25s/it]\u001b[A\n 88%|█████████████████████████████████████▉     | 37/42 [00:44<00:06,  1.24s/it]\u001b[A\n 90%|██████████████████████████████████████▉    | 38/42 [00:45<00:05,  1.25s/it]\u001b[A\n 90%|██████████████████████████████████████▉    | 38/42 [00:46<00:04,  1.22s/it]\u001b[A\n 93%|███████████████████████████████████████▉   | 39/42 [00:46<00:03,  1.29s/it]\u001b[A\n 93%|███████████████████████████████████████▉   | 39/42 [00:47<00:03,  1.22s/it]\u001b[A\n 95%|████████████████████████████████████████▉  | 40/42 [00:47<00:02,  1.27s/it]\u001b[A\n 95%|████████████████████████████████████████▉  | 40/42 [00:48<00:02,  1.23s/it]\u001b[A\n 98%|█████████████████████████████████████████▉ | 41/42 [00:49<00:01,  1.26s/it]\u001b[A\n 98%|█████████████████████████████████████████▉ | 41/42 [00:49<00:01,  1.24s/it]\u001b[A\n100%|███████████████████████████████████████████| 42/42 [00:50<00:00,  1.26s/it]\u001b[A\n100%|███████████████████████████████████████████| 42/42 [00:50<00:00,  1.13s/it]\u001b[A\n\nDownloading builder script: 100%|██████████| 6.34k/6.34k [00:00<00:00, 23.7MB/s]\u001b[A\u001b[A\nTrainer is attempting to log a value of \"{'precision': 0.5185995623632386, 'recall': 0.6353887399463807, 'f1': 0.5710843373493977, 'number': 373}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7597402597402597, 'recall': 0.7597402597402597, 'f1': 0.7597402597402597, 'number': 154}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6894117647058824, 'recall': 0.7751322751322751, 'f1': 0.7297633872976339, 'number': 378}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.711764705882353, 'recall': 0.8203389830508474, 'f1': 0.7622047244094488, 'number': 295}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6404833836858006, 'recall': 0.8582995951417004, 'f1': 0.7335640138408304, 'number': 247}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.75, 'recall': 0.7549668874172185, 'f1': 0.7524752475247526, 'number': 151}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7823529411764706, 'recall': 0.8580645161290322, 'f1': 0.8184615384615385, 'number': 465}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.696, 'recall': 0.7111716621253406, 'f1': 0.7035040431266846, 'number': 367}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6993865030674846, 'recall': 0.789838337182448, 'f1': 0.7418655097613882, 'number': 433}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.616, 'recall': 0.7368421052631579, 'f1': 0.6710239651416122, 'number': 209}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.0158870629966259, 'eval_ADDRESS': {'precision': 0.5185995623632386, 'recall': 0.6353887399463807, 'f1': 0.5710843373493977, 'number': 373}, 'eval_BOOK': {'precision': 0.7597402597402597, 'recall': 0.7597402597402597, 'f1': 0.7597402597402597, 'number': 154}, 'eval_COMPANY': {'precision': 0.6894117647058824, 'recall': 0.7751322751322751, 'f1': 0.7297633872976339, 'number': 378}, 'eval_GAME': {'precision': 0.711764705882353, 'recall': 0.8203389830508474, 'f1': 0.7622047244094488, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6404833836858006, 'recall': 0.8582995951417004, 'f1': 0.7335640138408304, 'number': 247}, 'eval_MOVIE': {'precision': 0.75, 'recall': 0.7549668874172185, 'f1': 0.7524752475247526, 'number': 151}, 'eval_NAME': {'precision': 0.7823529411764706, 'recall': 0.8580645161290322, 'f1': 0.8184615384615385, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.696, 'recall': 0.7111716621253406, 'f1': 0.7035040431266846, 'number': 367}, 'eval_POSITION': {'precision': 0.6993865030674846, 'recall': 0.789838337182448, 'f1': 0.7418655097613882, 'number': 433}, 'eval_SCENE': {'precision': 0.616, 'recall': 0.7368421052631579, 'f1': 0.6710239651416122, 'number': 209}, 'eval_overall_precision': 0.680734998564456, 'eval_overall_recall': 0.7718098958333334, 'eval_overall_f1': 0.7234172387490466, 'eval_overall_accuracy': 0.9951629979523455, 'eval_runtime': 59.0072, 'eval_samples_per_second': 22.76, 'eval_steps_per_second': 0.712, 'epoch': 1.0}\n100%|█████████████████████████████████████████| 336/336 [22:35<00:00,  3.72s/it]\n100%|███████████████████████████████████████████| 42/42 [00:57<00:00,  1.26s/it]\u001b[A\n{'train_runtime': 1355.3493, 'train_samples_per_second': 7.93, 'train_steps_per_second': 0.248, 'train_loss': 0.1609686783381871, 'epoch': 1.0}\n100%|█████████████████████████████████████████| 336/336 [22:35<00:00,  4.03s/it]\n[rank0]:[W613 09:16:37.620482574 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTrainer is attempting to log a value of \"{'precision': 0.5291479820627802, 'recall': 0.6327077747989276, 'f1': 0.5763125763125763, 'number': 373}\" of type <class 'dict'> for key \"eval/ADDRESS\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7222222222222222, 'recall': 0.7597402597402597, 'f1': 0.740506329113924, 'number': 154}\" of type <class 'dict'> for key \"eval/BOOK\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6713286713286714, 'recall': 0.7619047619047619, 'f1': 0.7137546468401488, 'number': 378}\" of type <class 'dict'> for key \"eval/COMPANY\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6965317919075145, 'recall': 0.8169491525423729, 'f1': 0.7519500780031201, 'number': 295}\" of type <class 'dict'> for key \"eval/GAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6369230769230769, 'recall': 0.8380566801619433, 'f1': 0.7237762237762237, 'number': 247}\" of type <class 'dict'> for key \"eval/GOVERNMENT\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8287671232876712, 'recall': 0.8013245033112583, 'f1': 0.8148148148148149, 'number': 151}\" of type <class 'dict'> for key \"eval/MOVIE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7875243664717348, 'recall': 0.8688172043010752, 'f1': 0.8261758691206543, 'number': 465}\" of type <class 'dict'> for key \"eval/NAME\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7015706806282722, 'recall': 0.7302452316076294, 'f1': 0.7156208277703604, 'number': 367}\" of type <class 'dict'> for key \"eval/ORGANIZATION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.6834381551362684, 'recall': 0.7528868360277137, 'f1': 0.7164835164835166, 'number': 433}\" of type <class 'dict'> for key \"eval/POSITION\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.5914396887159533, 'recall': 0.7272727272727273, 'f1': 0.6523605150214593, 'number': 209}\" of type <class 'dict'> for key \"eval/SCENE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n                                                                                \n\u001b[A{'eval_loss': 0.016729798167943954, 'eval_ADDRESS': {'precision': 0.5291479820627802, 'recall': 0.6327077747989276, 'f1': 0.5763125763125763, 'number': 373}, 'eval_BOOK': {'precision': 0.7222222222222222, 'recall': 0.7597402597402597, 'f1': 0.740506329113924, 'number': 154}, 'eval_COMPANY': {'precision': 0.6713286713286714, 'recall': 0.7619047619047619, 'f1': 0.7137546468401488, 'number': 378}, 'eval_GAME': {'precision': 0.6965317919075145, 'recall': 0.8169491525423729, 'f1': 0.7519500780031201, 'number': 295}, 'eval_GOVERNMENT': {'precision': 0.6369230769230769, 'recall': 0.8380566801619433, 'f1': 0.7237762237762237, 'number': 247}, 'eval_MOVIE': {'precision': 0.8287671232876712, 'recall': 0.8013245033112583, 'f1': 0.8148148148148149, 'number': 151}, 'eval_NAME': {'precision': 0.7875243664717348, 'recall': 0.8688172043010752, 'f1': 0.8261758691206543, 'number': 465}, 'eval_ORGANIZATION': {'precision': 0.7015706806282722, 'recall': 0.7302452316076294, 'f1': 0.7156208277703604, 'number': 367}, 'eval_POSITION': {'precision': 0.6834381551362684, 'recall': 0.7528868360277137, 'f1': 0.7164835164835166, 'number': 433}, 'eval_SCENE': {'precision': 0.5914396887159533, 'recall': 0.7272727272727273, 'f1': 0.6523605150214593, 'number': 209}, 'eval_overall_precision': 0.6775768016078093, 'eval_overall_recall': 0.7682291666666666, 'eval_overall_f1': 0.7200610221205187, 'eval_overall_accuracy': 0.9951004630491437, 'eval_runtime': 59.9392, 'eval_samples_per_second': 22.406, 'eval_steps_per_second': 0.701, 'epoch': 1.0}\n100%|█████████████████████████████████████████| 336/336 [22:36<00:00,  3.73s/it]\n100%|███████████████████████████████████████████| 42/42 [00:58<00:00,  1.13s/it]\u001b[A\n{'train_runtime': 1356.3295, 'train_samples_per_second': 7.924, 'train_steps_per_second': 0.248, 'train_loss': 0.15190516199384416, 'epoch': 1.0}\n100%|█████████████████████████████████████████| 336/336 [22:36<00:00,  4.04s/it]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# inference\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport torch\nmodel_path = \"ner_train/checkpoint-336\"\n# 加载保存的模型和 tokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.eval()  # 切换为推理模式\n\nid2label = model.config.id2label\n\ntext = \"小明在北京上班，不在美国上班\"\n\n# 字符级 token 列表\ntokens = list(text)\n\n# tokenizer 编码（字符输入，注意 is_split_into_words=True）\ninputs = tokenizer(tokens, return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n\n# 推理\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n\npred_ids = predictions[0].tolist()\ninput_ids = inputs[\"input_ids\"][0].tolist()\nattention_mask = inputs[\"attention_mask\"][0].tolist()\n\nfor char, pred_id, mask in zip(tokens, pred_ids, attention_mask):\n    if mask == 1:\n        label = id2label[pred_id]\n        print(f\"{char}\\t{label}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:19:22.603591Z","iopub.execute_input":"2025-06-13T09:19:22.604241Z","iopub.status.idle":"2025-06-13T09:19:22.940017Z","shell.execute_reply.started":"2025-06-13T09:19:22.604218Z","shell.execute_reply":"2025-06-13T09:19:22.939181Z"}},"outputs":[{"name":"stdout","text":"小\tO\n明\tO\n在\tO\n北\tO\n京\tB-ADDRESS\n上\tO\n班\tO\n，\tO\n不\tO\n在\tO\n美\tO\n国\tO\n上\tO\n班\tO\n","output_type":"stream"}],"execution_count":13}]}