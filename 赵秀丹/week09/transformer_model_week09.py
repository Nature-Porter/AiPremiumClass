{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport math\nimport torch\n\n##positional\n###模型\nclass PositionalEncoding(nn.Module):\n    def __init__(self,emb_size,dropout,maxlen=5000):\n        super().__init__()\n        ##行缩放数值\n        den=torch.exp(-torch.arange(0,emb_size,2)*math.log(10000)/emb_size)\n        ##位置编码索引\n        pos=torch.arange(0,maxlen).reshape(maxlen,1)\n        ##编码矩阵\n        pos_embedding=torch.zeros((maxlen,emb_size))\n        pos_embedding[:,0::2]=torch.sin(pos*den)\n        pos_embedding[:,1::2]=torch.cos(pos*den)\n        ##添加batch对应维度\n        pos_embedding=pos_embedding.unsqueeze(0)\n        ##dropout\n        self.dropout=nn.Dropout(dropout)\n        ##注册当前矩阵不参与参数更新\n        self.register_buffer('pos_embedding',pos_embedding)\n\n    def forward(self,token_embedding):\n        token_len=token_embedding.size(1)\n        add_emb=self.pos_embedding[:,:token_len,:]+token_embedding\n        return self.dropout(add_emb)\n##transformer\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self,d_model,nhead,num_enc_layers,num_dec_layers,\n                  dim_forward,dropout,enc_voc_size,dec_voc_size):\n        super().__init__()\n        ##transformer\n        self.transformer=nn.Transformer(d_model=d_model,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_enc_layers,\n                                       num_decoder_layers=num_dec_layers,\n                                       dim_feedforward=dim_forward,\n                                       dropout=dropout,\n                                       batch_first=True)\n        ##encoder input embedding\n        self.enc_emb=nn.Embedding(enc_voc_size,d_model)\n        ##decoder input embedding\n        self.dec_emb=nn.Embedding(dec_voc_size,d_model)\n        ##predict generate linear\n        self.predict=nn.Linear(d_model,dec_voc_size)  ##token预测基于解码器词典\n        ##positional encoding\n        self.pos_encoding=PositionalEncoding(d_model,dropout)\n\n    def forward(self,enc_inp,dec_inp,tgt_mask,enc_pad_mask,dec_pad_mask):\n        ##multi head attention 之间基于位置编码embedding生成\n        enc_emb=self.pos_encoding(self.enc_emb(enc_inp))\n        \n        dec_emb=self.pos_encoding(self.dec_emb(dec_inp))\n        ##调用transformer计算\n        outs=self.transformer(src=enc_emb,tgt=dec_emb,tgt_mask=tgt_mask,\n                             src_key_padding_mask=enc_pad_mask,\n                             tgt_key_padding_mask=dec_pad_mask,\n                                       batch_first=True)\n        ##推理\n        return self.predict(outs)\n\n    ##推理环节使用\n    def encode(self,enc_inp):\n       enc_emb=self.pos_encoding(self.enc_emb(enc_inp))\n       return self.transformer.encoder(enc_emb)\n\n\n    def decode(self,dec_inp,memory,dec_mask):\n       dec_emb=self.pos_encoding(self.dec_emb(dec_inp))\n       return self.transformer.decoder(dec_emb,memory,dec_mask)","metadata":{"_uuid":"036230c6-054e-4f32-8be9-0b4d0f99396c","_cell_guid":"0cb43a0c-0d9e-47e1-a973-9d5e015b9c34","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}