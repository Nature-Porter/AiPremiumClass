{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2726695,"sourceType":"datasetVersion","datasetId":1661983}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"83bd1d5f-d8cf-4de6-873f-56f163795e94","_cell_guid":"9c9c1a04-b185-4a79-b783-a0f02242eb38","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-16T16:32:20.772754Z","iopub.execute_input":"2025-05-16T16:32:20.773000Z","iopub.status.idle":"2025-05-16T16:32:22.157998Z","shell.execute_reply.started":"2025-05-16T16:32:20.772978Z","shell.execute_reply":"2025-05-16T16:32:22.157315Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/chinese-couplets/couplet/vocabs\n/kaggle/input/chinese-couplets/couplet/test/out.txt\n/kaggle/input/chinese-couplets/couplet/test/in.txt\n/kaggle/input/chinese-couplets/couplet/test/.in.txt.swp\n/kaggle/input/chinese-couplets/couplet/test/.out.txt.swp\n/kaggle/input/chinese-couplets/couplet/train/out.txt\n/kaggle/input/chinese-couplets/couplet/train/in.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"file_in='/kaggle/input/chinese-couplets/couplet/train/in.txt'\nfile_out='/kaggle/input/chinese-couplets/couplet/train/out.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:32:26.087303Z","iopub.execute_input":"2025-05-16T16:32:26.087962Z","iopub.status.idle":"2025-05-16T16:32:26.091425Z","shell.execute_reply.started":"2025-05-16T16:32:26.087939Z","shell.execute_reply":"2025-05-16T16:32:26.090666Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"###读取数据\ntry:\n    enc_tokens = []\n    dec_tokens = []\n    cnt=0\n\n    with open(file_in, 'r', encoding='utf-8') as f1, open(file_out, 'r', encoding='utf-8') as f2:\n        for line1, line2 in zip(f1, f2):\n            cnt+=1\n            if cnt==1000:\n                break\n            chs=line1+\" , \"+line2\n            chs = chs.split(\" \")\n            chs = [element for element in chs if element != \"\\n\"]\n\n            \n            for i in range(1,len(chs)):\n                enc=chs[:i]\n                dec=['<s>']+chs[i:]+['</s>']\n                enc_tokens.append(enc)\n                dec_tokens.append(dec)\n            \n            \n            \nexcept FileNotFoundError:\n    print(\"文件未找到，请检查文件路径。\")\nexcept Exception as e:\n    print(f\"读取文件时出现错误: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:32:28.795582Z","iopub.execute_input":"2025-05-16T16:32:28.795844Z","iopub.status.idle":"2025-05-16T16:32:28.838332Z","shell.execute_reply.started":"2025-05-16T16:32:28.795825Z","shell.execute_reply":"2025-05-16T16:32:28.837813Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(enc_tokens[:5])\nprint(\"----------------------\")\nprint(dec_tokens[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:32:33.308028Z","iopub.execute_input":"2025-05-16T16:32:33.308564Z","iopub.status.idle":"2025-05-16T16:32:33.312767Z","shell.execute_reply.started":"2025-05-16T16:32:33.308539Z","shell.execute_reply":"2025-05-16T16:32:33.312095Z"}},"outputs":[{"name":"stdout","text":"[['晚'], ['晚', '风'], ['晚', '风', '摇'], ['晚', '风', '摇', '树'], ['晚', '风', '摇', '树', '树']]\n----------------------\n[['<s>', '风', '摇', '树', '树', '还', '挺', ',', '晨', '露', '润', '花', '花', '更', '红', '</s>'], ['<s>', '摇', '树', '树', '还', '挺', ',', '晨', '露', '润', '花', '花', '更', '红', '</s>'], ['<s>', '树', '树', '还', '挺', ',', '晨', '露', '润', '花', '花', '更', '红', '</s>'], ['<s>', '树', '还', '挺', ',', '晨', '露', '润', '花', '花', '更', '红', '</s>'], ['<s>', '还', '挺', ',', '晨', '露', '润', '花', '花', '更', '红', '</s>']]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"len(dec_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:32:45.406134Z","iopub.execute_input":"2025-05-16T16:32:45.406621Z","iopub.status.idle":"2025-05-16T16:32:45.411897Z","shell.execute_reply.started":"2025-05-16T16:32:45.406595Z","shell.execute_reply":"2025-05-16T16:32:45.411220Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"18760"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"vocab_file='/kaggle/input/chinese-couplets/couplet/vocabs'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:32:48.347209Z","iopub.execute_input":"2025-05-16T16:32:48.347745Z","iopub.status.idle":"2025-05-16T16:32:48.350888Z","shell.execute_reply.started":"2025-05-16T16:32:48.347723Z","shell.execute_reply":"2025-05-16T16:32:48.350205Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"###读取词典\nwith open(vocab_file,encoding='utf-8') as f:\n    vocab=f.read().split(\"\\n\")\n    vocab=['<pad>']+[tk for tk in vocab if tk !='']\n    vocab={tk:i for i,tk in enumerate(vocab)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:32:50.761625Z","iopub.execute_input":"2025-05-16T16:32:50.762163Z","iopub.status.idle":"2025-05-16T16:32:50.773580Z","shell.execute_reply.started":"2025-05-16T16:32:50.762140Z","shell.execute_reply":"2025-05-16T16:32:50.773103Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"vocab['<s>']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:32:53.788457Z","iopub.execute_input":"2025-05-16T16:32:53.788718Z","iopub.status.idle":"2025-05-16T16:32:53.793630Z","shell.execute_reply.started":"2025-05-16T16:32:53.788700Z","shell.execute_reply":"2025-05-16T16:32:53.792753Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torch.nn as nn\nimport math\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:32:57.895851Z","iopub.execute_input":"2025-05-16T16:32:57.896523Z","iopub.status.idle":"2025-05-16T16:33:02.004594Z","shell.execute_reply.started":"2025-05-16T16:32:57.896499Z","shell.execute_reply":"2025-05-16T16:33:02.004038Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"###模型\nclass PositionalEncoding(nn.Module):\n    def __init__(self,emb_size,dropout,maxlen=5000):\n        super().__init__()\n        ##行缩放数值\n        den=torch.exp(-torch.arange(0,emb_size,2)*math.log(10000)/emb_size)\n        ##位置编码索引\n        pos=torch.arange(0,maxlen).reshape(maxlen,1)\n        ##编码矩阵\n        pos_embedding=torch.zeros((maxlen,emb_size))\n        pos_embedding[:,0::2]=torch.sin(pos*den)\n        pos_embedding[:,1::2]=torch.cos(pos*den)\n        ##添加batch对应维度\n        pos_embedding=pos_embedding.unsqueeze(0)\n        ##dropout\n        self.dropout=nn.Dropout(dropout)\n        ##注册当前矩阵不参与参数更新\n        self.register_buffer('pos_embedding',pos_embedding)\n\n    def forward(self,token_embedding):\n        token_len=token_embedding.size(1)\n        add_emb=self.pos_embedding[:,:token_len,:]+token_embedding\n        return self.dropout(add_emb)\n\nclass Seq2SeqTransformer(nn.Module):\n\n    def __init__(self,d_model,nhead,num_enc_layers,num_dec_layers,\n                dim_forward,dropout,enc_voc_size,dec_voc_size):\n        super().__init__()\n        ##transformer\n        self.transformer=nn.Transformer(d_model=d_model,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_enc_layers,\n                                       num_decoder_layers=num_dec_layers,\n                                       dim_feedforward=dim_forward,\n                                       dropout=dropout,\n                                                   batch_first=True  # 启用批次优先模式\n)\n        ##encoder input embedding\n        self.enc_emb=nn.Embedding(enc_voc_size,d_model)\n        ##decoder input embedding\n        self.dec_emb=nn.Embedding(dec_voc_size,d_model)\n        ##predict generate linear\n        self.predict=nn.Linear(d_model,dec_voc_size)\n        ##positional encoding\n        self.pos_encoding=PositionalEncoding(d_model,dropout)\n\n    def forward(self,enc_inp,dec_inp,tgt_mask,enc_pad_mask,dec_pad_mask):\n        ##multi head attention 之前基于位置编码embedding生成\n        enc_emb=self.pos_encoding(self.enc_emb(enc_inp))\n        dec_emb=self.pos_encoding(self.dec_emb(dec_inp))\n\n        ##调用transformer 计算\n        outs=self.transformer(src=enc_emb,tgt=dec_emb,tgt_mask=tgt_mask,\n                             src_key_padding_mask=enc_pad_mask,\n                             tgt_key_padding_mask=dec_pad_mask)\n        #推理\n        return self.predict(outs)\n\n       ##推理环节使用\n    def encode(self,enc_inp):\n        enc_emb=self.pos_encoding(self.enc_emb(enc_inp))\n        return self.transformer.encoder(enc_emb)\n\n    def decode(self,dec_inp,memory,dec_mask):\n        dec_emb=self.pos_encoding(self.dec_emb(dec_inp))\n        return self.transformer.decoder(dec_emb,memory,dec_mask)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:33:03.544659Z","iopub.execute_input":"2025-05-16T16:33:03.545034Z","iopub.status.idle":"2025-05-16T16:33:03.554202Z","shell.execute_reply.started":"2025-05-16T16:33:03.545014Z","shell.execute_reply":"2025-05-16T16:33:03.553527Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def get_proc(enc_voc, dec_voc):\n\n    # 嵌套函数定义\n    # 外部函数变量生命周期会延续到内部函数调用结束 （闭包）\n\n    def batch_proc(data):\n        \"\"\"\n        批次数据处理并返回\n        \"\"\"\n        enc_ids, dec_ids= [],[]\n        for enc,dec in data:\n            # token -> token index\n            enc_idx = [vocab[tk] for tk in enc]\n            dec_idx = [vocab[tk] for tk in dec]\n\n            # encoder_input\n            enc_ids.append(torch.tensor(enc_idx))\n            # decoder_input\n            dec_ids.append(torch.tensor(dec_idx))\n        \n        \n        # 数据转换张量 [batch, max_token_len]\n        # 用批次中最长token序列构建张量\n        enc_input = pad_sequence(enc_ids, batch_first=True)\n        dec_input = pad_sequence(dec_ids, batch_first=True)\n\n        # 返回数据都是模型训练和推理的需要\n        return enc_input, dec_input\n\n    # 返回回调函数\n    return batch_proc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:33:08.960450Z","iopub.execute_input":"2025-05-16T16:33:08.960956Z","iopub.status.idle":"2025-05-16T16:33:08.966088Z","shell.execute_reply.started":"2025-05-16T16:33:08.960935Z","shell.execute_reply":"2025-05-16T16:33:08.965225Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nds = list(zip(enc_tokens,dec_tokens))\ndl = DataLoader(ds, batch_size=64, shuffle=True, collate_fn=get_proc(vocab, vocab))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:33:11.651143Z","iopub.execute_input":"2025-05-16T16:33:11.651466Z","iopub.status.idle":"2025-05-16T16:33:11.659159Z","shell.execute_reply.started":"2025-05-16T16:33:11.651445Z","shell.execute_reply":"2025-05-16T16:33:11.658405Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"d_model=512\nnhead=8\nnum_enc_layers=6\nnum_dec_layers=6\ndim_feedforward=2048\ndropout=0.3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:33:14.056344Z","iopub.execute_input":"2025-05-16T16:33:14.056868Z","iopub.status.idle":"2025-05-16T16:33:14.060632Z","shell.execute_reply.started":"2025-05-16T16:33:14.056842Z","shell.execute_reply":"2025-05-16T16:33:14.059996Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device = torch.device('cuda')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:33:16.668583Z","iopub.execute_input":"2025-05-16T16:33:16.669143Z","iopub.status.idle":"2025-05-16T16:33:16.672538Z","shell.execute_reply.started":"2025-05-16T16:33:16.669122Z","shell.execute_reply":"2025-05-16T16:33:16.671817Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\nfrom torch.nn.utils.rnn import pad_sequence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:33:18.581108Z","iopub.execute_input":"2025-05-16T16:33:18.581373Z","iopub.status.idle":"2025-05-16T16:33:18.585084Z","shell.execute_reply.started":"2025-05-16T16:33:18.581355Z","shell.execute_reply":"2025-05-16T16:33:18.584426Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n# 生成掩码\ndef generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:33:20.799854Z","iopub.execute_input":"2025-05-16T16:33:20.800221Z","iopub.status.idle":"2025-05-16T16:33:20.804420Z","shell.execute_reply.started":"2025-05-16T16:33:20.800199Z","shell.execute_reply":"2025-05-16T16:33:20.803759Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# 构建训练模型\n# 模型构建\nmodel = Seq2SeqTransformer(\n    d_model=512,\n    nhead=8,\n    num_enc_layers=6,\n    num_dec_layers=6,\n    dim_forward=2048,\n    dropout=0.3,\nenc_voc_size=len(vocab),\ndec_voc_size=len(vocab))\nmodel.to(device)\n\n# 优化器、损失\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\ntrain_loss_cnt=0\ntotal_loss=0\nepochs=2\n# 训练\nfor epoch in range(epochs):\n    model.train()\n    tpbar = tqdm(dl)\n    for enc_input, dec_input in tpbar:\n        enc_input = enc_input.long()\n        dec_input = dec_input.long()\n        enc_input = enc_input.to(device)\n        dec_input = dec_input.to(device)\n\n        # 生成掩码\n        tgt_mask = generate_square_subsequent_mask(dec_input.size(1)).to(device)\n        enc_pad_mask = (enc_input == 0).to(device)\n        dec_pad_mask = (dec_input == 0).to(device)\n        # 前向传播 \n        logits = model(enc_input, dec_input,tgt_mask,enc_pad_mask,dec_pad_mask)\n        # 计算损失\n        tgt_output = dec_input[:, 1:].contiguous().view(-1)  # 移除 <bos>，形状：(batch * (tgt_len-1),)\n        output = logits[:, :-1].contiguous().view(-1, logits.size(-1))  # 移除最后一个位置（假设是 <eos>）\n        \n       \n        loss = criterion(output, tgt_output)\n            \n        # 反向传播和优化\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # 梯度裁剪\n        optimizer.step()\n            \n        total_loss += loss.item()\n        \n        # 打印训练信息\n        tpbar.set_description(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n\n\ntorch.save(model.state_dict(), 'transformer.bin')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:33:24.183438Z","iopub.execute_input":"2025-05-16T16:33:24.184012Z","iopub.status.idle":"2025-05-16T16:35:13.335250Z","shell.execute_reply.started":"2025-05-16T16:33:24.183992Z","shell.execute_reply":"2025-05-16T16:35:13.334429Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/294 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\nEpoch 1, Loss: 3.6583: 100%|██████████| 294/294 [00:53<00:00,  5.53it/s]\nEpoch 2, Loss: 4.1365: 100%|██████████| 294/294 [00:52<00:00,  5.61it/s]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":" ##1、通过词典把token转换为token_index\n    ##2、通过Dataloader把encoder,decoder封装为带有batch的训练数据\n    #3 Dataloader的collate_fn调用自定义转换方法填充模型训练数据\n    ##3.1  encoder矩阵使用pad_sequence填充\n    ##3.2 decoder前面部分训练数据dec_token_matrix[:,:-1,:]\n    ##3.3 decoder后面部分训练目标 dec_token_matrix[:,1:,:]\n    ##创建mask\n    #4.1、dec_mask上三角填充 -inf的mask\n    #4.2 enc_pad_mask :{enc矩阵==0}\n    #4.3 dec_pad_mask:{dec矩阵==0}\n    ##5 创建模型（根据GPU内存大小设计编码和解码器参数和层数）优化器 损失\n    ##6 训练模型并保存","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}