{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. 参考课堂案例，使用指定的数据集，编写代码实现ner模型训练和推流。\nhttps://huggingface.co/datasets/doushabao4766/msra_ner_k_V3","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ntry:\n    # 直接通过数据集路径加载\n    dataset = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n    print(dataset)\nexcept Exception as e:\n    print(f\"加载失败，错误信息：{e}\")\n    # 尝试指定缓存路径或重新下载\n    dataset = load_dataset(\"doushabao4766/msra_ner_k_V3\", download_mode=\"force_redownload\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:10.310326Z","iopub.execute_input":"2025-06-01T08:52:10.310543Z","iopub.status.idle":"2025-06-01T08:52:14.518906Z","shell.execute_reply.started":"2025-06-01T08:52:10.310525Z","shell.execute_reply":"2025-06-01T08:52:14.518302Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3d6d633262d4f1cbb18809c909bf697"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-42717a92413393f9.parquet:   0%|          | 0.00/13.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8c5fdd832e343b1ac4f012712ebbefb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-8899cab5fdab45bc.parquet:   0%|          | 0.00/946k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f9ad67a17f49488ba526e1570dce95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deb74e2c68324dda91a7132977f6cacf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae0892c1677d4961a9560000d9304415"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 3443\n    })\n})\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\n\n# 加载数据集\ndataset = load_dataset(\"doushabao4766/msra_ner_k_V3\")\n\n# 查看数据集结构\nprint(dataset)\n\n# 获取标签名称映射\nif 'train' in dataset:  # 检查训练集是否存在\n    # 获取标签特征\n    ner_feature = dataset['train'].features['ner_tags']\n    \n    # 打印标签名称列表\n    print(\"\\nNER标签对应的实体类型:\")\n    for idx, label in enumerate(ner_feature.feature.names):\n        print(f\"{idx}: {label}\")\nelse:\n    print(\"数据集中不包含训练集。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:14.519523Z","iopub.execute_input":"2025-06-01T08:52:14.519986Z","iopub.status.idle":"2025-06-01T08:52:15.661131Z","shell.execute_reply.started":"2025-06-01T08:52:14.519966Z","shell.execute_reply":"2025-06-01T08:52:15.660401Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n        num_rows: 3443\n    })\n})\n\nNER标签对应的实体类型:\n0: O\n1: B-PER\n2: I-PER\n3: B-ORG\n4: I-ORG\n5: B-LOC\n6: I-LOC\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:15.662885Z","iopub.execute_input":"2025-06-01T08:52:15.663101Z","iopub.status.idle":"2025-06-01T08:52:15.667538Z","shell.execute_reply.started":"2025-06-01T08:52:15.663084Z","shell.execute_reply":"2025-06-01T08:52:15.666859Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_=dataset['train']\ntrain_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:15.668151Z","iopub.execute_input":"2025-06-01T08:52:15.668332Z","iopub.status.idle":"2025-06-01T08:52:15.685352Z","shell.execute_reply.started":"2025-06-01T08:52:15.668317Z","shell.execute_reply":"2025-06-01T08:52:15.684643Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n    num_rows: 45001\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification,AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:15.685999Z","iopub.execute_input":"2025-06-01T08:52:15.686194Z","iopub.status.idle":"2025-06-01T08:52:27.259149Z","shell.execute_reply.started":"2025-06-01T08:52:15.686180Z","shell.execute_reply":"2025-06-01T08:52:27.258600Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690469914d0a4e43893290787e576550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"593d065e05ef4a84af93f3650809b0c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b063e78b8fa40d98a0eb32596845d76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1edddb3b371946b0924d8b75656e0b02"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def data_input_proc(item):\n    ##输入文本转换模型输入token索引\n    text = [\"\".join(val) for val in item['tokens']]\n    input_data=tokenizer(text,truncation=True,add_special_tokens=False,max_length=512)\n    adjust_labels=[]\n    ##上一步骤生成ner_tag中索引和token对齐\n    for k in range(len(input_data['input_ids'])):\n\n        word_ids=input_data.word_ids(k)\n        tags=item['ner_tags'][k]\n        \n        adjusted_label_ids=[]\n        i,prev_wid=-1,-1\n        for wid in word_ids:\n            if (wid!=prev_wid):\n                i+=1\n                prev_wid=wid\n            adjusted_label_ids.append(tags[i])\n        adjust_labels.append(adjusted_label_ids)\n    input_data['labels']=adjust_labels\n    return input_data \n\nds2=dataset.map(data_input_proc,batched=True) #batchted 每次传入自定义方法样板数量","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:27.259764Z","iopub.execute_input":"2025-06-01T08:52:27.260287Z","iopub.status.idle":"2025-06-01T08:52:36.115593Z","shell.execute_reply.started":"2025-06-01T08:52:27.260269Z","shell.execute_reply":"2025-06-01T08:52:36.114766Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5f9b87023784ef297ae2607a9eb9ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec401e93e0e4dc3ae3f499e14a427aa"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"for item in ds2['train']:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:36.116377Z","iopub.execute_input":"2025-06-01T08:52:36.116641Z","iopub.status.idle":"2025-06-01T08:52:36.121046Z","shell.execute_reply.started":"2025-06-01T08:52:36.116611Z","shell.execute_reply":"2025-06-01T08:52:36.120262Z"}},"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'knowledge': '', 'input_ids': [2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"##记录转换为pytorch\nds2.set_format('torch',columns=['input_ids','token_type_ids','attention_mask','labels'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:36.121886Z","iopub.execute_input":"2025-06-01T08:52:36.122090Z","iopub.status.idle":"2025-06-01T08:52:36.429814Z","shell.execute_reply.started":"2025-06-01T08:52:36.122074Z","shell.execute_reply":"2025-06-01T08:52:36.429057Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"####模型训练","metadata":{}},{"cell_type":"code","source":"for item in ds2['train']:\n    print(item)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:36.432247Z","iopub.execute_input":"2025-06-01T08:52:36.432463Z","iopub.status.idle":"2025-06-01T08:52:36.450785Z","shell.execute_reply.started":"2025-06-01T08:52:36.432448Z","shell.execute_reply":"2025-06-01T08:52:36.450214Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n        1159, 8013]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0])}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import TrainingArguments,Trainer\n\nargs=TrainingArguments(\n    output_dir=\"ner_train\"  ##模型训练工作目录\n    ,num_train_epochs=2\n    ,save_safetensors=False  ##特定格式的文件,是否保存为torch.load格式\n    ,per_device_train_batch_size=32 #训练批次\n    ,per_device_eval_batch_size=32\n    ,report_to='tensorboard' #模型输出记录\n     ,eval_strategy='epoch' # No  steps epoch \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:36.451507Z","iopub.execute_input":"2025-06-01T08:52:36.451766Z","iopub.status.idle":"2025-06-01T08:52:50.485351Z","shell.execute_reply.started":"2025-06-01T08:52:36.451742Z","shell.execute_reply":"2025-06-01T08:52:50.484781Z"}},"outputs":[{"name":"stderr","text":"2025-06-01 08:52:38.437067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748767958.618908      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748767958.671026      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 手动收集所有出现过的标签 ID\nall_tag_ids = set()\nfor example in dataset['train']:\n    all_tag_ids.update(example['ner_tags'])\n\n# 将 ID 转换为标签名称（如果已知映射关系）\nid_to_tag = dataset['train'].features['ner_tags'].feature.names\ntag_names = [id_to_tag[i] for i in sorted(all_tag_ids)]\n\n# 创建映射\nid2lbl = {i: tag for i, tag in enumerate(tag_names)}\nlbl2id = {tag: i for i, tag in enumerate(tag_names)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:50.486139Z","iopub.execute_input":"2025-06-01T08:52:50.486620Z","iopub.status.idle":"2025-06-01T08:52:55.255385Z","shell.execute_reply.started":"2025-06-01T08:52:50.486602Z","shell.execute_reply":"2025-06-01T08:52:55.254854Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(tag_names)\nprint(id2lbl)\nprint(lbl2id)\nprint(tag_names[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:55.256141Z","iopub.execute_input":"2025-06-01T08:52:55.256358Z","iopub.status.idle":"2025-06-01T08:52:55.260578Z","shell.execute_reply.started":"2025-06-01T08:52:55.256342Z","shell.execute_reply":"2025-06-01T08:52:55.259964Z"}},"outputs":[{"name":"stdout","text":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\nB-PER\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###Model\nfrom transformers import AutoModelForTokenClassification\nmodel=AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n                                                     num_labels=7,\n                                                     id2label=id2lbl,\n                                                     label2id=lbl2id)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:55.261304Z","iopub.execute_input":"2025-06-01T08:52:55.261603Z","iopub.status.idle":"2025-06-01T08:52:57.944332Z","shell.execute_reply.started":"2025-06-01T08:52:55.261580Z","shell.execute_reply":"2025-06-01T08:52:57.943620Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9d755393aaa4a2fa9d60dd00fc8c971"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=7, bias=True)\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"!pip install evaluate\n!pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:52:57.945162Z","iopub.execute_input":"2025-06-01T08:52:57.945426Z","iopub.status.idle":"2025-06-01T08:53:07.810522Z","shell.execute_reply.started":"2025-06-01T08:52:57.945410Z","shell.execute_reply":"2025-06-01T08:53:07.809572Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2025.3.0\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=05f61b837c675f3166f7be36c39f15d574ca0ee5a5015510f9410e3ebcc1f420\n  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\nimport evaluate\nimport seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:53:07.811716Z","iopub.execute_input":"2025-06-01T08:53:07.812056Z","iopub.status.idle":"2025-06-01T08:53:08.080077Z","shell.execute_reply.started":"2025-06-01T08:53:07.812015Z","shell.execute_reply":"2025-06-01T08:53:08.079529Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"##metric方法\ndef compute_metric(result):\n    ##获取评估对象\n    seqeval=evaluate.load('seqeval')\n    predicts,labels=result\n    predicts=np.argmax(predicts,axis=2)\n    ###获取评估对象\n    seqeval=evaluate.load('seqeval')\n    ###准备评估数据\n    predicts=[[tag_names[p] for p,l in zip(ps,ls) if l!=-100]\n                    for ps,ls in zip(predicts,labels)]\n    labels=[[tag_names[l] for p,l in zip(ps,ls) if l!=-100]\n                    for ps,ls in zip(predicts,labels) ]\n    results=seqeval.compute(predictions=predicts,references=labels)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:53:08.080804Z","iopub.execute_input":"2025-06-01T08:53:08.081039Z","iopub.status.idle":"2025-06-01T08:53:08.086236Z","shell.execute_reply.started":"2025-06-01T08:53:08.081020Z","shell.execute_reply":"2025-06-01T08:53:08.085633Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"##Trainer\n\ndata_collator=DataCollatorForTokenClassification(tokenizer=tokenizer\n                                                ,padding=True)\ntrainer=Trainer(\n    model,\n    args,\n    train_dataset=ds2['train'],\n    eval_dataset=ds2['test'],\n    data_collator=data_collator,\n    compute_metrics=compute_metric\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:53:08.087066Z","iopub.execute_input":"2025-06-01T08:53:08.087318Z","iopub.status.idle":"2025-06-01T08:53:08.679927Z","shell.execute_reply.started":"2025-06-01T08:53:08.087294Z","shell.execute_reply":"2025-06-01T08:53:08.679236Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T08:53:08.680609Z","iopub.execute_input":"2025-06-01T08:53:08.680849Z","iopub.status.idle":"2025-06-01T09:18:36.405014Z","shell.execute_reply.started":"2025-06-01T08:53:08.680833Z","shell.execute_reply":"2025-06-01T09:18:36.404396Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1408' max='1408' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1408/1408 25:24, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Loc</th>\n      <th>Org</th>\n      <th>Per</th>\n      <th>Overall Precision</th>\n      <th>Overall Recall</th>\n      <th>Overall F1</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.108100</td>\n      <td>0.046691</td>\n      <td>{'precision': 0.865518504946867, 'recall': 0.8296452406041447, 'f1': 0.8472022955523674, 'number': 2847}</td>\n      <td>{'precision': 0.7580174927113703, 'recall': 0.7884761182714177, 'f1': 0.7729468599033816, 'number': 1319}</td>\n      <td>{'precision': 0.8762541806020067, 'recall': 0.8692767086927671, 'f1': 0.8727514990006662, 'number': 1507}</td>\n      <td>0.842030</td>\n      <td>0.830601</td>\n      <td>0.836277</td>\n      <td>0.984538</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.043900</td>\n      <td>0.044094</td>\n      <td>{'precision': 0.870456998920475, 'recall': 0.84966631541974, 'f1': 0.8599360113757554, 'number': 2847}</td>\n      <td>{'precision': 0.7828282828282829, 'recall': 0.8225928733889311, 'f1': 0.8022181146025877, 'number': 1319}</td>\n      <td>{'precision': 0.8942498347653668, 'recall': 0.8978102189781022, 'f1': 0.8960264900662253, 'number': 1507}</td>\n      <td>0.855407</td>\n      <td>0.856161</td>\n      <td>0.855784</td>\n      <td>0.986470</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa8f4bed590144a2a0f0fc061366c30a"}},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'precision': 0.865518504946867, 'recall': 0.8296452406041447, 'f1': 0.8472022955523674, 'number': 2847}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7580174927113703, 'recall': 0.7884761182714177, 'f1': 0.7729468599033816, 'number': 1319}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8762541806020067, 'recall': 0.8692767086927671, 'f1': 0.8727514990006662, 'number': 1507}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTrainer is attempting to log a value of \"{'precision': 0.870456998920475, 'recall': 0.84966631541974, 'f1': 0.8599360113757554, 'number': 2847}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.7828282828282829, 'recall': 0.8225928733889311, 'f1': 0.8022181146025877, 'number': 1319}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'precision': 0.8942498347653668, 'recall': 0.8978102189781022, 'f1': 0.8960264900662253, 'number': 1507}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1408, training_loss=0.06329780681566759, metrics={'train_runtime': 1527.2386, 'train_samples_per_second': 58.931, 'train_steps_per_second': 0.922, 'total_flos': 7795678239418674.0, 'train_loss': 0.06329780681566759, 'epoch': 2.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"####模型推理","metadata":{}},{"cell_type":"code","source":"result=trainer.predict(ds2['test'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T09:18:36.405874Z","iopub.execute_input":"2025-06-01T09:18:36.406103Z","iopub.status.idle":"2025-06-01T09:19:03.102333Z","shell.execute_reply.started":"2025-06-01T09:18:36.406087Z","shell.execute_reply":"2025-06-01T09:19:03.101707Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"print(dataset['test'][10]['tokens'])\nprint(ds2['test'][10]['labels'])\n# print('predict:',result.label_ids[10] )\npred=[l  for l in result.label_ids[10]  if l!=-100]\nprint(pred)                 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T09:28:06.343553Z","iopub.execute_input":"2025-06-01T09:28:06.344159Z","iopub.status.idle":"2025-06-01T09:28:06.351511Z","shell.execute_reply.started":"2025-06-01T09:28:06.344134Z","shell.execute_reply":"2025-06-01T09:28:06.350896Z"}},"outputs":[{"name":"stdout","text":"['在', '跨', '世', '纪', '的', '征', '途', '上', '，', '在', '中', '国', '共', '产', '党', '领', '导', '下', '，', '我', '们', '要', '努', '力', '实', '现', '包', '括', '各', '民', '主', '党', '派', '、', '各', '人', '民', '团', '体', '、', '无', '党', '派', '人', '士', '在', '内', '的', '全', '体', '中', '国', '人', '民', '的', '大', '团', '结', '，', '实', '现', '包', '括', '大', '陆', '同', '胞', '、', '台', '港', '澳', '同', '胞', '和', '海', '外', '侨', '胞', '在', '内', '的', '所', '有', '爱', '国', '的', '中', '华', '儿', '女', '的', '大', '团', '结', '，', '从', '而', '战', '胜', '各', '种', '艰', '难', '险', '阻', '，', '实', '现', '跨', '世', '纪', '的', '宏', '伟', '蓝', '图', '。']\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 5, 5, 5, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"ds2['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T09:35:37.147844Z","iopub.execute_input":"2025-06-01T09:35:37.148581Z","iopub.status.idle":"2025-06-01T09:35:37.153153Z","shell.execute_reply.started":"2025-06-01T09:35:37.148547Z","shell.execute_reply":"2025-06-01T09:35:37.152455Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'tokens', 'ner_tags', 'knowledge', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 3443\n})"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\n# 设置设备\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"使用设备: {device}\")\n\n# 加载预训练模型和分词器\n##导入模型分词器 \n# model_name = \"dslim/bert-base-NER\"  # 预训练NER模型\n# 也可以使用其他模型，例如: \"bert-base-chinese\" 或 \"xlm-roberta-large-finetuned-conll03-english\"\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForTokenClassification.from_pretrained(model_name).to(device)\n\n# 定义标签映射（根据具体模型可能需要调整）\n# id2label = model.config.id2label\n# \n# 创建NER推理管道\nner_pipeline = pipeline(\n    \"ner\",\n    model=model,\n    tokenizer=tokenizer,\n    aggregation_strategy=\"simple\",  # 合并子词预测\n    device=device.index if device.type == \"cuda\" else -1\n)\n\ndef predict_entities(text: str) -> list:\n    \"\"\"\n    对输入文本进行NER预测\n    \n    Args:\n        text: 输入文本\n    \n    Returns:\n        识别出的实体列表，每个实体包含文本、类型、起始和结束位置\n    \"\"\"\n    # 使用管道进行预测\n    entities = ner_pipeline(text)\n    \n    # 处理预测结果\n    results = []\n    for entity in entities:\n        results.append({\n            \"entity\": entity[\"entity_group\"],\n            \"content\": entity[\"word\"].replace(\" \",\"\")\n        })\n    \n    return results\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:11:59.715790Z","iopub.execute_input":"2025-06-01T10:11:59.716077Z","iopub.status.idle":"2025-06-01T10:11:59.723560Z","shell.execute_reply.started":"2025-06-01T10:11:59.716057Z","shell.execute_reply":"2025-06-01T10:11:59.722784Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"使用设备: cuda\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# 示例用法\n\nsample_texts = [\n    \"豆包在上海的复旦大学参加学术会议\",\n    \"双方确定了今后发展中美关系的指导方针\",\n    \"习近平总书记在上海考察调研。\",  # 中文示例\n     \"张三今天去了北京故宫博物院参观。\",\n    \"阿里巴巴集团成立于1999年，总部位于杭州。\",\n    \"王小明在清华大学购买了华为手机。\",\n    \"上海复旦大学附属中山医院位于徐汇区。\",\n    \"鲁迅的《呐喊》是中国现代文学经典。\",\n    \"2023年7月我在纽约参加学术会议。\",\n    \"腾讯CEO马化腾出席了深圳科技大会。\",\n    \"从北京到上海需要经过济南和南京。\",\n    \"科学家李四在中科院物理研究所发表论文。\",\n    \"《哈利·波特》作者J.K.罗琳居住在英国伦敦。\"\n]\n\n# 对每个测试文本进行预测\nfor text in sample_texts:\n    entities = predict_entities(text)\n    print(\"输入：\",text)\n    print(\"输出：\",entities)\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:12:40.136247Z","iopub.execute_input":"2025-06-01T10:12:40.136548Z","iopub.status.idle":"2025-06-01T10:12:40.269846Z","shell.execute_reply.started":"2025-06-01T10:12:40.136517Z","shell.execute_reply":"2025-06-01T10:12:40.269181Z"}},"outputs":[{"name":"stdout","text":"输入： 豆包在上海的复旦大学参加学术会议\n输出： [{'entity': 'PER', 'content': '豆包'}, {'entity': 'LOC', 'content': '上海的'}, {'entity': 'ORG', 'content': '复旦大学'}]\n\n输入： 双方确定了今后发展中美关系的指导方针\n输出： [{'entity': 'LOC', 'content': '中'}, {'entity': 'LOC', 'content': '美'}]\n\n输入： 习近平总书记在上海考察调研。\n输出： [{'entity': 'PER', 'content': '习近平'}, {'entity': 'LOC', 'content': '上海'}]\n\n输入： 张三今天去了北京故宫博物院参观。\n输出： [{'entity': 'PER', 'content': '张三'}, {'entity': 'ORG', 'content': '北'}, {'entity': 'LOC', 'content': '京'}, {'entity': 'ORG', 'content': '故宫博物院'}]\n\n输入： 阿里巴巴集团成立于1999年，总部位于杭州。\n输出： [{'entity': 'ORG', 'content': '阿里巴巴集团'}, {'entity': 'LOC', 'content': '杭州'}]\n\n输入： 王小明在清华大学购买了华为手机。\n输出： [{'entity': 'PER', 'content': '王小明'}, {'entity': 'ORG', 'content': '清华大学'}, {'entity': 'ORG', 'content': '华为手'}]\n\n输入： 上海复旦大学附属中山医院位于徐汇区。\n输出： [{'entity': 'ORG', 'content': '上海复旦大学附属中山医院'}, {'entity': 'LOC', 'content': '徐汇区'}]\n\n输入： 鲁迅的《呐喊》是中国现代文学经典。\n输出： [{'entity': 'PER', 'content': '鲁迅'}, {'entity': 'LOC', 'content': '中国'}]\n\n输入： 2023年7月我在纽约参加学术会议。\n输出： [{'entity': 'LOC', 'content': '约参'}]\n\n输入： 腾讯CEO马化腾出席了深圳科技大会。\n输出： [{'entity': 'ORG', 'content': '腾讯'}, {'entity': 'PER', 'content': '马化腾'}, {'entity': 'LOC', 'content': '深圳'}]\n\n输入： 从北京到上海需要经过济南和南京。\n输出： [{'entity': 'LOC', 'content': '北京'}, {'entity': 'LOC', 'content': '上海'}, {'entity': 'LOC', 'content': '济南'}, {'entity': 'LOC', 'content': '南京'}]\n\n输入： 科学家李四在中科院物理研究所发表论文。\n输出： [{'entity': 'PER', 'content': '李四'}, {'entity': 'ORG', 'content': '中科院物理研究所'}]\n\n输入： 《哈利·波特》作者J.K.罗琳居住在英国伦敦。\n输出： [{'entity': 'PER', 'content': '哈利·波特'}, {'entity': 'PER', 'content': '罗琳'}, {'entity': 'LOC', 'content': '英国'}, {'entity': 'LOC', 'content': '伦敦'}]\n\n","output_type":"stream"}],"execution_count":66}]}