{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12432713,"sourceType":"datasetVersion","datasetId":7842292}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-10T16:08:56.436909Z","iopub.execute_input":"2025-07-10T16:08:56.437466Z","iopub.status.idle":"2025-07-10T16:08:56.767074Z","shell.execute_reply.started":"2025-07-10T16:08:56.437440Z","shell.execute_reply":"2025-07-10T16:08:56.766376Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/camel-xiangzi-1/Camel Xiangzi.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"##版本2，加上下三角掩码矩阵\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n##模型训练数据集 尽量使用gpu ,kaggle \n\n\ndef get_batch(split): ##split区别是训练和验证数据集\n    ##选择训练或者验证数据集\n    data=train_data if split=='train' else val_data\n    ##动态从数据集中选择一个位置索引\n    ix=torch.randint(len(data)-block_size-1,(batch_size,)) ##随机生成位置索引，向后截取block_size个字符作为训练\n    x=torch.stack([data[i:i+block_size] for i in ix])\n    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n\n    x=x.to(device)\n    y=y.to(device)\n    return x,y\n\nclass Head(nn.Module):\n     \"\"\"单头 self-attention\"\"\"\n     def __init__(self,n_embd):\n         super().__init__()\n         self.key=nn.Linear(n_embd,n_embd,bias=False)\n         self.query=nn.Linear(n_embd,n_embd,bias=False)\n         self.value=nn.Linear(n_embd,n_embd,bias=False)\n     def forward(self,input_x):\n         B,T,C=input_x.shape\n\n         k=self.key(input_x)\n         q=self.query(input_x)\n         v=self.value(input_x)\n\n         wei=q@k.transpose(-2,-1)*C**-0.5\n         T=wei.shape[-1]\n         tril=torch.tril(torch.ones(T,T,device=wei.device))\n         wei=wei.masked_fill(tril==0,float('-inf'))\n         wei=wei.softmax(dim=-1)\n\n         out=wei@v\n         return out\n\n\n    \n\nclass BingramLanguageModel(nn.Module):\n    def __init__(self,block_size, vocab_size,n_embd):\n        super().__init__()\n # 每个token都直接从Embedding中查询对应的logits值 以进⾏下⼀个token的推理\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        ##位置编码\n        self.position_embedding_table=nn.Embedding(block_size,n_embd)      \n         ##  one  head self-attention\n        self.sa_head=Head(n_embd)\n        #large model forward\n        self.lm_head=nn.Linear(n_embd,vocab_size)\n    def forward(self, idx, targets=None):\n        B,T=idx.shape\n \n # idx值和targets值都是整型张量 (B,T)\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        tok_emb.to(device)\n        pos_emb=self.position_embedding_table(torch.arange(T,device=device))\n        x=tok_emb+pos_emb\n        x=self.sa_head(x)\n        logits=self.lm_head(x)\n \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(-1)\n            loss = F.cross_entropy(logits, targets)\n \n        return logits, loss\n \n    def generate(self, idx, max_new_tokens):\n # idx (B,T) 数组对应着当前的输⼊内容 [1,1]\n        for _ in range(max_new_tokens):\n            # 模型推理\n            ##限定索引列的取值范围\n            idx_cond=idx[:,-block_size:]\n            logits, _ = self(idx_cond) # (B,T) -> (1,1...100)\n            # 获取最后⼀个时间步的输出\n            logits = logits[:, -1, :] # (1,100,65) -> (1,65)\n            # 应⽤softmax转换为概率值\n            probs = F.softmax(logits, dim=-1) # (B,C)\n            # 按权重值采样，返回对应的索引\n            # idx_next = torch.argmax(probs, dim=-1)\n            # 随机采样\n            idx_next = torch.multinomial(probs,num_samples=1) # (B,1)\n            # 应⽤采样后的索引\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) [1,2],[1,3]... [1,max_new_tokens]\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T16:09:00.402282Z","iopub.execute_input":"2025-07-10T16:09:00.402944Z","iopub.status.idle":"2025-07-10T16:09:02.036497Z","shell.execute_reply.started":"2025-07-10T16:09:00.402920Z","shell.execute_reply":"2025-07-10T16:09:02.035831Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"block_size=8\nbatch_size=32\nmax_iter=10000\nlearn_rate=1e-3\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nn_embd=32\neval_interval=500\neval_iters=200\n\nfile_name='/kaggle/input/camel-xiangzi-1/Camel Xiangzi.txt'\n\nwith open(file_name) as f:\n    text=f.read()\n\n###词典 编码器(函数），解码器（函数）\nchars=sorted(list(set(text)))\n\n\nstoi={ ch:i for  i,ch in enumerate(chars)}\nitos={ i:ch for  i,ch in enumerate(chars)}\nvocab_size=len(stoi)\n\nencode=lambda s:  [ stoi[c] for c in s ]\ndecode=lambda l:  ''.join( itos[i] for i in l )\n\n\n##文本转换 token index \ndata=torch.tensor(encode(text),dtype=torch.long)\n\n#拆分数据集\nn=int(len(data)*.9)\n\ntrain_data=data[:n]\nval_data=data[n:]\n\n###模型训练\nmodel=BingramLanguageModel(block_size,vocab_size,n_embd)\nmodel.to(device)\noptimizer=torch.optim.AdamW(model.parameters(),lr=learn_rate)\n@torch.no_grad()\ndef estimate_loss():\n    out={}\n    model.eval()\n    for split in ['train','val']:\n        losses=torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X,Y=get_batch(split)\n            X.to(device),Y.to(device)\n            logits,loss=model(X,Y)\n            losses[k]=loss.item()\n        out[split]=losses.mean()\n    model.train()\n    return out \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T16:09:07.319971Z","iopub.execute_input":"2025-07-10T16:09:07.320687Z","iopub.status.idle":"2025-07-10T16:09:08.459477Z","shell.execute_reply.started":"2025-07-10T16:09:07.320660Z","shell.execute_reply":"2025-07-10T16:09:08.458897Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # 放在代码最开头\nos.environ['TORCH_USE_CUDA_DSA'] = '1'     # 启用设备端断言详情\n\n\nfor iter in range(max_iter):\n    if iter % eval_interval==0:\n        losses=estimate_loss()\n        print(f\"step {iter} :train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    ##批次样本\n    xb,yb=get_batch('train')\n\n    logits,loss=model(xb,yb)\n\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n# 通过模型⽣成\nidx = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T16:09:13.619493Z","iopub.execute_input":"2025-07-10T16:09:13.620199Z","iopub.status.idle":"2025-07-10T16:09:55.127134Z","shell.execute_reply.started":"2025-07-10T16:09:13.620174Z","shell.execute_reply":"2025-07-10T16:09:55.126342Z"}},"outputs":[{"name":"stdout","text":"step 0 :train loss 7.9296, val loss 7.9365\nstep 500 :train loss 5.8629, val loss 6.0876\nstep 1000 :train loss 5.6886, val loss 5.9427\nstep 1500 :train loss 5.3220, val loss 5.6958\nstep 2000 :train loss 5.1150, val loss 5.5712\nstep 2500 :train loss 4.9691, val loss 5.4449\nstep 3000 :train loss 4.8068, val loss 5.3786\nstep 3500 :train loss 4.7457, val loss 5.3328\nstep 4000 :train loss 4.6489, val loss 5.2738\nstep 4500 :train loss 4.5693, val loss 5.2663\nstep 5000 :train loss 4.5463, val loss 5.2186\nstep 5500 :train loss 4.4914, val loss 5.1961\nstep 6000 :train loss 4.4568, val loss 5.1789\nstep 6500 :train loss 4.4194, val loss 5.1686\nstep 7000 :train loss 4.3856, val loss 5.1738\nstep 7500 :train loss 4.3543, val loss 5.1669\nstep 8000 :train loss 4.3339, val loss 5.1860\nstep 8500 :train loss 4.2837, val loss 5.1443\nstep 9000 :train loss 4.2778, val loss 5.1308\nstep 9500 :train loss 4.2622, val loss 5.1666\n\n　　“放，钱不怎好象承认识了话？”\n\n　　靠！”嘴。一流恰巧的圈儿，咱们，堵着，就是外。一老确收的书风拦住—哼数老头，便不愿意祥子又天还是不得词里。说什么儿找他不是工的，就是就是说不住，每点愁她的雨淡\n","output_type":"stream"}],"execution_count":4}]}