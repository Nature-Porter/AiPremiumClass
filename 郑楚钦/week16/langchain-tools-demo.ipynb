{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-07-09T11:45:07.197719Z","iopub.status.busy":"2025-07-09T11:45:07.197361Z","iopub.status.idle":"2025-07-09T11:45:11.789112Z","shell.execute_reply":"2025-07-09T11:45:11.787070Z","shell.execute_reply.started":"2025-07-09T11:45:07.197692Z"},"trusted":true},"outputs":[],"source":["!pip install langchain_community langchain_openai langchainhub faiss-cpu pdfminer.six > /dev/null"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-07-09T11:45:11.791699Z","iopub.status.busy":"2025-07-09T11:45:11.791353Z","iopub.status.idle":"2025-07-09T11:45:11.798207Z","shell.execute_reply":"2025-07-09T11:45:11.796638Z","shell.execute_reply.started":"2025-07-09T11:45:11.791667Z"},"trusted":true},"outputs":[],"source":["OPENAI_API_KEY = 'OPENAI_API_KEY'\n","BASE_URL = 'https://open.bigmodel.cn/api/paas/v4'\n","MODEL_NAME = 'glm-4-plus'\n","# MODEL_NAME = 'glm-z1-flashx'\n","EMBEDDING_MODEL_NAME = 'embedding-3'\n","PROMT_NAME = 'hwchase17/openai-functions-agent'\n","TAVILY_API_KEY = 'TAVILY_API_KEY'"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-07-09T11:45:11.799500Z","iopub.status.busy":"2025-07-09T11:45:11.799153Z","iopub.status.idle":"2025-07-09T11:45:11.910471Z","shell.execute_reply":"2025-07-09T11:45:11.909344Z","shell.execute_reply.started":"2025-07-09T11:45:11.799471Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='北京今天天气怎么样', additional_kwargs={}, response_metadata={}), HumanMessage(content='tavily_search_results_json', additional_kwargs={}, response_metadata={})]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n","  warnings.warn(\n"]}],"source":["from langchain import hub\n","\n","prompt = hub.pull(PROMT_NAME)\n","resp = prompt.invoke({\n","    \"agent_scratchpad\": [\"tavily_search_results_json\"],\n","    \"input\": \"北京今天天气怎么样\"\n","})\n","print(resp)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-07-09T11:45:11.912573Z","iopub.status.busy":"2025-07-09T11:45:11.912282Z","iopub.status.idle":"2025-07-09T11:45:11.919124Z","shell.execute_reply":"2025-07-09T11:45:11.918093Z","shell.execute_reply.started":"2025-07-09T11:45:11.912552Z"},"trusted":true},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","model = ChatOpenAI(model=MODEL_NAME, api_key=OPENAI_API_KEY, base_url=BASE_URL)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-07-09T11:45:11.920711Z","iopub.status.busy":"2025-07-09T11:45:11.920316Z","iopub.status.idle":"2025-07-09T11:45:11.940275Z","shell.execute_reply":"2025-07-09T11:45:11.939052Z","shell.execute_reply.started":"2025-07-09T11:45:11.920679Z"},"trusted":true},"outputs":[],"source":["def get_search_tool():\n","    from langchain_community.tools.tavily_search import TavilySearchResults\n","    search = TavilySearchResults(max_results=5, tavily_api_key=TAVILY_API_KEY)\n","    # resp = search.invoke('广东今天天气怎么样')\n","    # print(resp)\n","    return search"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-07-09T11:45:11.941833Z","iopub.status.busy":"2025-07-09T11:45:11.941391Z","iopub.status.idle":"2025-07-09T11:45:11.964499Z","shell.execute_reply":"2025-07-09T11:45:11.963418Z","shell.execute_reply.started":"2025-07-09T11:45:11.941806Z"},"trusted":true},"outputs":[],"source":["from langchain_community.document_loaders import WebBaseLoader, PDFMinerLoader\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain.tools.retriever import create_retriever_tool\n","import os\n","\n","def get_vector_store(web_path, embedding):\n","    loader = PDFMinerLoader(web_path) if web_path.endswith('.pdf') else WebBaseLoader(web_path)\n","    docs = loader.load()\n","    spliter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=100)\n","    splited_docs = spliter.split_documents(docs)\n","    print(len(splited_docs))\n","    vector_store = FAISS.from_documents(splited_docs, embedding)\n","    return vector_store\n","\n","def get_store_name(path):\n","    slash_index = path.rfind('/')+1\n","    dot_index = path.rfind('.',slash_index)\n","    store_name = path[slash_index:] if dot_index < 0 else path[slash_index:dot_index]\n","    return store_name    \n","\n","def load_vector_store(store_name, web_path, embedding):\n","    if os.path.exists(store_name):\n","        print('加载本地向量数据库', store_name)\n","        return FAISS.load_local(store_name, embedding, allow_dangerous_deserialization=True)\n","    vector_store = get_vector_store(web_path, embedding)\n","    vector_store.save_local(store_name)\n","    print('本地向量数据库保存成功', store_name)\n","    return vector_store\n","\n","def get_retriever_tool(path, description):\n","    embedding_model = OpenAIEmbeddings(\n","        api_key=OPENAI_API_KEY,\n","        base_url=BASE_URL,\n","        model='embedding-3',\n","    )\n","    store_name = get_store_name(path)\n","    vector_store = load_vector_store(store_name, path, embedding_model)\n","    retriever = vector_store.as_retriever(search_kwargs={'k': 5})\n","    tool = create_retriever_tool(retriever, f'{store_name} retriever', description=description)\n","    return tool"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-07-09T11:48:14.321521Z","iopub.status.busy":"2025-07-09T11:48:14.321153Z","iopub.status.idle":"2025-07-09T11:48:45.135934Z","shell.execute_reply":"2025-07-09T11:48:45.134852Z","shell.execute_reply.started":"2025-07-09T11:48:14.321497Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["加载本地向量数据库 2106.09685\n","\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `2106.09685 retriever` with `{'query': 'LOW-RANK ADAPTATION'}`\n","\n","\n","\u001b[0m\u001b[33;1m\u001b[1;3mLORA: LOW-RANK ADAPTATION OF LARGE LAN-\n","GUAGE MODELS\n","\n","Phillip Wallis\n","\n","Yelong Shen∗\n","Shean Wang\n","\n","Edward Hu∗\n","Yuanzhi Li\n","Microsoft Corporation\n","{edwardhu, yeshe, phwallis, zeyuana,\n","yuanzhil, swang, luw, wzchen}@microsoft.com\n","yuanzhil@andrew.cmu.edu\n","(Version 2)\n","\n","Lu Wang\n","\n","Weizhu Chen\n","\n","Zeyuan Allen-Zhu\n","\n","1\n","2\n","0\n","2\n","\n","t\n","c\n","O\n","6\n","1\n","\n","]\n","L\n","C\n",".\n","s\n","c\n","[\n","\n","2\n","v\n","5\n","8\n","6\n","9\n","0\n",".\n","6\n","0\n","1\n","2\n",":\n","v\n","i\n","X\n","r\n","a\n","\n","ABSTRACT\n","\n","An important paradigm of natural language processing consists of large-scale pre-\n","training on general domain data and adaptation to particular tasks or domains. As\n","we pre-train larger models, full ﬁne-tuning, which retrains all model parameters,\n","becomes less feasible. Using GPT-3 175B as an example – deploying indepen-\n","dent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively\n","expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-\n","trained model weights and injects trainable rank decomposition matrices into each\n","layer of the Transformer architecture, greatly reducing the number of trainable pa-\n","rameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam,\n","LoRA can reduce the number of trainable parameters by 10,000 times and the\n","GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁne-\n","tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\n","ing fewer trainable parameters, a higher training throughput, and, unlike adapters,\n","no additional inference latency. We also provide an empirical investigation into\n","rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of\n","LoRA. We release a package that facilitates the integration of LoRA with PyTorch\n","models and provide our implementations and model checkpoints for RoBERTa,\n","DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\n","\n","1\n","\n","INTRODUCTION\n","\n","F.2 ADDITIONAL EXPERIMENTS ON GPT-3\n","\n","We present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on\n","identifying the trade-off between performance and the number of trainable parameters.\n","\n","F.3 LOW-DATA REGIME\n","\n","To evaluate the performance of different adaptation approaches in the low-data regime. we randomly\n","sample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data\n","MNLI-n tasks. In Table 16, we show the performance of different adaptation approaches on MNLI-\n","n. To our surprise, PreﬁxEmbed and PreﬁxLayer performs very poorly on MNLI-100 dataset, with\n","PreﬁxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreﬁxLayer\n","performs better than PreﬁxEmbed but is still signiﬁcantly worse than Fine-Tune or LoRA on MNLI-\n","100. The gap between preﬁx-based approaches and LoRA/Fine-tuning becomes smaller as we in-\n","crease the number of training examples, which might suggest that preﬁx-based approaches are not\n","suitable for low-data tasks in GPT-3. LoRA achieves better performance than ﬁne-tuning on both\n","MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the\n","(±0.3) variance due to random seeds.\n","\n","The training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-\n","ble 17. We use a smaller learning rate for PreﬁxLayer on the MNLI-100 set, as the training loss does\n","not decrease with a larger learning rate.\n","\n","G MEASURING SIMILARITY BETWEEN SUBSPACES\n","\n","A, U j\n","In this paper we use the measure φ(A, B, i, j) = ψ(U i\n","to measure the subspace\n","B ∈ Rd×j, obtained by\n","similarity between two column orthonormal matrices U i\n","taking columns of the left singular matrices of A and B. We point out that this similarity is simply\n","a reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee\n","(2008).\n","\n","B) = (cid:107)U i(cid:62)\n","A ∈ Rd×i and U j\n","\n","A UB (cid:107)2\n","F\n","min{i,j}\n","\n","22\n","\fMethod\n","\n","Fine-Tune\n","\n","PreﬁxEmbed\n","\n","PreﬁxLayer\n","\n","AdapterH\n","\n","4.1 LOW-RANK-PARAMETRIZED UPDATE MATRICES\n","\n","A neural network contains many dense layers which perform matrix multiplication. The weight\n","matrices in these layers typically have full-rank. When adapting to a speciﬁc task, Aghajanyan et al.\n","(2020) shows that the pre-trained language models have a low “instrisic dimension” and can still\n","learn efﬁciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\n","size the updates to the weights also have a low “intrinsic rank” during adaptation. For a pre-trained\n","weight matrix W0 ∈ Rd×k, we constrain its update by representing the latter with a low-rank de-\n","composition W0 + ∆W = W0 + BA, where B ∈ Rd×r, A ∈ Rr×k, and the rank r (cid:28) min(d, k).\n","During training, W0 is frozen and does not receive gradient updates, while A and B contain trainable\n","parameters. Note both W0 and ∆W = BA are multiplied with the same input, and their respective\n","output vectors are summed coordinate-wise. For h = W0x, our modiﬁed forward pass yields:\n","\n","h = W0x + ∆W x = W0x + BAx\n","\n","(3)\n","\n","We illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for A and\n","zero for B, so ∆W = BA is zero at the beginning of training. We then scale ∆W x by α\n","r , where α\n","is a constant in r. When optimizing with Adam, tuning α is roughly the same as tuning the learning\n","rate if we scale the initialization appropriately. As a result, we simply set α to the ﬁrst r we try\n","and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\n","r (Yang & Hu, 2021).\n","\n","Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for\n","\n","matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010.\n","\n","Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n","1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\n","the 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/\n","v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001.\n","\n","Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep\n","In Proceedings of the 25th international conference\n","neural networks with multitask learning.\n","on Machine learning, ICML ’08, pp. 160–167, New York, NY, USA, July 2008. Association\n","for Computing Machinery.\n","ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\n","https://doi.org/10.1145/1390156.1390177.\n","\n","Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting\n","\n","parameters in deep learning, 2014.\n","\n","Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n","\n","bidirectional transformers for language understanding, 2019a.\n","\n","Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\n","Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019b.\n","URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805.\n","\n","William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\n","In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL\n","https://aclanthology.org/I05-5002.\n","\n","Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\n","challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\n","Natural Language Generation, pp. 124–133, 2017.\n","\n","13\n","\fBehrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\n","\n","B INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\n","\n","Adapter layers are external modules added to a pre-trained model in a sequential manner, whereas\n","our proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently,\n","adapter layers must be computed in addition to the base model, inevitably introducing additional\n","latency. While as pointed out in R¨uckl´e et al. (2020), the latency introduced by adapter layers can\n","be mitigated when the model batch size and/or sequence length is large enough to full utilize the\n","hardware parallelism. We conﬁrm their observation with a similar latency study on GPT-2 medium\n","and point out that there are scenarios, notably online inference where the batch size is small, where\n","the added latency can be signiﬁcant.\n","\n","We measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging\n","over 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension\n","r. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH,\n","and a recent, more efﬁcient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1\n","for more details on the designs. We plot the slow-down in percentage compared to the no-adapter\n","baseline in Figure 5.\n","\n","Figure 5: Percentage slow-down of inference latency compared to the no-adapter (r = 0) baseline.\n","The top row shows the result for AdapterH and the bottom row AdapterL. Larger batch size and\n","sequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an\n","online, short-sequence-length scenario. We tweak the colormap for better visibility.\n","\n","C DATASET DETAILS\u001b[0m\u001b[32;1m\u001b[1;3mLow-Rank Adaptation (LoRA) 是一种用于自然语言处理模型的适应方法，特别适用于大规模预训练模型。其核心思想是在保持预训练模型权重不变的情况下，通过在每个Transformer层中注入可训练的低秩分解矩阵来减少下游任务中可训练参数的数量。这种方法的主要优点包括：\n","\n","1. **减少可训练参数**：相比于完全微调（fine-tuning）所有模型参数，LoRA显著减少了需要训练的参数数量。例如，在GPT-3 175B模型上，LoRA可以将可训练参数减少10,000倍，GPU内存需求减少3倍。\n","\n","2. **提高训练效率**：由于减少了参数数量，LoRA具有更高的训练吞吐量。\n","\n","3. **保持模型质量**：尽管减少了可训练参数，LoRA在RoBERTa、DeBERTa、GPT-2和GPT-3等模型上的表现与完全微调相当甚至更优。\n","\n","4. **无额外推理延迟**：与适配器（adapter）方法不同，LoRA不会引入额外的推理延迟。\n","\n","### 使用原理\n","\n","LoRA的工作原理基于以下观察：预训练语言模型在适应特定任务时，其权重的更新具有低“内在秩”。具体来说，对于预训练的权重矩阵 \\( W_0 \\)，其更新被约束为低秩分解的形式 \\( W_0 + \\Delta W = W_0 + BA \\)，其中 \\( B \\in \\mathbb{R}^{d \\times r} \\)，\\( A \\in \\mathbb{R}^{r \\times k} \\)，且秩 \\( r \\ll \\min(d, k) \\)。在训练过程中，\\( W_0 \\) 被冻结，而 \\( A \\) 和 \\( B \\) 包含可训练参数。\n","\n","### 应用范围\n","\n","LoRA适用于需要在大规模预训练模型上进行特定任务或领域适应的场景，尤其在这些模型参数量巨大、完全微调不切实际的情况下。具体应用包括但不限于：\n","\n","- **文本分类**：在预训练模型基础上适应特定分类任务。\n","- **问答系统**： fine-tuning模型以更好地回答特定领域的问题。\n","- **文本生成**：调整模型以生成特定风格的文本。\n","\n","LoRA的提出为高效利用大规模预训练模型提供了一种新的途径，特别适合于资源受限或需要快速部署的场景。\n","\n","更多详细信息和技术细节可以在LoRA的GitHub仓库中找到：[https://github.com/microsoft/LoRA](https://github.com/microsoft/LoRA)。\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"data":{"text/plain":["{'input': '什么是LOW-RANK ADAPTATION，查一下它的使用原理和范围是什么？',\n"," 'output': 'Low-Rank Adaptation (LoRA) 是一种用于自然语言处理模型的适应方法，特别适用于大规模预训练模型。其核心思想是在保持预训练模型权重不变的情况下，通过在每个Transformer层中注入可训练的低秩分解矩阵来减少下游任务中可训练参数的数量。这种方法的主要优点包括：\\n\\n1. **减少可训练参数**：相比于完全微调（fine-tuning）所有模型参数，LoRA显著减少了需要训练的参数数量。例如，在GPT-3 175B模型上，LoRA可以将可训练参数减少10,000倍，GPU内存需求减少3倍。\\n\\n2. **提高训练效率**：由于减少了参数数量，LoRA具有更高的训练吞吐量。\\n\\n3. **保持模型质量**：尽管减少了可训练参数，LoRA在RoBERTa、DeBERTa、GPT-2和GPT-3等模型上的表现与完全微调相当甚至更优。\\n\\n4. **无额外推理延迟**：与适配器（adapter）方法不同，LoRA不会引入额外的推理延迟。\\n\\n### 使用原理\\n\\nLoRA的工作原理基于以下观察：预训练语言模型在适应特定任务时，其权重的更新具有低“内在秩”。具体来说，对于预训练的权重矩阵 \\\\( W_0 \\\\)，其更新被约束为低秩分解的形式 \\\\( W_0 + \\\\Delta W = W_0 + BA \\\\)，其中 \\\\( B \\\\in \\\\mathbb{R}^{d \\\\times r} \\\\)，\\\\( A \\\\in \\\\mathbb{R}^{r \\\\times k} \\\\)，且秩 \\\\( r \\\\ll \\\\min(d, k) \\\\)。在训练过程中，\\\\( W_0 \\\\) 被冻结，而 \\\\( A \\\\) 和 \\\\( B \\\\) 包含可训练参数。\\n\\n### 应用范围\\n\\nLoRA适用于需要在大规模预训练模型上进行特定任务或领域适应的场景，尤其在这些模型参数量巨大、完全微调不切实际的情况下。具体应用包括但不限于：\\n\\n- **文本分类**：在预训练模型基础上适应特定分类任务。\\n- **问答系统**： fine-tuning模型以更好地回答特定领域的问题。\\n- **文本生成**：调整模型以生成特定风格的文本。\\n\\nLoRA的提出为高效利用大规模预训练模型提供了一种新的途径，特别适合于资源受限或需要快速部署的场景。\\n\\n更多详细信息和技术细节可以在LoRA的GitHub仓库中找到：[https://github.com/microsoft/LoRA](https://github.com/microsoft/LoRA)。'}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# tools = [get_search_tool()]\n","tools = [\n","    get_search_tool(),\n","    get_retriever_tool(\n","        'https://arxiv.org/pdf/2106.09685.pdf',\n","        '大语言模型的低秩矩阵适应 LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS'\n","    )\n","]\n","\n","from langchain.agents import AgentExecutor\n","from langchain.agents import create_tool_calling_agent\n","\n","agent = create_tool_calling_agent(model, tools, prompt)\n","executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n","executor.invoke({\n","    \"input\": \"什么是LOW-RANK ADAPTATION，查一下它的使用原理和范围是什么？\"\n","})"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
