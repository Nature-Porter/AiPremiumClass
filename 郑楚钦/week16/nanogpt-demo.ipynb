{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0c6e12",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-09T08:28:12.141601Z",
     "iopub.status.busy": "2025-07-09T08:28:12.141306Z",
     "iopub.status.idle": "2025-07-09T08:28:12.190452Z",
     "shell.execute_reply": "2025-07-09T08:28:12.189698Z"
    },
    "papermill": {
     "duration": 0.053573,
     "end_time": "2025-07-09T08:28:12.191775",
     "exception": false,
     "start_time": "2025-07-09T08:28:12.138202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108611, 2748)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requests\n",
    "# input_txt_href = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "# text = requests.get(input_txt_href).text\n",
    "\n",
    "with open('/kaggle/input/wodejingshenjiayuan/.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list( set(text)))\n",
    "vocab_size = len(chars)\n",
    "s2i = {c:i for i,c in enumerate(chars)}\n",
    "i2s = {i:c for i,c in enumerate(chars)}\n",
    "encode = lambda s: [s2i[c] for c in s]\n",
    "decode = lambda l: ''.join([i2s[i] for i in l])\n",
    "\n",
    "data = encode(text)\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "n,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16bdb47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T08:28:12.196329Z",
     "iopub.status.busy": "2025-07-09T08:28:12.196094Z",
     "iopub.status.idle": "2025-07-09T08:28:16.113367Z",
     "shell.execute_reply": "2025-07-09T08:28:16.112625Z"
    },
    "papermill": {
     "duration": 3.920654,
     "end_time": "2025-07-09T08:28:16.114566",
     "exception": false,
     "start_time": "2025-07-09T08:28:12.193912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 166,  120, 2249, 1155,   82,  804, 2013, 1949],\n",
       "         [2031,  849,   89,   82, 2304,  455, 2316,  606]]),\n",
       " tensor([[ 120, 2249, 1155,   82,  804, 2013, 1949,  283],\n",
       "         [ 849,   89,   82, 2304,  455, 2316,  606, 1343]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# torch.manual_seed(317)\n",
    "\n",
    "def get_batch(data, batch_size = 4, block_size = 8):\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.tensor([data[i: i + block_size] for i in idx],dtype=torch.long)\n",
    "    y = torch.tensor([data[i+1:i+1 + block_size] for i in idx],dtype=torch.long)\n",
    "    return x,y\n",
    "\n",
    "get_batch(train_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80ce611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T08:28:16.118970Z",
     "iopub.status.busy": "2025-07-09T08:28:16.118696Z",
     "iopub.status.idle": "2025-07-09T08:28:16.128306Z",
     "shell.execute_reply": "2025-07-09T08:28:16.127815Z"
    },
    "papermill": {
     "duration": 0.012871,
     "end_time": "2025-07-09T08:28:16.129283",
     "exception": false,
     "start_time": "2025-07-09T08:28:16.116412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 多头\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_embed,dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd,head_embed,bias=False)\n",
    "        self.query = nn.Linear(n_embd,head_embed,bias=False)\n",
    "        self.value = nn.Linear(n_embd,head_embed,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        C = input_x.size(-1)\n",
    "        k = self.key(input_x)\n",
    "        q = self.query(input_x)\n",
    "        weight = q @ k.transpose(-2,-1) * C ** -0.5\n",
    "\n",
    "        T = weight.size(-1)\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        weight = weight.masked_fill(tril == 0, float('-inf'))\n",
    "        v = self.value(input_x)\n",
    "        weight = weight.softmax(dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, n_embd, head_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "        self.heads = nn.ModuleList([Head(n_embd,head_embd,dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd,n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = self.norm(x)\n",
    "        out = torch.cat([head(input) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,n_embd,num_heads,dropout):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHead(num_heads,n_embd,n_embd//num_heads,dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.LayerNorm(n_embd),\n",
    "            nn.Linear(n_embd,4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd,n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.sa_heads(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feea2b35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T08:28:16.133388Z",
     "iopub.status.busy": "2025-07-09T08:28:16.133170Z",
     "iopub.status.idle": "2025-07-09T08:28:16.139722Z",
     "shell.execute_reply": "2025-07-09T08:28:16.139165Z"
    },
    "papermill": {
     "duration": 0.009838,
     "end_time": "2025-07-09T08:28:16.140857",
     "exception": false,
     "start_time": "2025-07-09T08:28:16.131019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BingramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd,num_heads,dropout,num_block):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, num_heads, dropout) for _ in range(num_block)],\n",
    "            nn.LayerNorm(n_embd),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        token_emb = self.embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(idx.size(-1)))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        # print(logits.shape)\n",
    "        if targets != None:\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_len):\n",
    "        for _ in range(max_len):\n",
    "            logits, loss = self(idx[:, -self.block_size:])\n",
    "            # print(logits.shape)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7657c270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T08:28:16.145324Z",
     "iopub.status.busy": "2025-07-09T08:28:16.144916Z",
     "iopub.status.idle": "2025-07-09T11:01:55.945028Z",
     "shell.execute_reply": "2025-07-09T11:01:55.944199Z"
    },
    "papermill": {
     "duration": 9219.809342,
     "end_time": "2025-07-09T11:01:55.952008",
     "exception": false,
     "start_time": "2025-07-09T08:28:16.142666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "step 0: train loss 8.074625015258789 , validate loss 7.977770805358887\n",
      "我在北京的街头看到况粗普察业士谬巨聊齿藐甘炸胥侧版捕眠学牛师剔释告码惯律硝罩判仁煺如案鳄珍千拍者罢\n",
      "step 100: train loss 5.6737518310546875 , validate loss 5.836033821105957\n",
      "step 200: train loss 4.743711471557617 , validate loss 5.2092156410217285\n",
      "step 300: train loss 4.298342704772949 , validate loss 5.048758029937744\n",
      "step 400: train loss 4.038207054138184 , validate loss 4.991559028625488\n",
      "step 500: train loss 3.826084613800049 , validate loss 4.99304723739624\n",
      "step 600: train loss 3.7003636360168457 , validate loss 5.060132026672363\n",
      "step 700: train loss 3.5778374671936035 , validate loss 5.095638751983643\n",
      "step 800: train loss 3.465257167816162 , validate loss 5.190592288970947\n",
      "step 900: train loss 3.367635726928711 , validate loss 5.256810188293457\n",
      "step 1000: train loss 3.2895050048828125 , validate loss 5.291468620300293\n",
      "我在北京的街头看到了骂；后一口也不安翁一面，我看不给她之处。据说实，我就喜欢把这四这两饭吃凉来面，\n",
      "step 1100: train loss 3.2233943939208984 , validate loss 5.390722751617432\n",
      "step 1200: train loss 3.1579298973083496 , validate loss 5.436142921447754\n",
      "step 1300: train loss 3.0716233253479004 , validate loss 5.506984233856201\n",
      "step 1400: train loss 3.004093885421753 , validate loss 5.592612266540527\n",
      "step 1500: train loss 2.9576590061187744 , validate loss 5.682576656341553\n",
      "step 1600: train loss 2.8968706130981445 , validate loss 5.790794372558594\n",
      "step 1700: train loss 2.843946933746338 , validate loss 5.873342037200928\n",
      "step 1800: train loss 2.8036398887634277 , validate loss 5.905410289764404\n",
      "step 1900: train loss 2.7430474758148193 , validate loss 6.017955303192139\n",
      "step 2000: train loss 2.7167975902557373 , validate loss 6.1064887046813965\n",
      "我在北京的街头看到一次，弄鬼—我想不I历史原来改变。我们的情形也很好的大学才受到。\n",
      "　　我国电话说\n",
      "step 2100: train loss 2.669079542160034 , validate loss 6.183773994445801\n",
      "step 2200: train loss 2.6390655040740967 , validate loss 6.291817665100098\n",
      "step 2300: train loss 2.6074044704437256 , validate loss 6.2759504318237305\n",
      "step 2400: train loss 2.6019320487976074 , validate loss 6.38836669921875\n",
      "step 2500: train loss 2.5509390830993652 , validate loss 6.445313930511475\n",
      "step 2600: train loss 2.5288894176483154 , validate loss 6.505192756652832\n",
      "step 2700: train loss 2.4970362186431885 , validate loss 6.566196441650391\n",
      "step 2800: train loss 2.4700088500976562 , validate loss 6.628511905670166\n",
      "step 2900: train loss 2.4561619758605957 , validate loss 6.647028923034668\n",
      "step 3000: train loss 2.451439619064331 , validate loss 6.734480381011963\n",
      "我在北京的街头看到。说明古典人肯说和的人性使有趣味历史的总是——没什么种流露走调查。但是社会学角度\n",
      "step 3100: train loss 2.4198410511016846 , validate loss 6.789450645446777\n",
      "step 3200: train loss 2.4195070266723633 , validate loss 6.829414367675781\n",
      "step 3300: train loss 2.3981573581695557 , validate loss 6.914423942565918\n",
      "step 3400: train loss 2.3770835399627686 , validate loss 6.901510715484619\n",
      "step 3500: train loss 2.3583078384399414 , validate loss 6.9392852783203125\n",
      "step 3600: train loss 2.333557605743408 , validate loss 7.036650657653809\n",
      "step 3700: train loss 2.3410353660583496 , validate loss 7.089041709899902\n",
      "step 3800: train loss 2.321084499359131 , validate loss 7.082508087158203\n",
      "step 3900: train loss 2.3131232261657715 , validate loss 7.172000885009766\n",
      "step 4000: train loss 2.311089038848877 , validate loss 7.148283958435059\n",
      "我在北京的街头看到特勒，让她比寒碜。但第二，七年在街头中华教师，见了一台。这样弗雷，我们说，在影内\n",
      "step 4100: train loss 2.2964510917663574 , validate loss 7.244101524353027\n",
      "step 4200: train loss 2.289658308029175 , validate loss 7.266812324523926\n",
      "step 4300: train loss 2.2680466175079346 , validate loss 7.265377044677734\n",
      "step 4400: train loss 2.2630863189697266 , validate loss 7.3529157638549805\n",
      "step 4500: train loss 2.257108211517334 , validate loss 7.438760280609131\n",
      "step 4600: train loss 2.247650146484375 , validate loss 7.380495548248291\n",
      "step 4700: train loss 2.233971118927002 , validate loss 7.4139084815979\n",
      "step 4800: train loss 2.2327182292938232 , validate loss 7.484358310699463\n",
      "step 4900: train loss 2.224940061569214 , validate loss 7.460597038269043\n",
      "step 5000: train loss 2.2036585807800293 , validate loss 7.48635196685791\n",
      "我在北京的街头看到城墙，可怎意虽然有道，但也就能不能派，但是想到最美好的东西呢。比方说，篇新讲音乐\n",
      "step 5100: train loss 2.2088871002197266 , validate loss 7.496434211730957\n",
      "step 5200: train loss 2.1868374347686768 , validate loss 7.597311973571777\n",
      "step 5300: train loss 2.187034845352173 , validate loss 7.628912925720215\n",
      "step 5400: train loss 2.196408748626709 , validate loss 7.638233184814453\n",
      "step 5500: train loss 2.1832594871520996 , validate loss 7.628187656402588\n",
      "step 5600: train loss 2.178572177886963 , validate loss 7.6642746925354\n",
      "step 5700: train loss 2.168541193008423 , validate loss 7.643424034118652\n",
      "step 5800: train loss 2.1666781902313232 , validate loss 7.662889003753662\n",
      "step 5900: train loss 2.160950183868408 , validate loss 7.661416530609131\n",
      "step 6000: train loss 2.155425548553467 , validate loss 7.808237552642822\n",
      "我在北京的街头看到的东西就算什么可抱怨：明。想得不一声，自己的愿没有科学修养，而后学家就越不会惊一\n",
      "step 6100: train loss 2.1570968627929688 , validate loss 7.760082244873047\n",
      "step 6200: train loss 2.152611255645752 , validate loss 7.729424953460693\n",
      "step 6300: train loss 2.1400279998779297 , validate loss 7.756041049957275\n",
      "step 6400: train loss 2.1365256309509277 , validate loss 7.795693397521973\n",
      "step 6500: train loss 2.13358736038208 , validate loss 7.867831230163574\n",
      "step 6600: train loss 2.127516746520996 , validate loss 7.796994686126709\n",
      "step 6700: train loss 2.1254141330718994 , validate loss 7.897431373596191\n",
      "step 6800: train loss 2.125074863433838 , validate loss 7.839437007904053\n",
      "step 6900: train loss 2.1218347549438477 , validate loss 7.926034450531006\n",
      "step 7000: train loss 2.1072258949279785 , validate loss 7.855225563049316\n",
      "我在北京的街头看到的歌剧看法干部材的电影，就像头平哪。故事讲这个地址。直至于它可对环境的态度，已经\n",
      "step 7100: train loss 2.1039466857910156 , validate loss 7.912898063659668\n",
      "step 7200: train loss 2.107222557067871 , validate loss 7.880129337310791\n",
      "step 7300: train loss 2.102524757385254 , validate loss 8.01909351348877\n",
      "step 7400: train loss 2.100898504257202 , validate loss 7.962726593017578\n",
      "step 7500: train loss 2.1077418327331543 , validate loss 7.9476728439331055\n",
      "step 7600: train loss 2.0967929363250732 , validate loss 7.9980058670043945\n",
      "step 7700: train loss 2.0823333263397217 , validate loss 8.073896408081055\n",
      "step 7800: train loss 2.095691204071045 , validate loss 8.029348373413086\n",
      "step 7900: train loss 2.0854554176330566 , validate loss 8.050216674804688\n",
      "step 8000: train loss 2.0862619876861572 , validate loss 7.96929931640625\n",
      "我在北京的街头看到后学历会里，他在对数中年手里，间放着雨伞，不必须是好的事，可以重要讨论自由———\n",
      "step 8100: train loss 2.0796289443969727 , validate loss 7.975018501281738\n",
      "step 8200: train loss 2.0773749351501465 , validate loss 8.054388046264648\n",
      "step 8300: train loss 2.08707332611084 , validate loss 8.11174488067627\n",
      "step 8400: train loss 2.0832877159118652 , validate loss 8.216779708862305\n",
      "step 8500: train loss 2.0761373043060303 , validate loss 8.048556327819824\n",
      "step 8600: train loss 2.0632033348083496 , validate loss 8.163366317749023\n",
      "step 8700: train loss 2.0667834281921387 , validate loss 8.139163970947266\n",
      "step 8800: train loss 2.064117670059204 , validate loss 8.041352272033691\n",
      "step 8900: train loss 2.070260524749756 , validate loss 8.045299530029297\n",
      "step 9000: train loss 2.062898874282837 , validate loss 8.066606521606445\n",
      "我在北京的街头看到处的歌阶才。难免要把电影更见得到了，让我看不出。这一种电影用工作为“媚雅”焊死掉\n",
      "step 9100: train loss 2.0612919330596924 , validate loss 8.189081192016602\n",
      "step 9200: train loss 2.050363063812256 , validate loss 8.146930694580078\n",
      "step 9300: train loss 2.0684025287628174 , validate loss 8.138650894165039\n",
      "step 9400: train loss 2.0559804439544678 , validate loss 8.176824569702148\n",
      "step 9500: train loss 2.049381732940674 , validate loss 8.233389854431152\n",
      "step 9600: train loss 2.0616064071655273 , validate loss 8.136800765991211\n",
      "step 9700: train loss 2.042205333709717 , validate loss 8.155355453491211\n",
      "step 9800: train loss 2.0477168560028076 , validate loss 8.23983097076416\n",
      "step 9900: train loss 2.050933599472046 , validate loss 8.151338577270508\n",
      "我在北京的街头看到这时员在茶座里坐。这个政府把美国正经成一种悲大，甚多数学会学会给予不到。从电影院\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4096\n",
    "block_size = 40\n",
    "train_steps = int(1e4)\n",
    "val_steps = train_steps / 100\n",
    "n_embd = 64\n",
    "num_heads=8\n",
    "dropout = 0.2\n",
    "num_block = 4\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    torch.set_default_device('cuda')\n",
    "    print('device: cuda')\n",
    "model = BingramLanguageModel(vocab_size,block_size,n_embd,num_heads,dropout,num_block)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(batch_size,block_size):\n",
    "    model.eval()\n",
    "    x, y = get_batch(val_data, batch_size, block_size)\n",
    "    _, loss = model(x, y)\n",
    "    model.train()\n",
    "    return loss\n",
    "\n",
    "def test_generate():\n",
    "    test_idx = torch.tensor([encode('我在北京的街头看到')], dtype=torch.long)\n",
    "    test_logits = model.generate(test_idx, max_len=block_size)\n",
    "    print(decode(test_logits[0].tolist()))\n",
    "\n",
    "for steps in range(train_steps):\n",
    "    x, y = get_batch(train_data, batch_size, block_size)\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % val_steps == 0: \n",
    "        val_loss = estimate_loss(batch_size,block_size)\n",
    "        print(f\"step {steps}: train loss {loss.item()} , validate loss {val_loss.item()}\")\n",
    "        if steps % (10 * val_steps) == 0:\n",
    "            test_generate()\n",
    "test_generate()\n",
    "torch.save(model, 'model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7831907,
     "sourceId": 12417877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9230.152298,
   "end_time": "2025-07-09T11:01:58.180165",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-09T08:28:08.027867",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
