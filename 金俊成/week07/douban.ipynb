{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1683,"sourceType":"datasetVersion","datasetId":600}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 读取数据集\nimport pandas as pd\ndf = pd.read_csv('/kaggle/input/doubanmovieshortcomments/DMSC.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T01:19:51.642630Z","iopub.execute_input":"2025-04-18T01:19:51.643806Z","iopub.status.idle":"2025-04-18T01:20:08.050028Z","shell.execute_reply.started":"2025-04-18T01:19:51.643759Z","shell.execute_reply":"2025-04-18T01:20:08.049121Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   ID           Movie_Name_EN Movie_Name_CN  Crawl_Date  Number Username  \\\n0   0  Avengers Age of Ultron        复仇者联盟2  2017-01-22       1       然潘   \n1   1  Avengers Age of Ultron        复仇者联盟2  2017-01-22       2    更深的白色   \n2   2  Avengers Age of Ultron        复仇者联盟2  2017-01-22       3   有意识的贱民   \n3   3  Avengers Age of Ultron        复仇者联盟2  2017-01-22       4  不老的李大爷耶   \n4   4  Avengers Age of Ultron        复仇者联盟2  2017-01-22       5  ZephyrO   \n\n         Date  Star                                            Comment  Like  \n0  2015-05-13     3                                      连奥创都知道整容要去韩国。  2404  \n1  2015-04-24     2   非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...  1231  \n2  2015-04-26     2   2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...  1052  \n3  2015-04-23     4   《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...  1045  \n4  2015-04-22     2                                  虽然从头打到尾，但是真的很无聊啊。   723  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Movie_Name_EN</th>\n      <th>Movie_Name_CN</th>\n      <th>Crawl_Date</th>\n      <th>Number</th>\n      <th>Username</th>\n      <th>Date</th>\n      <th>Star</th>\n      <th>Comment</th>\n      <th>Like</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>1</td>\n      <td>然潘</td>\n      <td>2015-05-13</td>\n      <td>3</td>\n      <td>连奥创都知道整容要去韩国。</td>\n      <td>2404</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>2</td>\n      <td>更深的白色</td>\n      <td>2015-04-24</td>\n      <td>2</td>\n      <td>非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...</td>\n      <td>1231</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>3</td>\n      <td>有意识的贱民</td>\n      <td>2015-04-26</td>\n      <td>2</td>\n      <td>2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...</td>\n      <td>1052</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>4</td>\n      <td>不老的李大爷耶</td>\n      <td>2015-04-23</td>\n      <td>4</td>\n      <td>《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...</td>\n      <td>1045</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>5</td>\n      <td>ZephyrO</td>\n      <td>2015-04-22</td>\n      <td>2</td>\n      <td>虽然从头打到尾，但是真的很无聊啊。</td>\n      <td>723</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import jieba\n# 预过滤\ndata = df[(df['Star'].astype(int) >= 1) & (df['Star'].astype(int) <= 5) & (df['Star'].astype(int) != 3)]\n\ndata = data[:10000]\n# 将评分数据映射为标签，1-2为消极取0，4-5为积极取1\ndata = data['Star'].apply(lambda x: 1 if x >= 4 else 0)\n# 处理评论数据\ncomments = df['Comment'].apply(lambda x: jieba.lcut(x))\n\nds_comments = list(zip(comments,data))\nds_comments[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T01:20:15.024947Z","iopub.execute_input":"2025-04-18T01:20:15.025260Z","iopub.status.idle":"2025-04-18T01:26:33.374709Z","shell.execute_reply.started":"2025-04-18T01:20:15.025221Z","shell.execute_reply":"2025-04-18T01:26:33.373736Z"}},"outputs":[{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.784 seconds.\nPrefix dict has been built successfully.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[([' ', '连', '奥创', '都', '知道', '整容', '要', '去', '韩国', '。'], 0),\n ([' ',\n   '非常',\n   '失望',\n   '，',\n   '剧本',\n   '完全',\n   '敷衍了事',\n   '，',\n   '主线',\n   '剧情',\n   '没',\n   '突破',\n   '大家',\n   '可以',\n   '理解',\n   '，',\n   '可',\n   '所有',\n   '的',\n   '人物',\n   '都',\n   '缺乏',\n   '动机',\n   '，',\n   '正邪',\n   '之间',\n   '、',\n   '妇联',\n   '内部',\n   '都',\n   '没什么',\n   '火花',\n   '。',\n   '团结',\n   '-',\n   '分裂',\n   '-',\n   '团结',\n   '的',\n   '三段式',\n   '虽然',\n   '老套',\n   '但',\n   '其实',\n   '也',\n   '可以',\n   '利用',\n   '积攒',\n   '下来',\n   '的',\n   '形象',\n   '魅力',\n   '搞',\n   '出',\n   '意思',\n   '，',\n   '但',\n   '剧本',\n   '写得',\n   '非常',\n   '肤浅',\n   '、',\n   '平面',\n   '。',\n   '场面',\n   '上',\n   '调度',\n   '混乱',\n   '呆板',\n   '，',\n   '满屏',\n   '的',\n   '铁甲',\n   '审美疲劳',\n   '。',\n   '只有',\n   '笑',\n   '点算',\n   '得',\n   '上',\n   '差强人意',\n   '。'],\n  0)]"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import pickle\n# 二进制文件保存数据\nwith open('/kaggle/working/comments.pkl', 'wb') as f:\n    pickle.dump(ds_comments, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T01:27:12.723821Z","iopub.execute_input":"2025-04-18T01:27:12.725924Z","iopub.status.idle":"2025-04-18T01:27:12.839707Z","shell.execute_reply.started":"2025-04-18T01:27:12.725870Z","shell.execute_reply":"2025-04-18T01:27:12.838565Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport pickle\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 加载训练语料\nwith open('/kaggle/working/comments.pkl','rb') as f:\n    comments_data = pickle.load(f)\nvocab = set()\nfor line in comments_data:\n    vocab.update(line[0])\nvocab = ['PAD','UNK']+list(vocab)\nw2idx = {word:idx for idx,word in enumerate(vocab)}\nlen(w2idx)\nvocab = w2idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:13:19.941083Z","iopub.execute_input":"2025-04-18T02:13:19.941507Z","iopub.status.idle":"2025-04-18T02:13:20.065737Z","shell.execute_reply.started":"2025-04-18T02:13:19.941481Z","shell.execute_reply":"2025-04-18T02:13:20.064716Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence # 长度不同张量填充为相同长度\n\n\ndef convert_data(batch_data):\n    comments,votes = [],[]\n    for comment,vote in batch_data:\n        comments.append(torch.tensor([vocab.get(word,vocab['UNK']) for word in comment]))\n        votes.append(vote)\n    commt_tensor = pad_sequence(comments, batch_first=True, padding_value=vocab['PAD'])\n    labels = torch.tensor(votes)\n    return commt_tensor,labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:16:06.040359Z","iopub.execute_input":"2025-04-18T02:16:06.040739Z","iopub.status.idle":"2025-04-18T02:16:06.067947Z","shell.execute_reply.started":"2025-04-18T02:16:06.040713Z","shell.execute_reply":"2025-04-18T02:16:06.066953Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n# 通过Dataset构建DataLoader\ndataloader = DataLoader(comments_data, batch_size=100, shuffle=True, \n                        collate_fn=convert_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:16:09.798152Z","iopub.execute_input":"2025-04-18T02:16:09.798541Z","iopub.status.idle":"2025-04-18T02:16:09.804216Z","shell.execute_reply.started":"2025-04-18T02:16:09.798515Z","shell.execute_reply":"2025-04-18T02:16:09.803230Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\n# 构建模型\n# vocab_size: 词汇表大小\n# embedding_dim: 词嵌入维度\n# hidden_size: LSTM隐藏层大小\n# num_classes: 分类数量\nvocab_size = len(vocab)\nembedding_dim = 100\nhidden_size = 128\nnum_classes = 2\n\nclass Comments_classifier(nn.Module):\n    def __init__(self,vocab_size,embedding_dim,hidden_size,num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.rnn = nn.LSTM(embedding_dim,hidden_size,batch_first=True)\n        self.fc = nn.Linear(hidden_size,num_classes)\n    def forward(self,input_ids):\n        embedded = self.embedding(input_ids)\n        output, _ = self.rnn(embedded)\n        output = self.fc(output[:,-1,:])\n        return output\nmodel = Comments_classifier(vocab_size,embedding_dim,hidden_size,num_classes)\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nepochs = 5\nfor epoch in range(epochs):\n    for i,(cmt,lbl) in enumerate(dataloader):\n        cmt = cmt.to(device)\n        lbl = lbl.to(device)\n        outputs = model(cmt)\n        loss = criterion(outputs,lbl)\n        # 反向传播和优化\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if i%100 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}],Step {i},Loss:{loss}')\n# 保存模型\ntorch.save(model.state_dict(), 'comments_classifier.pth')\n# 模型词典\ntorch.save(vocab, 'comments_vocab.pth')\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:18:04.269481Z","iopub.execute_input":"2025-04-18T02:18:04.270478Z","iopub.status.idle":"2025-04-18T02:19:27.466451Z","shell.execute_reply.started":"2025-04-18T02:18:04.270444Z","shell.execute_reply":"2025-04-18T02:19:27.465397Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5],Step 0,Loss:0.6934474110603333\nEpoch [2/5],Step 0,Loss:0.6283988356590271\nEpoch [3/5],Step 0,Loss:0.5508683919906616\nEpoch [4/5],Step 0,Loss:0.4933689832687378\nEpoch [5/5],Step 0,Loss:0.6810575127601624\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"embedding_dim = 100\nhidden_size = 128\nnum_classes = 2\n\n# 加载词典\nvocab = torch.load('comments_vocab.pth')\n# 测试模型\ncomment1 = '这部电影真好看！全程无尿点'\ncomment2 = '非常失望'\n\n# 将评论转换为索引\ncomment1_idx = torch.tensor([vocab.get(word, vocab['UNK']) for word in jieba.lcut(comment1)])\ncomment2_idx = torch.tensor([vocab.get(word, vocab['UNK']) for word in jieba.lcut(comment2)])\nprint(comment2_idx)\n# 将评论转换为tensor\ncomment1_idx = comment1_idx.unsqueeze(0).to(device)  # 添加batch维度    \ncomment2_idx = comment2_idx.unsqueeze(0).to(device)  # 添加batch维度\n\n# 加载模型\nmodel = Comments_classifier(len(vocab), embedding_dim, hidden_size, num_classes)\nmodel.load_state_dict(torch.load('comments_classifier.pth'))\nmodel.to(device)\n\n# 模型推理\npred1 = model(comment1_idx)\npred2 = model(comment2_idx)\nprint(pred1)\n\n# 取最大值的索引作为预测结果\npred1 = torch.argmax(pred1, dim=1).item()\npred2 = torch.argmax(pred2, dim=1).item()\nprint(f'评论1预测结果: {pred1}')\nprint(f'评论2预测结果: {pred2}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T02:24:24.799878Z","iopub.execute_input":"2025-04-18T02:24:24.801108Z","iopub.status.idle":"2025-04-18T02:24:24.867039Z","shell.execute_reply.started":"2025-04-18T02:24:24.801070Z","shell.execute_reply":"2025-04-18T02:24:24.865907Z"}},"outputs":[{"name":"stdout","text":"tensor([13897, 12452])\ntensor([[-0.8218,  0.9575]], grad_fn=<AddmmBackward0>)\n评论1预测结果: 1\n评论2预测结果: 1\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2725039639.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  vocab = torch.load('comments_vocab.pth')\n/tmp/ipykernel_31/2725039639.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('comments_classifier.pth'))\n","output_type":"stream"}],"execution_count":31}]}