{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile week12_ddp.py\n\nimport os\nfrom transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\nimport numpy as np\nfrom datasets import ClassLabel\nimport evaluate\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\nfrom datasets import load_dataset\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n# 设置分布式环境\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n# 清理分布式环境\ndef cleanup():\n    dist.destroy_process_group()\n\ndef train(rank, world_size):\n    setup(rank, world_size)\n    ds = load_dataset('doushabao4766/msra_ner_k_V3')\n    label_list = ds[\"train\"].features[\"ner_tags\"].feature.names  # ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n    \n    label2id = {label: i for i, label in enumerate(label_list)}\n    id2label = {i: label for i, label in enumerate(label_list)}\n    \n    # 加载预训练的BERT模型和分词器\n    # model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-chinese\")\n    model = AutoModelForTokenClassification.from_pretrained(\n        \"google-bert/bert-base-chinese\",\n        num_labels=7,      # 设置为 7\n        id2label=id2label,\n        label2id=label2id\n    )\n    model.to(rank)\n    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n    \n    \n    \n    def tokenize_and_align_labels(examples):\n        tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    \n        all_labels = []\n        for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n            labels = []\n            previous_word_idx = None\n            for word_idx in word_ids:\n                if word_idx is None:\n                    labels.append(-100)\n                elif word_idx != previous_word_idx:\n                    labels.append(examples[\"ner_tags\"][i][word_idx])\n                else:\n                    labels.append(examples[\"ner_tags\"][i][word_idx])\n                previous_word_idx = word_idx\n            all_labels.append(labels)\n    \n        tokenized_inputs[\"labels\"] = all_labels\n        return tokenized_inputs\n    \n    # 应用预处理\n    tokenized_datasets = ds.map(tokenize_and_align_labels, batched=True)\n    \n    # 训练参数\n    training_args = TrainingArguments(\n        output_dir=\"./ner_model\",\n        save_strategy=\"epoch\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=2,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=50,\n        report_to=\"none\",  # 不使用wandb等\n        local_rank=rank,   # 当前进程 RANK\n        fp16=True,               # 使用混合精度\n        lr_scheduler_type='linear',  # 动态学习率\n        warmup_steps=100,        # 预热步数\n        ddp_find_unused_parameters=False  # 优化DDP性能\n    )\n    \n    # 数据收集器\n    data_collator = DataCollatorForTokenClassification(tokenizer)\n    \n    # 评估指标\n    metric = evaluate.load(\"seqeval\")  # 加载序列标注指标\n    \n    def compute_metrics(p):\n        predictions, labels = p\n        predictions = np.argmax(predictions, axis=2)\n        true_predictions = [\n            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n        true_labels = [\n            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n        results = metric.compute(predictions=true_predictions, references=true_labels)\n        return {\n            \"precision\": results[\"overall_precision\"],\n            \"recall\": results[\"overall_recall\"],\n            \"f1\": results[\"overall_f1\"],\n            \"accuracy\": results[\"overall_accuracy\"],\n        }\n    # Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"test\"],\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    \n    # 训练\n    trainer.train()\n    \n    # 保存模型\n    trainer.save_model(\"bert-msra-ner\")\n\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:57:04.560371Z","iopub.execute_input":"2025-06-12T15:57:04.561071Z","iopub.status.idle":"2025-06-12T15:57:04.567492Z","shell.execute_reply.started":"2025-06-12T15:57:04.561042Z","shell.execute_reply":"2025-06-12T15:57:04.566779Z"}},"outputs":[{"name":"stdout","text":"Overwriting week12_ddp.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q evaluate seqeval\n!python week12_ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:57:07.540217Z","iopub.execute_input":"2025-06-12T15:57:07.540504Z","iopub.status.idle":"2025-06-12T16:46:50.256008Z","shell.execute_reply.started":"2025-06-12T15:57:07.540483Z","shell.execute_reply":"2025-06-12T16:46:50.255279Z"}},"outputs":[{"name":"stdout","text":"2025-06-12 15:57:14.105863: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749743834.129113     239 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749743834.136200     239 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-12 15:57:23.763183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749743843.785603     253 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749743843.792842     253 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-12 15:57:23.801598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749743843.824746     254 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749743843.831659     254 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nMap: 100%|█████████████████████████| 3443/3443 [00:01<00:00, 2502.16 examples/s]\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\nMap: 100%|█████████████████████████| 3443/3443 [00:01<00:00, 2484.90 examples/s]\ntorch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n/kaggle/working/week12_ddp.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/kaggle/working/week12_ddp.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n  0%|                                                  | 0/2814 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.9852, 'grad_norm': 99911.7890625, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}\n{'loss': 1.1436, 'grad_norm': 92107.3671875, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}\n{'loss': 0.1609, 'grad_norm': 93943.828125, 'learning_rate': 1.98e-05, 'epoch': 0.07}\n{'loss': 0.1593, 'grad_norm': 76129.234375, 'learning_rate': 1.98e-05, 'epoch': 0.07}\n{'loss': 0.0685, 'grad_norm': 84344.2890625, 'learning_rate': 1.963890935887988e-05, 'epoch': 0.11}\n{'loss': 0.067, 'grad_norm': 108090.5625, 'learning_rate': 1.963890935887988e-05, 'epoch': 0.11}\n{'loss': 0.0514, 'grad_norm': 202745.015625, 'learning_rate': 1.9270449521002212e-05, 'epoch': 0.14}\n{'loss': 0.0502, 'grad_norm': 194369.015625, 'learning_rate': 1.9270449521002212e-05, 'epoch': 0.14}\n{'loss': 0.0449, 'grad_norm': 90250.6171875, 'learning_rate': 1.8901989683124543e-05, 'epoch': 0.18}\n{'loss': 0.0432, 'grad_norm': 92980.546875, 'learning_rate': 1.8901989683124543e-05, 'epoch': 0.18}\n{'loss': 0.0405, 'grad_norm': 91829.65625, 'learning_rate': 1.853352984524687e-05, 'epoch': 0.21}\n{'loss': 0.0414, 'grad_norm': 106730.0703125, 'learning_rate': 1.853352984524687e-05, 'epoch': 0.21}\n{'loss': 0.0402, 'grad_norm': 63525.35546875, 'learning_rate': 1.8165070007369198e-05, 'epoch': 0.25}\n{'loss': 0.0415, 'grad_norm': 32324.13671875, 'learning_rate': 1.8165070007369198e-05, 'epoch': 0.25}\n{'loss': 0.0413, 'grad_norm': 129174.71875, 'learning_rate': 1.7796610169491526e-05, 'epoch': 0.28}\n{'loss': 0.0424, 'grad_norm': 94317.8984375, 'learning_rate': 1.7796610169491526e-05, 'epoch': 0.28}\n{'loss': 0.0374, 'grad_norm': 164804.65625, 'learning_rate': 1.7428150331613856e-05, 'epoch': 0.32}\n{'loss': 0.0373, 'grad_norm': 117619.2734375, 'learning_rate': 1.7428150331613856e-05, 'epoch': 0.32}\n{'loss': 0.0332, 'grad_norm': 34465.953125, 'learning_rate': 1.7059690493736184e-05, 'epoch': 0.36}\n{'loss': 0.0324, 'grad_norm': 42024.140625, 'learning_rate': 1.7059690493736184e-05, 'epoch': 0.36}\n{'loss': 0.0326, 'grad_norm': 62288.57421875, 'learning_rate': 1.669123065585851e-05, 'epoch': 0.39}\n{'loss': 0.0317, 'grad_norm': 91242.390625, 'learning_rate': 1.669123065585851e-05, 'epoch': 0.39}\n{'loss': 0.0251, 'grad_norm': 71772.15625, 'learning_rate': 1.6322770817980842e-05, 'epoch': 0.43}\n{'loss': 0.0242, 'grad_norm': 84443.6328125, 'learning_rate': 1.6322770817980842e-05, 'epoch': 0.43}\n{'loss': 0.0305, 'grad_norm': 50774.91015625, 'learning_rate': 1.595431098010317e-05, 'epoch': 0.46}\n{'loss': 0.0293, 'grad_norm': 56324.88671875, 'learning_rate': 1.595431098010317e-05, 'epoch': 0.46}\n{'loss': 0.0271, 'grad_norm': 16130.5498046875, 'learning_rate': 1.5585851142225497e-05, 'epoch': 0.5}\n{'loss': 0.0267, 'grad_norm': 17662.068359375, 'learning_rate': 1.5585851142225497e-05, 'epoch': 0.5}\n{'loss': 0.0288, 'grad_norm': 50441.94921875, 'learning_rate': 1.5217391304347828e-05, 'epoch': 0.53}\n{'loss': 0.0299, 'grad_norm': 38914.625, 'learning_rate': 1.5217391304347828e-05, 'epoch': 0.53}\n{'loss': 0.0245, 'grad_norm': 66997.6953125, 'learning_rate': 1.4848931466470156e-05, 'epoch': 0.57}\n{'loss': 0.0223, 'grad_norm': 74187.1953125, 'learning_rate': 1.4848931466470156e-05, 'epoch': 0.57}\n{'loss': 0.0234, 'grad_norm': 64446.828125, 'learning_rate': 1.4480471628592485e-05, 'epoch': 0.6}\n{'loss': 0.0228, 'grad_norm': 75035.5, 'learning_rate': 1.4480471628592485e-05, 'epoch': 0.6}\n{'loss': 0.0278, 'grad_norm': 38062.24609375, 'learning_rate': 1.4112011790714812e-05, 'epoch': 0.64}\n{'loss': 0.0269, 'grad_norm': 57564.87109375, 'learning_rate': 1.4112011790714812e-05, 'epoch': 0.64}\n{'loss': 0.0207, 'grad_norm': 43213.08203125, 'learning_rate': 1.3743551952837142e-05, 'epoch': 0.68}\n{'loss': 0.0197, 'grad_norm': 50003.484375, 'learning_rate': 1.3743551952837142e-05, 'epoch': 0.68}\n{'loss': 0.024, 'grad_norm': 28341.625, 'learning_rate': 1.337509211495947e-05, 'epoch': 0.71}\n{'loss': 0.0236, 'grad_norm': 33666.5546875, 'learning_rate': 1.337509211495947e-05, 'epoch': 0.71}\n{'loss': 0.0272, 'grad_norm': 54999.4609375, 'learning_rate': 1.3006632277081798e-05, 'epoch': 0.75}\n{'loss': 0.0277, 'grad_norm': 43695.16796875, 'learning_rate': 1.3006632277081798e-05, 'epoch': 0.75}\n{'loss': 0.0217, 'grad_norm': 37804.6484375, 'learning_rate': 1.2638172439204129e-05, 'epoch': 0.78}\n{'loss': 0.0223, 'grad_norm': 56302.55078125, 'learning_rate': 1.2638172439204129e-05, 'epoch': 0.78}\n{'loss': 0.0264, 'grad_norm': 83117.0234375, 'learning_rate': 1.2269712601326455e-05, 'epoch': 0.82}\n{'loss': 0.0274, 'grad_norm': 99659.0078125, 'learning_rate': 1.2269712601326455e-05, 'epoch': 0.82}\n{'loss': 0.0259, 'grad_norm': 49462.34375, 'learning_rate': 1.1901252763448786e-05, 'epoch': 0.85}\n{'loss': 0.0248, 'grad_norm': 42137.59765625, 'learning_rate': 1.1901252763448786e-05, 'epoch': 0.85}\n{'loss': 0.0237, 'grad_norm': 50734.921875, 'learning_rate': 1.1532792925571115e-05, 'epoch': 0.89}\n{'loss': 0.024, 'grad_norm': 64508.71484375, 'learning_rate': 1.1532792925571115e-05, 'epoch': 0.89}\n{'loss': 0.0188, 'grad_norm': 97607.734375, 'learning_rate': 1.1164333087693442e-05, 'epoch': 0.92}\n{'loss': 0.0191, 'grad_norm': 89406.0625, 'learning_rate': 1.1164333087693442e-05, 'epoch': 0.92}\n{'loss': 0.0211, 'grad_norm': 6580.79833984375, 'learning_rate': 1.0795873249815772e-05, 'epoch': 0.96}\n{'loss': 0.0207, 'grad_norm': 30796.138671875, 'learning_rate': 1.0795873249815772e-05, 'epoch': 0.96}\n{'loss': 0.0232, 'grad_norm': 82035.0234375, 'learning_rate': 1.0427413411938099e-05, 'epoch': 1.0}\n{'loss': 0.0228, 'grad_norm': 74983.484375, 'learning_rate': 1.0427413411938099e-05, 'epoch': 1.0}\n 50%|███████████████████▌                   | 1407/2814 [24:36<11:44,  2.00it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n 50%|███████████████████▌                   | 1411/2814 [24:39<16:53,  1.38it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.0152, 'grad_norm': 27007.576171875, 'learning_rate': 1.0058953574060428e-05, 'epoch': 1.03}\n{'loss': 0.0149, 'grad_norm': 53068.73046875, 'learning_rate': 1.0058953574060428e-05, 'epoch': 1.03}\n{'loss': 0.0125, 'grad_norm': 34025.44140625, 'learning_rate': 9.690493736182757e-06, 'epoch': 1.07}\n{'loss': 0.0127, 'grad_norm': 16918.796875, 'learning_rate': 9.690493736182757e-06, 'epoch': 1.07}\n{'loss': 0.0158, 'grad_norm': 36321.39453125, 'learning_rate': 9.322033898305085e-06, 'epoch': 1.1}\n{'loss': 0.0143, 'grad_norm': 20763.703125, 'learning_rate': 9.322033898305085e-06, 'epoch': 1.1}\n{'loss': 0.0143, 'grad_norm': 19860.97265625, 'learning_rate': 8.953574060427414e-06, 'epoch': 1.14}\n{'loss': 0.0134, 'grad_norm': 21839.396484375, 'learning_rate': 8.953574060427414e-06, 'epoch': 1.14}\n{'loss': 0.0133, 'grad_norm': 117994.375, 'learning_rate': 8.585114222549743e-06, 'epoch': 1.17}\n{'loss': 0.0133, 'grad_norm': 97541.125, 'learning_rate': 8.585114222549743e-06, 'epoch': 1.17}\n{'loss': 0.0143, 'grad_norm': 5255.990234375, 'learning_rate': 8.21665438467207e-06, 'epoch': 1.21}\n{'loss': 0.0142, 'grad_norm': 3989.5869140625, 'learning_rate': 8.21665438467207e-06, 'epoch': 1.21}\n{'loss': 0.0159, 'grad_norm': 16977.154296875, 'learning_rate': 7.8481945467944e-06, 'epoch': 1.24}\n{'loss': 0.016, 'grad_norm': 9637.021484375, 'learning_rate': 7.8481945467944e-06, 'epoch': 1.24}\n{'loss': 0.0111, 'grad_norm': 64127.3515625, 'learning_rate': 7.479734708916728e-06, 'epoch': 1.28}\n{'loss': 0.0114, 'grad_norm': 62469.45703125, 'learning_rate': 7.479734708916728e-06, 'epoch': 1.28}\n{'loss': 0.012, 'grad_norm': 46819.77734375, 'learning_rate': 7.1112748710390575e-06, 'epoch': 1.31}\n{'loss': 0.0112, 'grad_norm': 41743.50390625, 'learning_rate': 7.1112748710390575e-06, 'epoch': 1.31}\n{'loss': 0.0138, 'grad_norm': 120489.1796875, 'learning_rate': 6.742815033161386e-06, 'epoch': 1.35}\n{'loss': 0.0136, 'grad_norm': 103139.4375, 'learning_rate': 6.742815033161386e-06, 'epoch': 1.35}\n{'loss': 0.0156, 'grad_norm': 4754.55908203125, 'learning_rate': 6.374355195283714e-06, 'epoch': 1.39}\n{'loss': 0.0166, 'grad_norm': 2720.9296875, 'learning_rate': 6.374355195283714e-06, 'epoch': 1.39}\n{'loss': 0.0129, 'grad_norm': 101094.203125, 'learning_rate': 6.005895357406043e-06, 'epoch': 1.42}\n{'loss': 0.0133, 'grad_norm': 99712.125, 'learning_rate': 6.005895357406043e-06, 'epoch': 1.42}\n{'loss': 0.015, 'grad_norm': 54096.2421875, 'learning_rate': 5.637435519528372e-06, 'epoch': 1.46}\n{'loss': 0.0141, 'grad_norm': 50142.625, 'learning_rate': 5.637435519528372e-06, 'epoch': 1.46}\n{'loss': 0.0126, 'grad_norm': 117554.40625, 'learning_rate': 5.268975681650701e-06, 'epoch': 1.49}\n{'loss': 0.0132, 'grad_norm': 99173.7421875, 'learning_rate': 5.268975681650701e-06, 'epoch': 1.49}\n{'loss': 0.0107, 'grad_norm': 79729.4140625, 'learning_rate': 4.900515843773029e-06, 'epoch': 1.53}\n{'loss': 0.0106, 'grad_norm': 59505.89453125, 'learning_rate': 4.900515843773029e-06, 'epoch': 1.53}\n{'loss': 0.0103, 'grad_norm': 32188.921875, 'learning_rate': 4.532056005895358e-06, 'epoch': 1.56}\n{'loss': 0.0107, 'grad_norm': 5920.23388671875, 'learning_rate': 4.532056005895358e-06, 'epoch': 1.56}\n{'loss': 0.0113, 'grad_norm': 88163.921875, 'learning_rate': 4.163596168017686e-06, 'epoch': 1.6}\n{'loss': 0.0109, 'grad_norm': 76974.890625, 'learning_rate': 4.163596168017686e-06, 'epoch': 1.6}\n{'loss': 0.0111, 'grad_norm': 50143.26171875, 'learning_rate': 3.795136330140015e-06, 'epoch': 1.63}\n{'loss': 0.0105, 'grad_norm': 61488.24609375, 'learning_rate': 3.795136330140015e-06, 'epoch': 1.63}\n{'loss': 0.0114, 'grad_norm': 11645.8759765625, 'learning_rate': 3.4266764922623435e-06, 'epoch': 1.67}\n{'loss': 0.0123, 'grad_norm': 22812.353515625, 'learning_rate': 3.4266764922623435e-06, 'epoch': 1.67}\n{'loss': 0.0091, 'grad_norm': 23498.8828125, 'learning_rate': 3.0582166543846727e-06, 'epoch': 1.71}\n{'loss': 0.0086, 'grad_norm': 75475.21875, 'learning_rate': 3.0582166543846727e-06, 'epoch': 1.71}\n{'loss': 0.0136, 'grad_norm': 80704.5546875, 'learning_rate': 2.689756816507001e-06, 'epoch': 1.74}\n{'loss': 0.0127, 'grad_norm': 203444.140625, 'learning_rate': 2.689756816507001e-06, 'epoch': 1.74}\n{'loss': 0.0162, 'grad_norm': 24576.822265625, 'learning_rate': 2.3212969786293298e-06, 'epoch': 1.78}\n{'loss': 0.0163, 'grad_norm': 26352.880859375, 'learning_rate': 2.3212969786293298e-06, 'epoch': 1.78}\n{'loss': 0.0122, 'grad_norm': 18206.55859375, 'learning_rate': 1.952837140751658e-06, 'epoch': 1.81}\n{'loss': 0.0121, 'grad_norm': 19533.697265625, 'learning_rate': 1.952837140751658e-06, 'epoch': 1.81}\n{'loss': 0.0125, 'grad_norm': 55089.546875, 'learning_rate': 1.584377302873987e-06, 'epoch': 1.85}\n{'loss': 0.0144, 'grad_norm': 72081.2734375, 'learning_rate': 1.584377302873987e-06, 'epoch': 1.85}\n{'loss': 0.0115, 'grad_norm': 37910.77734375, 'learning_rate': 1.2159174649963156e-06, 'epoch': 1.88}\n{'loss': 0.0128, 'grad_norm': 45418.5703125, 'learning_rate': 1.2159174649963156e-06, 'epoch': 1.88}\n{'loss': 0.0128, 'grad_norm': 69240.6953125, 'learning_rate': 8.474576271186441e-07, 'epoch': 1.92}\n{'loss': 0.0135, 'grad_norm': 83921.8046875, 'learning_rate': 8.474576271186441e-07, 'epoch': 1.92}\n{'loss': 0.0117, 'grad_norm': 8493.64453125, 'learning_rate': 4.789977892409728e-07, 'epoch': 1.95}\n{'loss': 0.0107, 'grad_norm': 12230.41015625, 'learning_rate': 4.789977892409728e-07, 'epoch': 1.95}\n{'loss': 0.013, 'grad_norm': 58584.58203125, 'learning_rate': 1.105379513633014e-07, 'epoch': 1.99}\n{'loss': 0.0129, 'grad_norm': 56075.69921875, 'learning_rate': 1.105379513633014e-07, 'epoch': 1.99}\n{'train_runtime': 2949.7514, 'train_samples_per_second': 30.512, 'train_steps_per_second': 0.954, 'train_loss': 0.04122118386833822, 'epoch': 2.0}\n100%|███████████████████████████████████████| 2814/2814 [49:09<00:00,  1.05s/it]\n{'train_runtime': 2952.1233, 'train_samples_per_second': 30.487, 'train_steps_per_second': 0.953, 'train_loss': 0.04384835463152257, 'epoch': 2.0}\n100%|███████████████████████████████████████| 2814/2814 [49:12<00:00,  1.05s/it]\n[rank0]:[W612 16:46:47.917715158 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification  # 或其他模型类\nfrom datasets import load_dataset\n\nds = load_dataset('doushabao4766/msra_ner_k_V3')\nlabel_list = ds[\"train\"].features[\"ner_tags\"].feature.names  # ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for i, label in enumerate(label_list)}\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/ner_model/checkpoint-1407\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"/kaggle/working/ner_model/checkpoint-1407\")\n\ndef ner_inference(text: str):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    model.to(device)\n\n    words = list(text)\n\n    inputs = tokenizer(\n        words,\n        is_split_into_words=True,\n        return_tensors=\"pt\",\n        truncation=True\n    )\n    word_ids = inputs.word_ids()  # 先拿 word_ids\n    inputs = {k: v.to(device) for k, v in inputs.items()}  # 转设备\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n    pred_ids = outputs.logits.argmax(dim=-1)[0].tolist()\n\n    entities = []\n    cur_entity = []\n    cur_type = None\n\n    for idx, word_idx in enumerate(word_ids):\n        if word_idx is None:\n            continue\n\n    \n        label = id2label[pred_ids[idx]]\n        prefix, ent_type = (label.split(\"-\", 1) + [None])[:2]\n\n        if prefix == \"B\":\n            if cur_entity:\n                entities.append({\"entity\": cur_type, \"content\": \"\".join(cur_entity)})\n            cur_entity = [words[word_idx]]\n            cur_type = ent_type\n\n        elif prefix == \"I\" and cur_type == ent_type:\n            cur_entity.append(words[word_idx])\n\n        else:\n            if cur_entity:\n                entities.append({\"entity\": cur_type, \"content\": \"\".join(cur_entity)})\n                cur_entity = []\n                cur_type = None\n\n    if cur_entity:\n        entities.append({\"entity\": cur_type, \"content\": \"\".join(cur_entity)})\n\n    return entities\n\n# —— 测试一下 —— \ntext = \"双方确定了今后发展中美关系的指导方针。\"\nprint(ner_inference(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:54:55.105892Z","iopub.execute_input":"2025-06-12T16:54:55.106572Z","iopub.status.idle":"2025-06-12T16:54:57.406944Z","shell.execute_reply.started":"2025-06-12T16:54:55.106547Z","shell.execute_reply":"2025-06-12T16:54:57.406153Z"}},"outputs":[{"name":"stdout","text":"[{'entity': 'LOC', 'content': '中'}, {'entity': 'LOC', 'content': '美'}]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}