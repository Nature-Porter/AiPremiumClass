{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1683,"sourceType":"datasetVersion","datasetId":600}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:26:29.319295Z","iopub.execute_input":"2025-04-17T12:26:29.319464Z","iopub.status.idle":"2025-04-17T12:26:30.392903Z","shell.execute_reply.started":"2025-04-17T12:26:29.319448Z","shell.execute_reply":"2025-04-17T12:26:30.392031Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/doubanmovieshortcomments/DMSC.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 1 文本预处理","metadata":{}},{"cell_type":"code","source":"comments = pd.read_csv('/kaggle/input/doubanmovieshortcomments/DMSC.csv')\ncomments.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:26:34.071927Z","iopub.execute_input":"2025-04-17T12:26:34.072604Z","iopub.status.idle":"2025-04-17T12:26:48.405868Z","shell.execute_reply.started":"2025-04-17T12:26:34.072550Z","shell.execute_reply":"2025-04-17T12:26:48.405193Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   ID           Movie_Name_EN Movie_Name_CN  Crawl_Date  Number Username  \\\n0   0  Avengers Age of Ultron        复仇者联盟2  2017-01-22       1       然潘   \n1   1  Avengers Age of Ultron        复仇者联盟2  2017-01-22       2    更深的白色   \n2   2  Avengers Age of Ultron        复仇者联盟2  2017-01-22       3   有意识的贱民   \n3   3  Avengers Age of Ultron        复仇者联盟2  2017-01-22       4  不老的李大爷耶   \n4   4  Avengers Age of Ultron        复仇者联盟2  2017-01-22       5  ZephyrO   \n\n         Date  Star                                            Comment  Like  \n0  2015-05-13     3                                      连奥创都知道整容要去韩国。  2404  \n1  2015-04-24     2   非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...  1231  \n2  2015-04-26     2   2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...  1052  \n3  2015-04-23     4   《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...  1045  \n4  2015-04-22     2                                  虽然从头打到尾，但是真的很无聊啊。   723  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Movie_Name_EN</th>\n      <th>Movie_Name_CN</th>\n      <th>Crawl_Date</th>\n      <th>Number</th>\n      <th>Username</th>\n      <th>Date</th>\n      <th>Star</th>\n      <th>Comment</th>\n      <th>Like</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>1</td>\n      <td>然潘</td>\n      <td>2015-05-13</td>\n      <td>3</td>\n      <td>连奥创都知道整容要去韩国。</td>\n      <td>2404</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>2</td>\n      <td>更深的白色</td>\n      <td>2015-04-24</td>\n      <td>2</td>\n      <td>非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...</td>\n      <td>1231</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>3</td>\n      <td>有意识的贱民</td>\n      <td>2015-04-26</td>\n      <td>2</td>\n      <td>2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...</td>\n      <td>1052</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>4</td>\n      <td>不老的李大爷耶</td>\n      <td>2015-04-23</td>\n      <td>4</td>\n      <td>《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...</td>\n      <td>1045</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Avengers Age of Ultron</td>\n      <td>复仇者联盟2</td>\n      <td>2017-01-22</td>\n      <td>5</td>\n      <td>ZephyrO</td>\n      <td>2015-04-22</td>\n      <td>2</td>\n      <td>虽然从头打到尾，但是真的很无聊啊。</td>\n      <td>723</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"comments.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:26:48.407150Z","iopub.execute_input":"2025-04-17T12:26:48.407390Z","iopub.status.idle":"2025-04-17T12:26:48.596498Z","shell.execute_reply.started":"2025-04-17T12:26:48.407371Z","shell.execute_reply":"2025-04-17T12:26:48.595854Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                 ID        Number          Star          Like\ncount  2.125056e+06  2.125056e+06  2.125056e+06  2.125056e+06\nmean   1.062528e+06  4.609775e+04  3.638320e+00  1.078081e+00\nstd    6.134510e+05  3.191518e+04  1.240807e+00  5.436271e+01\nmin    0.000000e+00  1.000000e+00  1.000000e+00  0.000000e+00\n25%    5.312638e+05  1.964100e+04  3.000000e+00  0.000000e+00\n50%    1.062528e+06  4.074900e+04  4.000000e+00  0.000000e+00\n75%    1.593791e+06  6.824200e+04  5.000000e+00  0.000000e+00\nmax    2.125055e+06  1.412000e+05  5.000000e+00  1.549900e+04","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Number</th>\n      <th>Star</th>\n      <th>Like</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.125056e+06</td>\n      <td>2.125056e+06</td>\n      <td>2.125056e+06</td>\n      <td>2.125056e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.062528e+06</td>\n      <td>4.609775e+04</td>\n      <td>3.638320e+00</td>\n      <td>1.078081e+00</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>6.134510e+05</td>\n      <td>3.191518e+04</td>\n      <td>1.240807e+00</td>\n      <td>5.436271e+01</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.312638e+05</td>\n      <td>1.964100e+04</td>\n      <td>3.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.062528e+06</td>\n      <td>4.074900e+04</td>\n      <td>4.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.593791e+06</td>\n      <td>6.824200e+04</td>\n      <td>5.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.125055e+06</td>\n      <td>1.412000e+05</td>\n      <td>5.000000e+00</td>\n      <td>1.549900e+04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"comments['label'] = np.where(comments['Star']<3,0,1)\ncomments_select = comments.loc[comments['Star']!=3,['label','Comment']].copy()\ncomments_select.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:26:48.597160Z","iopub.execute_input":"2025-04-17T12:26:48.597366Z","iopub.status.idle":"2025-04-17T12:26:48.825930Z","shell.execute_reply.started":"2025-04-17T12:26:48.597352Z","shell.execute_reply":"2025-04-17T12:26:48.825056Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   label                                            Comment\n1      0   非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...\n2      0   2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...\n3      1   《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...\n4      0                                  虽然从头打到尾，但是真的很无聊啊。\n6      0   只有一颗彩蛋必须降一星。外加漫威的编剧是有心无力了吧。复仇者联盟只能永远着手与团队的和与不...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>Comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>虽然从头打到尾，但是真的很无聊啊。</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>只有一颗彩蛋必须降一星。外加漫威的编剧是有心无力了吧。复仇者联盟只能永远着手与团队的和与不...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# 2 构建词典并保存","metadata":{}},{"cell_type":"code","source":"import jieba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:26:48.827251Z","iopub.execute_input":"2025-04-17T12:26:48.827451Z","iopub.status.idle":"2025-04-17T12:26:49.068116Z","shell.execute_reply.started":"2025-04-17T12:26:48.827434Z","shell.execute_reply":"2025-04-17T12:26:49.067604Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 构建词典\nvocab = set()\nfor idx,row in comments_select.iterrows():\n    words = jieba.cut(row['Comment'])\n    for word in words:\n        vocab.add(word)\nprint(len(vocab))        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:26:49.068735Z","iopub.execute_input":"2025-04-17T12:26:49.068976Z","iopub.status.idle":"2025-04-17T12:32:09.099858Z","shell.execute_reply.started":"2025-04-17T12:26:49.068955Z","shell.execute_reply":"2025-04-17T12:32:09.099103Z"}},"outputs":[{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.623 seconds.\nPrefix dict has been built successfully.\n","output_type":"stream"},{"name":"stdout","text":"287578\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"vocab_add =  ['PAD','UNK'] + list(vocab)  # PAD: padding, UNK: unknown\nw2idx = {word: idx for idx, word in enumerate(vocab_add)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:16:19.302413Z","iopub.execute_input":"2025-04-17T13:16:19.303162Z","iopub.status.idle":"2025-04-17T13:16:19.434212Z","shell.execute_reply.started":"2025-04-17T13:16:19.303131Z","shell.execute_reply":"2025-04-17T13:16:19.433400Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import json\nwith open(\"/kaggle/working/w2idx.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(w2idx, f, ensure_ascii=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:16:31.795083Z","iopub.execute_input":"2025-04-17T13:16:31.795340Z","iopub.status.idle":"2025-04-17T13:16:32.045406Z","shell.execute_reply.started":"2025-04-17T13:16:31.795320Z","shell.execute_reply":"2025-04-17T13:16:32.044676Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"#  3加载词典","metadata":{}},{"cell_type":"code","source":"import json\n# 从 JSON 文件加载字典\nwith open(\"/kaggle/working/w2idx.json\", \"r\", encoding=\"utf-8\") as f:\n    w2idx = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:16:33.777434Z","iopub.execute_input":"2025-04-17T13:16:33.778073Z","iopub.status.idle":"2025-04-17T13:16:33.939206Z","shell.execute_reply.started":"2025-04-17T13:16:33.778049Z","shell.execute_reply":"2025-04-17T13:16:33.938687Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# 4 模型训练 评估 测试","metadata":{"execution":{"iopub.status.busy":"2025-04-16T13:13:30.344975Z","iopub.execute_input":"2025-04-16T13:13:30.345641Z","iopub.status.idle":"2025-04-16T13:13:30.360603Z","shell.execute_reply.started":"2025-04-16T13:13:30.345618Z","shell.execute_reply":"2025-04-16T13:13:30.359958Z"}}},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence \nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:16:36.106454Z","iopub.execute_input":"2025-04-17T13:16:36.107334Z","iopub.status.idle":"2025-04-17T13:16:36.110740Z","shell.execute_reply.started":"2025-04-17T13:16:36.107310Z","shell.execute_reply":"2025-04-17T13:16:36.110118Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def convert_data(batch_data):\n    comments, votes = [],[]\n    for comment, vote in batch_data:\n        tokenized = jieba.lcut(comment) if isinstance(comment, str) else comment\n        comments.append(torch.tensor([w2idx.get(word, w2idx['UNK']) for word in tokenized]))\n        votes.append(vote)\n    \n    commt = pad_sequence(comments, batch_first=True, padding_value=w2idx['PAD'])\n    labels = torch.tensor(votes)\n    return commt, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:16:39.327113Z","iopub.execute_input":"2025-04-17T13:16:39.327444Z","iopub.status.idle":"2025-04-17T13:16:39.332767Z","shell.execute_reply.started":"2025-04-17T13:16:39.327414Z","shell.execute_reply":"2025-04-17T13:16:39.331990Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"processed_data = [(row['Comment'], row['label'])  # 确保列名对应\n    for _, row in comments_select.iterrows()]\n\ntrain_data, test_data = train_test_split(\n    processed_data, \n    test_size=0.2,\n    random_state=42  # 固定随机种子保证可复现性\n)\n\ntrain_dataloader = DataLoader(train_data, \n                            batch_size=256, \n                            shuffle=True, \n                            collate_fn=convert_data)\n\ntest_dataloader = DataLoader(test_data,\n                           batch_size=256,\n                           shuffle=False,\n                           collate_fn=convert_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:32:09.709023Z","iopub.execute_input":"2025-04-17T13:32:09.709283Z","iopub.status.idle":"2025-04-17T13:33:08.957445Z","shell.execute_reply.started":"2025-04-17T13:32:09.709263Z","shell.execute_reply":"2025-04-17T13:33:08.956899Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"class Comments_Classifier(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # padding_idx=0\n        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, input_ids):\n        # input_ids: (batch_size, seq_len)\n        # embedded: (batch_size, seq_len, embedding_dim)\n        embedded = self.embedding(input_ids)\n        # output: (batch_size, seq_len, hidden_size)\n        output, (hidden, _) = self.rnn(embedded)\n        output = self.fc(output[:, -1, :])  # 取最后一个时间步的输出\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:17:40.080377Z","iopub.execute_input":"2025-04-17T13:17:40.081201Z","iopub.status.idle":"2025-04-17T13:17:40.085918Z","shell.execute_reply.started":"2025-04-17T13:17:40.081180Z","shell.execute_reply":"2025-04-17T13:17:40.085328Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"#模型训练\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Comments_Classifier(len(w2idx), 100, 128, 2).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# 训练循环\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    for i, (cmt, lbl) in enumerate(train_dataloader):\n        cmt, lbl = cmt.to(device), lbl.to(device)\n        \n        # 前向传播\n        outputs = model(cmt)\n        loss = criterion(outputs, lbl)\n        \n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # 每10个batch打印进度\n        if (i+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T13:33:19.940799Z","iopub.execute_input":"2025-04-17T13:33:19.941521Z","iopub.status.idle":"2025-04-17T13:53:17.050520Z","shell.execute_reply.started":"2025-04-17T13:33:19.941481Z","shell.execute_reply":"2025-04-17T13:53:17.049502Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch [1/5], Step [10/5158], Loss: 0.5490\nEpoch [1/5], Step [20/5158], Loss: 0.5215\nEpoch [1/5], Step [30/5158], Loss: 0.6436\nEpoch [1/5], Step [40/5158], Loss: 0.5982\nEpoch [1/5], Step [50/5158], Loss: 0.5510\nEpoch [1/5], Step [60/5158], Loss: 0.4371\nEpoch [1/5], Step [70/5158], Loss: 0.6484\nEpoch [1/5], Step [80/5158], Loss: 0.5547\nEpoch [1/5], Step [90/5158], Loss: 0.5368\nEpoch [1/5], Step [100/5158], Loss: 0.5455\nEpoch [1/5], Step [110/5158], Loss: 0.5091\nEpoch [1/5], Step [120/5158], Loss: 0.5340\nEpoch [1/5], Step [130/5158], Loss: 0.5022\nEpoch [1/5], Step [140/5158], Loss: 0.5577\nEpoch [1/5], Step [150/5158], Loss: 0.4969\nEpoch [1/5], Step [160/5158], Loss: 0.5160\nEpoch [1/5], Step [170/5158], Loss: 0.5666\nEpoch [1/5], Step [180/5158], Loss: 0.5545\nEpoch [1/5], Step [190/5158], Loss: 0.5317\nEpoch [1/5], Step [200/5158], Loss: 0.5556\nEpoch [1/5], Step [210/5158], Loss: 0.5413\nEpoch [1/5], Step [220/5158], Loss: 0.5181\nEpoch [1/5], Step [230/5158], Loss: 0.5642\nEpoch [1/5], Step [240/5158], Loss: 0.5108\nEpoch [1/5], Step [250/5158], Loss: 0.5160\nEpoch [1/5], Step [260/5158], Loss: 0.5069\nEpoch [1/5], Step [270/5158], Loss: 0.5456\nEpoch [1/5], Step [280/5158], Loss: 0.5824\nEpoch [1/5], Step [290/5158], Loss: 0.5306\nEpoch [1/5], Step [300/5158], Loss: 0.5266\nEpoch [1/5], Step [310/5158], Loss: 0.4624\nEpoch [1/5], Step [320/5158], Loss: 0.5551\nEpoch [1/5], Step [330/5158], Loss: 0.5618\nEpoch [1/5], Step [340/5158], Loss: 0.4965\nEpoch [1/5], Step [350/5158], Loss: 0.4859\nEpoch [1/5], Step [360/5158], Loss: 0.4551\nEpoch [1/5], Step [370/5158], Loss: 0.5447\nEpoch [1/5], Step [380/5158], Loss: 0.5497\nEpoch [1/5], Step [390/5158], Loss: 0.5558\nEpoch [1/5], Step [400/5158], Loss: 0.5525\nEpoch [1/5], Step [410/5158], Loss: 0.6051\nEpoch [1/5], Step [420/5158], Loss: 0.5544\nEpoch [1/5], Step [430/5158], Loss: 0.5705\nEpoch [1/5], Step [440/5158], Loss: 0.5263\nEpoch [1/5], Step [450/5158], Loss: 0.5435\nEpoch [1/5], Step [460/5158], Loss: 0.5346\nEpoch [1/5], Step [470/5158], Loss: 0.5241\nEpoch [1/5], Step [480/5158], Loss: 0.5080\nEpoch [1/5], Step [490/5158], Loss: 0.5500\nEpoch [1/5], Step [500/5158], Loss: 0.5408\nEpoch [1/5], Step [510/5158], Loss: 0.5447\nEpoch [1/5], Step [520/5158], Loss: 0.5056\nEpoch [1/5], Step [530/5158], Loss: 0.5255\nEpoch [1/5], Step [540/5158], Loss: 0.5084\nEpoch [1/5], Step [550/5158], Loss: 0.5401\nEpoch [1/5], Step [560/5158], Loss: 0.5907\nEpoch [1/5], Step [570/5158], Loss: 0.4904\nEpoch [1/5], Step [580/5158], Loss: 0.4959\nEpoch [1/5], Step [590/5158], Loss: 0.5540\nEpoch [1/5], Step [600/5158], Loss: 0.5119\nEpoch [1/5], Step [610/5158], Loss: 0.5720\nEpoch [1/5], Step [620/5158], Loss: 0.5783\nEpoch [1/5], Step [630/5158], Loss: 0.5444\nEpoch [1/5], Step [640/5158], Loss: 0.5651\nEpoch [1/5], Step [650/5158], Loss: 0.5402\nEpoch [1/5], Step [660/5158], Loss: 0.5454\nEpoch [1/5], Step [670/5158], Loss: 0.5000\nEpoch [1/5], Step [680/5158], Loss: 0.5003\nEpoch [1/5], Step [690/5158], Loss: 0.5667\nEpoch [1/5], Step [700/5158], Loss: 0.5401\nEpoch [1/5], Step [710/5158], Loss: 0.5503\nEpoch [1/5], Step [720/5158], Loss: 0.5309\nEpoch [1/5], Step [730/5158], Loss: 0.5878\nEpoch [1/5], Step [740/5158], Loss: 0.5204\nEpoch [1/5], Step [750/5158], Loss: 0.5495\nEpoch [1/5], Step [760/5158], Loss: 0.5186\nEpoch [1/5], Step [770/5158], Loss: 0.5209\nEpoch [1/5], Step [780/5158], Loss: 0.4914\nEpoch [1/5], Step [790/5158], Loss: 0.5350\nEpoch [1/5], Step [800/5158], Loss: 0.5550\nEpoch [1/5], Step [810/5158], Loss: 0.5206\nEpoch [1/5], Step [820/5158], Loss: 0.5395\nEpoch [1/5], Step [830/5158], Loss: 0.5582\nEpoch [1/5], Step [840/5158], Loss: 0.5248\nEpoch [1/5], Step [850/5158], Loss: 0.5386\nEpoch [1/5], Step [860/5158], Loss: 0.5022\nEpoch [1/5], Step [870/5158], Loss: 0.5014\nEpoch [1/5], Step [880/5158], Loss: 0.5039\nEpoch [1/5], Step [890/5158], Loss: 0.5993\nEpoch [1/5], Step [900/5158], Loss: 0.5367\nEpoch [1/5], Step [910/5158], Loss: 0.4263\nEpoch [1/5], Step [920/5158], Loss: 0.4852\nEpoch [1/5], Step [930/5158], Loss: 0.4007\nEpoch [1/5], Step [940/5158], Loss: 0.4527\nEpoch [1/5], Step [950/5158], Loss: 0.4109\nEpoch [1/5], Step [960/5158], Loss: 0.3876\nEpoch [1/5], Step [970/5158], Loss: 0.4146\nEpoch [1/5], Step [980/5158], Loss: 0.4022\nEpoch [1/5], Step [990/5158], Loss: 0.3917\nEpoch [1/5], Step [1000/5158], Loss: 0.4381\nEpoch [1/5], Step [1010/5158], Loss: 0.4476\nEpoch [1/5], Step [1020/5158], Loss: 0.3917\nEpoch [1/5], Step [1030/5158], Loss: 0.3632\nEpoch [1/5], Step [1040/5158], Loss: 0.3037\nEpoch [1/5], Step [1050/5158], Loss: 0.4140\nEpoch [1/5], Step [1060/5158], Loss: 0.3561\nEpoch [1/5], Step [1070/5158], Loss: 0.3625\nEpoch [1/5], Step [1080/5158], Loss: 0.3234\nEpoch [1/5], Step [1090/5158], Loss: 0.2812\nEpoch [1/5], Step [1100/5158], Loss: 0.3760\nEpoch [1/5], Step [1110/5158], Loss: 0.3497\nEpoch [1/5], Step [1120/5158], Loss: 0.3919\nEpoch [1/5], Step [1130/5158], Loss: 0.3165\nEpoch [1/5], Step [1140/5158], Loss: 0.2729\nEpoch [1/5], Step [1150/5158], Loss: 0.2779\nEpoch [1/5], Step [1160/5158], Loss: 0.4335\nEpoch [1/5], Step [1170/5158], Loss: 0.3975\nEpoch [1/5], Step [1180/5158], Loss: 0.3306\nEpoch [1/5], Step [1190/5158], Loss: 0.3460\nEpoch [1/5], Step [1200/5158], Loss: 0.3297\nEpoch [1/5], Step [1210/5158], Loss: 0.3257\nEpoch [1/5], Step [1220/5158], Loss: 0.3447\nEpoch [1/5], Step [1230/5158], Loss: 0.3152\nEpoch [1/5], Step [1240/5158], Loss: 0.3355\nEpoch [1/5], Step [1250/5158], Loss: 0.2706\nEpoch [1/5], Step [1260/5158], Loss: 0.3058\nEpoch [1/5], Step [1270/5158], Loss: 0.2816\nEpoch [1/5], Step [1280/5158], Loss: 0.2625\nEpoch [1/5], Step [1290/5158], Loss: 0.3055\nEpoch [1/5], Step [1300/5158], Loss: 0.3127\nEpoch [1/5], Step [1310/5158], Loss: 0.2472\nEpoch [1/5], Step [1320/5158], Loss: 0.3208\nEpoch [1/5], Step [1330/5158], Loss: 0.2753\nEpoch [1/5], Step [1340/5158], Loss: 0.3804\nEpoch [1/5], Step [1350/5158], Loss: 0.3009\nEpoch [1/5], Step [1360/5158], Loss: 0.2854\nEpoch [1/5], Step [1370/5158], Loss: 0.3201\nEpoch [1/5], Step [1380/5158], Loss: 0.2867\nEpoch [1/5], Step [1390/5158], Loss: 0.3073\nEpoch [1/5], Step [1400/5158], Loss: 0.3061\nEpoch [1/5], Step [1410/5158], Loss: 0.3011\nEpoch [1/5], Step [1420/5158], Loss: 0.2731\nEpoch [1/5], Step [1430/5158], Loss: 0.3022\nEpoch [1/5], Step [1440/5158], Loss: 0.2584\nEpoch [1/5], Step [1450/5158], Loss: 0.2365\nEpoch [1/5], Step [1460/5158], Loss: 0.2798\nEpoch [1/5], Step [1470/5158], Loss: 0.2549\nEpoch [1/5], Step [1480/5158], Loss: 0.2751\nEpoch [1/5], Step [1490/5158], Loss: 0.2678\nEpoch [1/5], Step [1500/5158], Loss: 0.3006\nEpoch [1/5], Step [1510/5158], Loss: 0.2762\nEpoch [1/5], Step [1520/5158], Loss: 0.3274\nEpoch [1/5], Step [1530/5158], Loss: 0.3341\nEpoch [1/5], Step [1540/5158], Loss: 0.2923\nEpoch [1/5], Step [1550/5158], Loss: 0.3441\nEpoch [1/5], Step [1560/5158], Loss: 0.2988\nEpoch [1/5], Step [1570/5158], Loss: 0.2726\nEpoch [1/5], Step [1580/5158], Loss: 0.2469\nEpoch [1/5], Step [1590/5158], Loss: 0.2193\nEpoch [1/5], Step [1600/5158], Loss: 0.2599\nEpoch [1/5], Step [1610/5158], Loss: 0.2536\nEpoch [1/5], Step [1620/5158], Loss: 0.2699\nEpoch [1/5], Step [1630/5158], Loss: 0.3280\nEpoch [1/5], Step [1640/5158], Loss: 0.2678\nEpoch [1/5], Step [1650/5158], Loss: 0.2899\nEpoch [1/5], Step [1660/5158], Loss: 0.2764\nEpoch [1/5], Step [1670/5158], Loss: 0.2531\nEpoch [1/5], Step [1680/5158], Loss: 0.3389\nEpoch [1/5], Step [1690/5158], Loss: 0.2333\nEpoch [1/5], Step [1700/5158], Loss: 0.2745\nEpoch [1/5], Step [1710/5158], Loss: 0.2373\nEpoch [1/5], Step [1720/5158], Loss: 0.3148\nEpoch [1/5], Step [1730/5158], Loss: 0.2516\nEpoch [1/5], Step [1740/5158], Loss: 0.3088\nEpoch [1/5], Step [1750/5158], Loss: 0.2584\nEpoch [1/5], Step [1760/5158], Loss: 0.2356\nEpoch [1/5], Step [1770/5158], Loss: 0.2363\nEpoch [1/5], Step [1780/5158], Loss: 0.3219\nEpoch [1/5], Step [1790/5158], Loss: 0.2750\nEpoch [1/5], Step [1800/5158], Loss: 0.2945\nEpoch [1/5], Step [1810/5158], Loss: 0.2506\nEpoch [1/5], Step [1820/5158], Loss: 0.3203\nEpoch [1/5], Step [1830/5158], Loss: 0.2712\nEpoch [1/5], Step [1840/5158], Loss: 0.2649\nEpoch [1/5], Step [1850/5158], Loss: 0.3091\nEpoch [1/5], Step [1860/5158], Loss: 0.2908\nEpoch [1/5], Step [1870/5158], Loss: 0.2692\nEpoch [1/5], Step [1880/5158], Loss: 0.2157\nEpoch [1/5], Step [1890/5158], Loss: 0.3318\nEpoch [1/5], Step [1900/5158], Loss: 0.2755\nEpoch [1/5], Step [1910/5158], Loss: 0.2296\nEpoch [1/5], Step [1920/5158], Loss: 0.2333\nEpoch [1/5], Step [1930/5158], Loss: 0.2168\nEpoch [1/5], Step [1940/5158], Loss: 0.2462\nEpoch [1/5], Step [1950/5158], Loss: 0.2568\nEpoch [1/5], Step [1960/5158], Loss: 0.2685\nEpoch [1/5], Step [1970/5158], Loss: 0.2613\nEpoch [1/5], Step [1980/5158], Loss: 0.2885\nEpoch [1/5], Step [1990/5158], Loss: 0.2617\nEpoch [1/5], Step [2000/5158], Loss: 0.2929\nEpoch [1/5], Step [2010/5158], Loss: 0.2475\nEpoch [1/5], Step [2020/5158], Loss: 0.2751\nEpoch [1/5], Step [2030/5158], Loss: 0.2361\nEpoch [1/5], Step [2040/5158], Loss: 0.2810\nEpoch [1/5], Step [2050/5158], Loss: 0.2647\nEpoch [1/5], Step [2060/5158], Loss: 0.2773\nEpoch [1/5], Step [2070/5158], Loss: 0.3044\nEpoch [1/5], Step [2080/5158], Loss: 0.2417\nEpoch [1/5], Step [2090/5158], Loss: 0.2574\nEpoch [1/5], Step [2100/5158], Loss: 0.2990\nEpoch [1/5], Step [2110/5158], Loss: 0.2578\nEpoch [1/5], Step [2120/5158], Loss: 0.2965\nEpoch [1/5], Step [2130/5158], Loss: 0.2333\nEpoch [1/5], Step [2140/5158], Loss: 0.2886\nEpoch [1/5], Step [2150/5158], Loss: 0.2870\nEpoch [1/5], Step [2160/5158], Loss: 0.2053\nEpoch [1/5], Step [2170/5158], Loss: 0.2258\nEpoch [1/5], Step [2180/5158], Loss: 0.2987\nEpoch [1/5], Step [2190/5158], Loss: 0.2417\nEpoch [1/5], Step [2200/5158], Loss: 0.2881\nEpoch [1/5], Step [2210/5158], Loss: 0.2064\nEpoch [1/5], Step [2220/5158], Loss: 0.2946\nEpoch [1/5], Step [2230/5158], Loss: 0.2302\nEpoch [1/5], Step [2240/5158], Loss: 0.2931\nEpoch [1/5], Step [2250/5158], Loss: 0.2306\nEpoch [1/5], Step [2260/5158], Loss: 0.2553\nEpoch [1/5], Step [2270/5158], Loss: 0.2843\nEpoch [1/5], Step [2280/5158], Loss: 0.2779\nEpoch [1/5], Step [2290/5158], Loss: 0.2249\nEpoch [1/5], Step [2300/5158], Loss: 0.2708\nEpoch [1/5], Step [2310/5158], Loss: 0.2357\nEpoch [1/5], Step [2320/5158], Loss: 0.2899\nEpoch [1/5], Step [2330/5158], Loss: 0.2540\nEpoch [1/5], Step [2340/5158], Loss: 0.2752\nEpoch [1/5], Step [2350/5158], Loss: 0.2657\nEpoch [1/5], Step [2360/5158], Loss: 0.2986\nEpoch [1/5], Step [2370/5158], Loss: 0.2552\nEpoch [1/5], Step [2380/5158], Loss: 0.2523\nEpoch [1/5], Step [2390/5158], Loss: 0.2749\nEpoch [1/5], Step [2400/5158], Loss: 0.1955\nEpoch [1/5], Step [2410/5158], Loss: 0.1959\nEpoch [1/5], Step [2420/5158], Loss: 0.2040\nEpoch [1/5], Step [2430/5158], Loss: 0.2436\nEpoch [1/5], Step [2440/5158], Loss: 0.2371\nEpoch [1/5], Step [2450/5158], Loss: 0.2357\nEpoch [1/5], Step [2460/5158], Loss: 0.2801\nEpoch [1/5], Step [2470/5158], Loss: 0.2326\nEpoch [1/5], Step [2480/5158], Loss: 0.2902\nEpoch [1/5], Step [2490/5158], Loss: 0.2587\nEpoch [1/5], Step [2500/5158], Loss: 0.2389\nEpoch [1/5], Step [2510/5158], Loss: 0.2175\nEpoch [1/5], Step [2520/5158], Loss: 0.2533\nEpoch [1/5], Step [2530/5158], Loss: 0.1945\nEpoch [1/5], Step [2540/5158], Loss: 0.2594\nEpoch [1/5], Step [2550/5158], Loss: 0.2997\nEpoch [1/5], Step [2560/5158], Loss: 0.2605\nEpoch [1/5], Step [2570/5158], Loss: 0.2539\nEpoch [1/5], Step [2580/5158], Loss: 0.2241\nEpoch [1/5], Step [2590/5158], Loss: 0.2318\nEpoch [1/5], Step [2600/5158], Loss: 0.2705\nEpoch [1/5], Step [2610/5158], Loss: 0.2646\nEpoch [1/5], Step [2620/5158], Loss: 0.2269\nEpoch [1/5], Step [2630/5158], Loss: 0.2388\nEpoch [1/5], Step [2640/5158], Loss: 0.1931\nEpoch [1/5], Step [2650/5158], Loss: 0.2743\nEpoch [1/5], Step [2660/5158], Loss: 0.2124\nEpoch [1/5], Step [2670/5158], Loss: 0.2852\nEpoch [1/5], Step [2680/5158], Loss: 0.2623\nEpoch [1/5], Step [2690/5158], Loss: 0.1731\nEpoch [1/5], Step [2700/5158], Loss: 0.2647\nEpoch [1/5], Step [2710/5158], Loss: 0.2384\nEpoch [1/5], Step [2720/5158], Loss: 0.2678\nEpoch [1/5], Step [2730/5158], Loss: 0.2709\nEpoch [1/5], Step [2740/5158], Loss: 0.2425\nEpoch [1/5], Step [2750/5158], Loss: 0.2512\nEpoch [1/5], Step [2760/5158], Loss: 0.2261\nEpoch [1/5], Step [2770/5158], Loss: 0.2225\nEpoch [1/5], Step [2780/5158], Loss: 0.2389\nEpoch [1/5], Step [2790/5158], Loss: 0.2919\nEpoch [1/5], Step [2800/5158], Loss: 0.2781\nEpoch [1/5], Step [2810/5158], Loss: 0.2340\nEpoch [1/5], Step [2820/5158], Loss: 0.2832\nEpoch [1/5], Step [2830/5158], Loss: 0.2616\nEpoch [1/5], Step [2840/5158], Loss: 0.2838\nEpoch [1/5], Step [2850/5158], Loss: 0.2460\nEpoch [1/5], Step [2860/5158], Loss: 0.2537\nEpoch [1/5], Step [2870/5158], Loss: 0.2200\nEpoch [1/5], Step [2880/5158], Loss: 0.2222\nEpoch [1/5], Step [2890/5158], Loss: 0.1987\nEpoch [1/5], Step [2900/5158], Loss: 0.2432\nEpoch [1/5], Step [2910/5158], Loss: 0.2328\nEpoch [1/5], Step [2920/5158], Loss: 0.2409\nEpoch [1/5], Step [2930/5158], Loss: 0.2375\nEpoch [1/5], Step [2940/5158], Loss: 0.1751\nEpoch [1/5], Step [2950/5158], Loss: 0.3397\nEpoch [1/5], Step [2960/5158], Loss: 0.2827\nEpoch [1/5], Step [2970/5158], Loss: 0.2384\nEpoch [1/5], Step [2980/5158], Loss: 0.2227\nEpoch [1/5], Step [2990/5158], Loss: 0.2518\nEpoch [1/5], Step [3000/5158], Loss: 0.2479\nEpoch [1/5], Step [3010/5158], Loss: 0.2897\nEpoch [1/5], Step [3020/5158], Loss: 0.2379\nEpoch [1/5], Step [3030/5158], Loss: 0.2083\nEpoch [1/5], Step [3040/5158], Loss: 0.1859\nEpoch [1/5], Step [3050/5158], Loss: 0.2647\nEpoch [1/5], Step [3060/5158], Loss: 0.2720\nEpoch [1/5], Step [3070/5158], Loss: 0.2579\nEpoch [1/5], Step [3080/5158], Loss: 0.2626\nEpoch [1/5], Step [3090/5158], Loss: 0.1869\nEpoch [1/5], Step [3100/5158], Loss: 0.2069\nEpoch [1/5], Step [3110/5158], Loss: 0.2135\nEpoch [1/5], Step [3120/5158], Loss: 0.2457\nEpoch [1/5], Step [3130/5158], Loss: 0.2916\nEpoch [1/5], Step [3140/5158], Loss: 0.2174\nEpoch [1/5], Step [3150/5158], Loss: 0.2176\nEpoch [1/5], Step [3160/5158], Loss: 0.2341\nEpoch [1/5], Step [3170/5158], Loss: 0.2067\nEpoch [1/5], Step [3180/5158], Loss: 0.3122\nEpoch [1/5], Step [3190/5158], Loss: 0.2083\nEpoch [1/5], Step [3200/5158], Loss: 0.3039\nEpoch [1/5], Step [3210/5158], Loss: 0.2373\nEpoch [1/5], Step [3220/5158], Loss: 0.2390\nEpoch [1/5], Step [3230/5158], Loss: 0.2303\nEpoch [1/5], Step [3240/5158], Loss: 0.2478\nEpoch [1/5], Step [3250/5158], Loss: 0.2092\nEpoch [1/5], Step [3260/5158], Loss: 0.2070\nEpoch [1/5], Step [3270/5158], Loss: 0.2315\nEpoch [1/5], Step [3280/5158], Loss: 0.2116\nEpoch [1/5], Step [3290/5158], Loss: 0.2415\nEpoch [1/5], Step [3300/5158], Loss: 0.2229\nEpoch [1/5], Step [3310/5158], Loss: 0.1868\nEpoch [1/5], Step [3320/5158], Loss: 0.1862\nEpoch [1/5], Step [3330/5158], Loss: 0.2187\nEpoch [1/5], Step [3340/5158], Loss: 0.2547\nEpoch [1/5], Step [3350/5158], Loss: 0.2264\nEpoch [1/5], Step [3360/5158], Loss: 0.2682\nEpoch [1/5], Step [3370/5158], Loss: 0.1979\nEpoch [1/5], Step [3380/5158], Loss: 0.2605\nEpoch [1/5], Step [3390/5158], Loss: 0.2189\nEpoch [1/5], Step [3400/5158], Loss: 0.2516\nEpoch [1/5], Step [3410/5158], Loss: 0.2808\nEpoch [1/5], Step [3420/5158], Loss: 0.2732\nEpoch [1/5], Step [3430/5158], Loss: 0.2086\nEpoch [1/5], Step [3440/5158], Loss: 0.2216\nEpoch [1/5], Step [3450/5158], Loss: 0.2771\nEpoch [1/5], Step [3460/5158], Loss: 0.1994\nEpoch [1/5], Step [3470/5158], Loss: 0.2344\nEpoch [1/5], Step [3480/5158], Loss: 0.2335\nEpoch [1/5], Step [3490/5158], Loss: 0.2421\nEpoch [1/5], Step [3500/5158], Loss: 0.2023\nEpoch [1/5], Step [3510/5158], Loss: 0.2317\nEpoch [1/5], Step [3520/5158], Loss: 0.2576\nEpoch [1/5], Step [3530/5158], Loss: 0.2757\nEpoch [1/5], Step [3540/5158], Loss: 0.2503\nEpoch [1/5], Step [3550/5158], Loss: 0.2275\nEpoch [1/5], Step [3560/5158], Loss: 0.1876\nEpoch [1/5], Step [3570/5158], Loss: 0.1913\nEpoch [1/5], Step [3580/5158], Loss: 0.1984\nEpoch [1/5], Step [3590/5158], Loss: 0.2388\nEpoch [1/5], Step [3600/5158], Loss: 0.2694\nEpoch [1/5], Step [3610/5158], Loss: 0.2052\nEpoch [1/5], Step [3620/5158], Loss: 0.2747\nEpoch [1/5], Step [3630/5158], Loss: 0.2548\nEpoch [1/5], Step [3640/5158], Loss: 0.2517\nEpoch [1/5], Step [3650/5158], Loss: 0.2434\nEpoch [1/5], Step [3660/5158], Loss: 0.2248\nEpoch [1/5], Step [3670/5158], Loss: 0.2167\nEpoch [1/5], Step [3680/5158], Loss: 0.1735\nEpoch [1/5], Step [3690/5158], Loss: 0.2089\nEpoch [1/5], Step [3700/5158], Loss: 0.2689\nEpoch [1/5], Step [3710/5158], Loss: 0.2275\nEpoch [1/5], Step [3720/5158], Loss: 0.2012\nEpoch [1/5], Step [3730/5158], Loss: 0.2650\nEpoch [1/5], Step [3740/5158], Loss: 0.2397\nEpoch [1/5], Step [3750/5158], Loss: 0.2840\nEpoch [1/5], Step [3760/5158], Loss: 0.2072\nEpoch [1/5], Step [3770/5158], Loss: 0.2510\nEpoch [1/5], Step [3780/5158], Loss: 0.2164\nEpoch [1/5], Step [3790/5158], Loss: 0.2059\nEpoch [1/5], Step [3800/5158], Loss: 0.2670\nEpoch [1/5], Step [3810/5158], Loss: 0.2078\nEpoch [1/5], Step [3820/5158], Loss: 0.2510\nEpoch [1/5], Step [3830/5158], Loss: 0.2029\nEpoch [1/5], Step [3840/5158], Loss: 0.2340\nEpoch [1/5], Step [3850/5158], Loss: 0.2432\nEpoch [1/5], Step [3860/5158], Loss: 0.2561\nEpoch [1/5], Step [3870/5158], Loss: 0.2199\nEpoch [1/5], Step [3880/5158], Loss: 0.2368\nEpoch [1/5], Step [3890/5158], Loss: 0.1822\nEpoch [1/5], Step [3900/5158], Loss: 0.2786\nEpoch [1/5], Step [3910/5158], Loss: 0.2375\nEpoch [1/5], Step [3920/5158], Loss: 0.2366\nEpoch [1/5], Step [3930/5158], Loss: 0.2999\nEpoch [1/5], Step [3940/5158], Loss: 0.2936\nEpoch [1/5], Step [3950/5158], Loss: 0.2933\nEpoch [1/5], Step [3960/5158], Loss: 0.1984\nEpoch [1/5], Step [3970/5158], Loss: 0.2613\nEpoch [1/5], Step [3980/5158], Loss: 0.1992\nEpoch [1/5], Step [3990/5158], Loss: 0.1731\nEpoch [1/5], Step [4000/5158], Loss: 0.2125\nEpoch [1/5], Step [4010/5158], Loss: 0.2369\nEpoch [1/5], Step [4020/5158], Loss: 0.2837\nEpoch [1/5], Step [4030/5158], Loss: 0.2396\nEpoch [1/5], Step [4040/5158], Loss: 0.1979\nEpoch [1/5], Step [4050/5158], Loss: 0.1901\nEpoch [1/5], Step [4060/5158], Loss: 0.2024\nEpoch [1/5], Step [4070/5158], Loss: 0.1962\nEpoch [1/5], Step [4080/5158], Loss: 0.2193\nEpoch [1/5], Step [4090/5158], Loss: 0.1766\nEpoch [1/5], Step [4100/5158], Loss: 0.2017\nEpoch [1/5], Step [4110/5158], Loss: 0.2037\nEpoch [1/5], Step [4120/5158], Loss: 0.2832\nEpoch [1/5], Step [4130/5158], Loss: 0.1589\nEpoch [1/5], Step [4140/5158], Loss: 0.2085\nEpoch [1/5], Step [4150/5158], Loss: 0.2607\nEpoch [1/5], Step [4160/5158], Loss: 0.1808\nEpoch [1/5], Step [4170/5158], Loss: 0.2267\nEpoch [1/5], Step [4180/5158], Loss: 0.2951\nEpoch [1/5], Step [4190/5158], Loss: 0.1973\nEpoch [1/5], Step [4200/5158], Loss: 0.2056\nEpoch [1/5], Step [4210/5158], Loss: 0.2366\nEpoch [1/5], Step [4220/5158], Loss: 0.2400\nEpoch [1/5], Step [4230/5158], Loss: 0.2897\nEpoch [1/5], Step [4240/5158], Loss: 0.1818\nEpoch [1/5], Step [4250/5158], Loss: 0.1773\nEpoch [1/5], Step [4260/5158], Loss: 0.1377\nEpoch [1/5], Step [4270/5158], Loss: 0.2680\nEpoch [1/5], Step [4280/5158], Loss: 0.2140\nEpoch [1/5], Step [4290/5158], Loss: 0.2408\nEpoch [1/5], Step [4300/5158], Loss: 0.1874\nEpoch [1/5], Step [4310/5158], Loss: 0.2409\nEpoch [1/5], Step [4320/5158], Loss: 0.1992\nEpoch [1/5], Step [4330/5158], Loss: 0.1911\nEpoch [1/5], Step [4340/5158], Loss: 0.3269\nEpoch [1/5], Step [4350/5158], Loss: 0.2284\nEpoch [1/5], Step [4360/5158], Loss: 0.2420\nEpoch [1/5], Step [4370/5158], Loss: 0.2419\nEpoch [1/5], Step [4380/5158], Loss: 0.2736\nEpoch [1/5], Step [4390/5158], Loss: 0.1849\nEpoch [1/5], Step [4400/5158], Loss: 0.1690\nEpoch [1/5], Step [4410/5158], Loss: 0.2528\nEpoch [1/5], Step [4420/5158], Loss: 0.2305\nEpoch [1/5], Step [4430/5158], Loss: 0.1794\nEpoch [1/5], Step [4440/5158], Loss: 0.2013\nEpoch [1/5], Step [4450/5158], Loss: 0.2318\nEpoch [1/5], Step [4460/5158], Loss: 0.2216\nEpoch [1/5], Step [4470/5158], Loss: 0.2392\nEpoch [1/5], Step [4480/5158], Loss: 0.1758\nEpoch [1/5], Step [4490/5158], Loss: 0.1882\nEpoch [1/5], Step [4500/5158], Loss: 0.2692\nEpoch [1/5], Step [4510/5158], Loss: 0.2338\nEpoch [1/5], Step [4520/5158], Loss: 0.2193\nEpoch [1/5], Step [4530/5158], Loss: 0.2513\nEpoch [1/5], Step [4540/5158], Loss: 0.2402\nEpoch [1/5], Step [4550/5158], Loss: 0.1903\nEpoch [1/5], Step [4560/5158], Loss: 0.2304\nEpoch [1/5], Step [4570/5158], Loss: 0.2133\nEpoch [1/5], Step [4580/5158], Loss: 0.2595\nEpoch [1/5], Step [4590/5158], Loss: 0.1877\nEpoch [1/5], Step [4600/5158], Loss: 0.2567\nEpoch [1/5], Step [4610/5158], Loss: 0.3558\nEpoch [1/5], Step [4620/5158], Loss: 0.2395\nEpoch [1/5], Step [4630/5158], Loss: 0.2308\nEpoch [1/5], Step [4640/5158], Loss: 0.2505\nEpoch [1/5], Step [4650/5158], Loss: 0.1990\nEpoch [1/5], Step [4660/5158], Loss: 0.2757\nEpoch [1/5], Step [4670/5158], Loss: 0.2622\nEpoch [1/5], Step [4680/5158], Loss: 0.2387\nEpoch [1/5], Step [4690/5158], Loss: 0.2346\nEpoch [1/5], Step [4700/5158], Loss: 0.2517\nEpoch [1/5], Step [4710/5158], Loss: 0.1969\nEpoch [1/5], Step [4720/5158], Loss: 0.1739\nEpoch [1/5], Step [4730/5158], Loss: 0.2139\nEpoch [1/5], Step [4740/5158], Loss: 0.2487\nEpoch [1/5], Step [4750/5158], Loss: 0.1947\nEpoch [1/5], Step [4760/5158], Loss: 0.2185\nEpoch [1/5], Step [4770/5158], Loss: 0.1730\nEpoch [1/5], Step [4780/5158], Loss: 0.2113\nEpoch [1/5], Step [4790/5158], Loss: 0.1883\nEpoch [1/5], Step [4800/5158], Loss: 0.2193\nEpoch [1/5], Step [4810/5158], Loss: 0.2497\nEpoch [1/5], Step [4820/5158], Loss: 0.2283\nEpoch [1/5], Step [4830/5158], Loss: 0.1874\nEpoch [1/5], Step [4840/5158], Loss: 0.2330\nEpoch [1/5], Step [4850/5158], Loss: 0.1939\nEpoch [1/5], Step [4860/5158], Loss: 0.1664\nEpoch [1/5], Step [4870/5158], Loss: 0.2130\nEpoch [1/5], Step [4880/5158], Loss: 0.1959\nEpoch [1/5], Step [4890/5158], Loss: 0.1920\nEpoch [1/5], Step [4900/5158], Loss: 0.2185\nEpoch [1/5], Step [4910/5158], Loss: 0.1942\nEpoch [1/5], Step [4920/5158], Loss: 0.2629\nEpoch [1/5], Step [4930/5158], Loss: 0.2008\nEpoch [1/5], Step [4940/5158], Loss: 0.2116\nEpoch [1/5], Step [4950/5158], Loss: 0.1762\nEpoch [1/5], Step [4960/5158], Loss: 0.1787\nEpoch [1/5], Step [4970/5158], Loss: 0.2578\nEpoch [1/5], Step [4980/5158], Loss: 0.2522\nEpoch [1/5], Step [4990/5158], Loss: 0.2394\nEpoch [1/5], Step [5000/5158], Loss: 0.2079\nEpoch [1/5], Step [5010/5158], Loss: 0.2054\nEpoch [1/5], Step [5020/5158], Loss: 0.1752\nEpoch [1/5], Step [5030/5158], Loss: 0.2613\nEpoch [1/5], Step [5040/5158], Loss: 0.2114\nEpoch [1/5], Step [5050/5158], Loss: 0.2323\nEpoch [1/5], Step [5060/5158], Loss: 0.1871\nEpoch [1/5], Step [5070/5158], Loss: 0.1979\nEpoch [1/5], Step [5080/5158], Loss: 0.1759\nEpoch [1/5], Step [5090/5158], Loss: 0.1712\nEpoch [1/5], Step [5100/5158], Loss: 0.2165\nEpoch [1/5], Step [5110/5158], Loss: 0.2541\nEpoch [1/5], Step [5120/5158], Loss: 0.3033\nEpoch [1/5], Step [5130/5158], Loss: 0.2050\nEpoch [1/5], Step [5140/5158], Loss: 0.1828\nEpoch [1/5], Step [5150/5158], Loss: 0.2232\nEpoch [2/5], Step [10/5158], Loss: 0.2342\nEpoch [2/5], Step [20/5158], Loss: 0.1902\nEpoch [2/5], Step [30/5158], Loss: 0.2171\nEpoch [2/5], Step [40/5158], Loss: 0.2537\nEpoch [2/5], Step [50/5158], Loss: 0.1732\nEpoch [2/5], Step [60/5158], Loss: 0.2317\nEpoch [2/5], Step [70/5158], Loss: 0.2571\nEpoch [2/5], Step [80/5158], Loss: 0.2093\nEpoch [2/5], Step [90/5158], Loss: 0.1765\nEpoch [2/5], Step [100/5158], Loss: 0.1707\nEpoch [2/5], Step [110/5158], Loss: 0.2175\nEpoch [2/5], Step [120/5158], Loss: 0.2163\nEpoch [2/5], Step [130/5158], Loss: 0.1674\nEpoch [2/5], Step [140/5158], Loss: 0.2317\nEpoch [2/5], Step [150/5158], Loss: 0.1874\nEpoch [2/5], Step [160/5158], Loss: 0.1848\nEpoch [2/5], Step [170/5158], Loss: 0.1974\nEpoch [2/5], Step [180/5158], Loss: 0.1967\nEpoch [2/5], Step [190/5158], Loss: 0.1372\nEpoch [2/5], Step [200/5158], Loss: 0.1614\nEpoch [2/5], Step [210/5158], Loss: 0.2120\nEpoch [2/5], Step [220/5158], Loss: 0.2351\nEpoch [2/5], Step [230/5158], Loss: 0.1838\nEpoch [2/5], Step [240/5158], Loss: 0.2281\nEpoch [2/5], Step [250/5158], Loss: 0.2152\nEpoch [2/5], Step [260/5158], Loss: 0.1723\nEpoch [2/5], Step [270/5158], Loss: 0.1889\nEpoch [2/5], Step [280/5158], Loss: 0.1506\nEpoch [2/5], Step [290/5158], Loss: 0.3034\nEpoch [2/5], Step [300/5158], Loss: 0.2311\nEpoch [2/5], Step [310/5158], Loss: 0.2153\nEpoch [2/5], Step [320/5158], Loss: 0.2163\nEpoch [2/5], Step [330/5158], Loss: 0.2898\nEpoch [2/5], Step [340/5158], Loss: 0.1876\nEpoch [2/5], Step [350/5158], Loss: 0.2054\nEpoch [2/5], Step [360/5158], Loss: 0.1981\nEpoch [2/5], Step [370/5158], Loss: 0.1790\nEpoch [2/5], Step [380/5158], Loss: 0.1712\nEpoch [2/5], Step [390/5158], Loss: 0.2060\nEpoch [2/5], Step [400/5158], Loss: 0.2315\nEpoch [2/5], Step [410/5158], Loss: 0.1792\nEpoch [2/5], Step [420/5158], Loss: 0.1725\nEpoch [2/5], Step [430/5158], Loss: 0.1688\nEpoch [2/5], Step [440/5158], Loss: 0.2363\nEpoch [2/5], Step [450/5158], Loss: 0.1661\nEpoch [2/5], Step [460/5158], Loss: 0.2284\nEpoch [2/5], Step [470/5158], Loss: 0.1844\nEpoch [2/5], Step [480/5158], Loss: 0.1405\nEpoch [2/5], Step [490/5158], Loss: 0.1727\nEpoch [2/5], Step [500/5158], Loss: 0.2079\nEpoch [2/5], Step [510/5158], Loss: 0.2107\nEpoch [2/5], Step [520/5158], Loss: 0.1787\nEpoch [2/5], Step [530/5158], Loss: 0.2069\nEpoch [2/5], Step [540/5158], Loss: 0.1891\nEpoch [2/5], Step [550/5158], Loss: 0.2300\nEpoch [2/5], Step [560/5158], Loss: 0.1546\nEpoch [2/5], Step [570/5158], Loss: 0.1545\nEpoch [2/5], Step [580/5158], Loss: 0.2192\nEpoch [2/5], Step [590/5158], Loss: 0.2837\nEpoch [2/5], Step [600/5158], Loss: 0.1942\nEpoch [2/5], Step [610/5158], Loss: 0.2093\nEpoch [2/5], Step [620/5158], Loss: 0.2445\nEpoch [2/5], Step [630/5158], Loss: 0.2016\nEpoch [2/5], Step [640/5158], Loss: 0.2034\nEpoch [2/5], Step [650/5158], Loss: 0.1889\nEpoch [2/5], Step [660/5158], Loss: 0.1377\nEpoch [2/5], Step [670/5158], Loss: 0.2041\nEpoch [2/5], Step [680/5158], Loss: 0.1706\nEpoch [2/5], Step [690/5158], Loss: 0.1892\nEpoch [2/5], Step [700/5158], Loss: 0.2094\nEpoch [2/5], Step [710/5158], Loss: 0.2211\nEpoch [2/5], Step [720/5158], Loss: 0.1906\nEpoch [2/5], Step [730/5158], Loss: 0.2184\nEpoch [2/5], Step [740/5158], Loss: 0.1413\nEpoch [2/5], Step [750/5158], Loss: 0.2220\nEpoch [2/5], Step [760/5158], Loss: 0.2039\nEpoch [2/5], Step [770/5158], Loss: 0.2347\nEpoch [2/5], Step [780/5158], Loss: 0.2394\nEpoch [2/5], Step [790/5158], Loss: 0.2279\nEpoch [2/5], Step [800/5158], Loss: 0.1876\nEpoch [2/5], Step [810/5158], Loss: 0.1958\nEpoch [2/5], Step [820/5158], Loss: 0.1671\nEpoch [2/5], Step [830/5158], Loss: 0.1965\nEpoch [2/5], Step [840/5158], Loss: 0.1973\nEpoch [2/5], Step [850/5158], Loss: 0.2572\nEpoch [2/5], Step [860/5158], Loss: 0.1698\nEpoch [2/5], Step [870/5158], Loss: 0.1801\nEpoch [2/5], Step [880/5158], Loss: 0.2230\nEpoch [2/5], Step [890/5158], Loss: 0.1413\nEpoch [2/5], Step [900/5158], Loss: 0.1893\nEpoch [2/5], Step [910/5158], Loss: 0.2735\nEpoch [2/5], Step [920/5158], Loss: 0.1789\nEpoch [2/5], Step [930/5158], Loss: 0.1908\nEpoch [2/5], Step [940/5158], Loss: 0.1773\nEpoch [2/5], Step [950/5158], Loss: 0.1843\nEpoch [2/5], Step [960/5158], Loss: 0.2242\nEpoch [2/5], Step [970/5158], Loss: 0.2465\nEpoch [2/5], Step [980/5158], Loss: 0.2915\nEpoch [2/5], Step [990/5158], Loss: 0.2730\nEpoch [2/5], Step [1000/5158], Loss: 0.1844\nEpoch [2/5], Step [1010/5158], Loss: 0.1477\nEpoch [2/5], Step [1020/5158], Loss: 0.2232\nEpoch [2/5], Step [1030/5158], Loss: 0.1796\nEpoch [2/5], Step [1040/5158], Loss: 0.1352\nEpoch [2/5], Step [1050/5158], Loss: 0.2326\nEpoch [2/5], Step [1060/5158], Loss: 0.2552\nEpoch [2/5], Step [1070/5158], Loss: 0.2765\nEpoch [2/5], Step [1080/5158], Loss: 0.1881\nEpoch [2/5], Step [1090/5158], Loss: 0.1960\nEpoch [2/5], Step [1100/5158], Loss: 0.2025\nEpoch [2/5], Step [1110/5158], Loss: 0.1995\nEpoch [2/5], Step [1120/5158], Loss: 0.2183\nEpoch [2/5], Step [1130/5158], Loss: 0.2096\nEpoch [2/5], Step [1140/5158], Loss: 0.1896\nEpoch [2/5], Step [1150/5158], Loss: 0.2154\nEpoch [2/5], Step [1160/5158], Loss: 0.1789\nEpoch [2/5], Step [1170/5158], Loss: 0.1521\nEpoch [2/5], Step [1180/5158], Loss: 0.2403\nEpoch [2/5], Step [1190/5158], Loss: 0.2090\nEpoch [2/5], Step [1200/5158], Loss: 0.2367\nEpoch [2/5], Step [1210/5158], Loss: 0.1869\nEpoch [2/5], Step [1220/5158], Loss: 0.2152\nEpoch [2/5], Step [1230/5158], Loss: 0.1564\nEpoch [2/5], Step [1240/5158], Loss: 0.2006\nEpoch [2/5], Step [1250/5158], Loss: 0.2299\nEpoch [2/5], Step [1260/5158], Loss: 0.2572\nEpoch [2/5], Step [1270/5158], Loss: 0.2189\nEpoch [2/5], Step [1280/5158], Loss: 0.1905\nEpoch [2/5], Step [1290/5158], Loss: 0.2192\nEpoch [2/5], Step [1300/5158], Loss: 0.2005\nEpoch [2/5], Step [1310/5158], Loss: 0.2322\nEpoch [2/5], Step [1320/5158], Loss: 0.1423\nEpoch [2/5], Step [1330/5158], Loss: 0.1875\nEpoch [2/5], Step [1340/5158], Loss: 0.1617\nEpoch [2/5], Step [1350/5158], Loss: 0.2287\nEpoch [2/5], Step [1360/5158], Loss: 0.1757\nEpoch [2/5], Step [1370/5158], Loss: 0.2016\nEpoch [2/5], Step [1380/5158], Loss: 0.2629\nEpoch [2/5], Step [1390/5158], Loss: 0.1698\nEpoch [2/5], Step [1400/5158], Loss: 0.1962\nEpoch [2/5], Step [1410/5158], Loss: 0.1820\nEpoch [2/5], Step [1420/5158], Loss: 0.2397\nEpoch [2/5], Step [1430/5158], Loss: 0.1937\nEpoch [2/5], Step [1440/5158], Loss: 0.1795\nEpoch [2/5], Step [1450/5158], Loss: 0.1992\nEpoch [2/5], Step [1460/5158], Loss: 0.2143\nEpoch [2/5], Step [1470/5158], Loss: 0.1953\nEpoch [2/5], Step [1480/5158], Loss: 0.2472\nEpoch [2/5], Step [1490/5158], Loss: 0.2314\nEpoch [2/5], Step [1500/5158], Loss: 0.1554\nEpoch [2/5], Step [1510/5158], Loss: 0.2369\nEpoch [2/5], Step [1520/5158], Loss: 0.1578\nEpoch [2/5], Step [1530/5158], Loss: 0.1867\nEpoch [2/5], Step [1540/5158], Loss: 0.1810\nEpoch [2/5], Step [1550/5158], Loss: 0.2221\nEpoch [2/5], Step [1560/5158], Loss: 0.2460\nEpoch [2/5], Step [1570/5158], Loss: 0.2181\nEpoch [2/5], Step [1580/5158], Loss: 0.1610\nEpoch [2/5], Step [1590/5158], Loss: 0.1965\nEpoch [2/5], Step [1600/5158], Loss: 0.2792\nEpoch [2/5], Step [1610/5158], Loss: 0.1802\nEpoch [2/5], Step [1620/5158], Loss: 0.2238\nEpoch [2/5], Step [1630/5158], Loss: 0.1888\nEpoch [2/5], Step [1640/5158], Loss: 0.1852\nEpoch [2/5], Step [1650/5158], Loss: 0.2303\nEpoch [2/5], Step [1660/5158], Loss: 0.2151\nEpoch [2/5], Step [1670/5158], Loss: 0.1600\nEpoch [2/5], Step [1680/5158], Loss: 0.2343\nEpoch [2/5], Step [1690/5158], Loss: 0.2400\nEpoch [2/5], Step [1700/5158], Loss: 0.1405\nEpoch [2/5], Step [1710/5158], Loss: 0.1753\nEpoch [2/5], Step [1720/5158], Loss: 0.1646\nEpoch [2/5], Step [1730/5158], Loss: 0.2472\nEpoch [2/5], Step [1740/5158], Loss: 0.1795\nEpoch [2/5], Step [1750/5158], Loss: 0.1791\nEpoch [2/5], Step [1760/5158], Loss: 0.1830\nEpoch [2/5], Step [1770/5158], Loss: 0.1500\nEpoch [2/5], Step [1780/5158], Loss: 0.2181\nEpoch [2/5], Step [1790/5158], Loss: 0.2162\nEpoch [2/5], Step [1800/5158], Loss: 0.1852\nEpoch [2/5], Step [1810/5158], Loss: 0.1764\nEpoch [2/5], Step [1820/5158], Loss: 0.1750\nEpoch [2/5], Step [1830/5158], Loss: 0.1552\nEpoch [2/5], Step [1840/5158], Loss: 0.2388\nEpoch [2/5], Step [1850/5158], Loss: 0.1764\nEpoch [2/5], Step [1860/5158], Loss: 0.2681\nEpoch [2/5], Step [1870/5158], Loss: 0.3054\nEpoch [2/5], Step [1880/5158], Loss: 0.1862\nEpoch [2/5], Step [1890/5158], Loss: 0.2019\nEpoch [2/5], Step [1900/5158], Loss: 0.2505\nEpoch [2/5], Step [1910/5158], Loss: 0.1940\nEpoch [2/5], Step [1920/5158], Loss: 0.1833\nEpoch [2/5], Step [1930/5158], Loss: 0.2212\nEpoch [2/5], Step [1940/5158], Loss: 0.2090\nEpoch [2/5], Step [1950/5158], Loss: 0.1273\nEpoch [2/5], Step [1960/5158], Loss: 0.2248\nEpoch [2/5], Step [1970/5158], Loss: 0.2088\nEpoch [2/5], Step [1980/5158], Loss: 0.2424\nEpoch [2/5], Step [1990/5158], Loss: 0.1888\nEpoch [2/5], Step [2000/5158], Loss: 0.2043\nEpoch [2/5], Step [2010/5158], Loss: 0.2226\nEpoch [2/5], Step [2020/5158], Loss: 0.2272\nEpoch [2/5], Step [2030/5158], Loss: 0.2189\nEpoch [2/5], Step [2040/5158], Loss: 0.2153\nEpoch [2/5], Step [2050/5158], Loss: 0.1541\nEpoch [2/5], Step [2060/5158], Loss: 0.2061\nEpoch [2/5], Step [2070/5158], Loss: 0.2067\nEpoch [2/5], Step [2080/5158], Loss: 0.2523\nEpoch [2/5], Step [2090/5158], Loss: 0.1551\nEpoch [2/5], Step [2100/5158], Loss: 0.2067\nEpoch [2/5], Step [2110/5158], Loss: 0.2491\nEpoch [2/5], Step [2120/5158], Loss: 0.2354\nEpoch [2/5], Step [2130/5158], Loss: 0.1932\nEpoch [2/5], Step [2140/5158], Loss: 0.1979\nEpoch [2/5], Step [2150/5158], Loss: 0.2234\nEpoch [2/5], Step [2160/5158], Loss: 0.1940\nEpoch [2/5], Step [2170/5158], Loss: 0.1564\nEpoch [2/5], Step [2180/5158], Loss: 0.2107\nEpoch [2/5], Step [2190/5158], Loss: 0.2317\nEpoch [2/5], Step [2200/5158], Loss: 0.1849\nEpoch [2/5], Step [2210/5158], Loss: 0.1684\nEpoch [2/5], Step [2220/5158], Loss: 0.1728\nEpoch [2/5], Step [2230/5158], Loss: 0.1951\nEpoch [2/5], Step [2240/5158], Loss: 0.1746\nEpoch [2/5], Step [2250/5158], Loss: 0.2074\nEpoch [2/5], Step [2260/5158], Loss: 0.2541\nEpoch [2/5], Step [2270/5158], Loss: 0.2147\nEpoch [2/5], Step [2280/5158], Loss: 0.2252\nEpoch [2/5], Step [2290/5158], Loss: 0.2038\nEpoch [2/5], Step [2300/5158], Loss: 0.1472\nEpoch [2/5], Step [2310/5158], Loss: 0.1439\nEpoch [2/5], Step [2320/5158], Loss: 0.1700\nEpoch [2/5], Step [2330/5158], Loss: 0.2240\nEpoch [2/5], Step [2340/5158], Loss: 0.1880\nEpoch [2/5], Step [2350/5158], Loss: 0.1569\nEpoch [2/5], Step [2360/5158], Loss: 0.1694\nEpoch [2/5], Step [2370/5158], Loss: 0.1947\nEpoch [2/5], Step [2380/5158], Loss: 0.2303\nEpoch [2/5], Step [2390/5158], Loss: 0.2074\nEpoch [2/5], Step [2400/5158], Loss: 0.1594\nEpoch [2/5], Step [2410/5158], Loss: 0.2224\nEpoch [2/5], Step [2420/5158], Loss: 0.1890\nEpoch [2/5], Step [2430/5158], Loss: 0.2203\nEpoch [2/5], Step [2440/5158], Loss: 0.2422\nEpoch [2/5], Step [2450/5158], Loss: 0.1818\nEpoch [2/5], Step [2460/5158], Loss: 0.1502\nEpoch [2/5], Step [2470/5158], Loss: 0.1623\nEpoch [2/5], Step [2480/5158], Loss: 0.2076\nEpoch [2/5], Step [2490/5158], Loss: 0.2567\nEpoch [2/5], Step [2500/5158], Loss: 0.1900\nEpoch [2/5], Step [2510/5158], Loss: 0.1773\nEpoch [2/5], Step [2520/5158], Loss: 0.2249\nEpoch [2/5], Step [2530/5158], Loss: 0.2060\nEpoch [2/5], Step [2540/5158], Loss: 0.2051\nEpoch [2/5], Step [2550/5158], Loss: 0.2315\nEpoch [2/5], Step [2560/5158], Loss: 0.2172\nEpoch [2/5], Step [2570/5158], Loss: 0.2068\nEpoch [2/5], Step [2580/5158], Loss: 0.1811\nEpoch [2/5], Step [2590/5158], Loss: 0.1908\nEpoch [2/5], Step [2600/5158], Loss: 0.1952\nEpoch [2/5], Step [2610/5158], Loss: 0.2052\nEpoch [2/5], Step [2620/5158], Loss: 0.1718\nEpoch [2/5], Step [2630/5158], Loss: 0.1844\nEpoch [2/5], Step [2640/5158], Loss: 0.2194\nEpoch [2/5], Step [2650/5158], Loss: 0.1933\nEpoch [2/5], Step [2660/5158], Loss: 0.1646\nEpoch [2/5], Step [2670/5158], Loss: 0.1774\nEpoch [2/5], Step [2680/5158], Loss: 0.2000\nEpoch [2/5], Step [2690/5158], Loss: 0.1909\nEpoch [2/5], Step [2700/5158], Loss: 0.1572\nEpoch [2/5], Step [2710/5158], Loss: 0.2188\nEpoch [2/5], Step [2720/5158], Loss: 0.2499\nEpoch [2/5], Step [2730/5158], Loss: 0.2292\nEpoch [2/5], Step [2740/5158], Loss: 0.1867\nEpoch [2/5], Step [2750/5158], Loss: 0.1893\nEpoch [2/5], Step [2760/5158], Loss: 0.1761\nEpoch [2/5], Step [2770/5158], Loss: 0.2007\nEpoch [2/5], Step [2780/5158], Loss: 0.1727\nEpoch [2/5], Step [2790/5158], Loss: 0.2409\nEpoch [2/5], Step [2800/5158], Loss: 0.2000\nEpoch [2/5], Step [2810/5158], Loss: 0.2397\nEpoch [2/5], Step [2820/5158], Loss: 0.2039\nEpoch [2/5], Step [2830/5158], Loss: 0.2077\nEpoch [2/5], Step [2840/5158], Loss: 0.2554\nEpoch [2/5], Step [2850/5158], Loss: 0.1835\nEpoch [2/5], Step [2860/5158], Loss: 0.1780\nEpoch [2/5], Step [2870/5158], Loss: 0.1697\nEpoch [2/5], Step [2880/5158], Loss: 0.2171\nEpoch [2/5], Step [2890/5158], Loss: 0.1896\nEpoch [2/5], Step [2900/5158], Loss: 0.1574\nEpoch [2/5], Step [2910/5158], Loss: 0.1532\nEpoch [2/5], Step [2920/5158], Loss: 0.2144\nEpoch [2/5], Step [2930/5158], Loss: 0.2086\nEpoch [2/5], Step [2940/5158], Loss: 0.1931\nEpoch [2/5], Step [2950/5158], Loss: 0.2124\nEpoch [2/5], Step [2960/5158], Loss: 0.1649\nEpoch [2/5], Step [2970/5158], Loss: 0.1747\nEpoch [2/5], Step [2980/5158], Loss: 0.2214\nEpoch [2/5], Step [2990/5158], Loss: 0.1795\nEpoch [2/5], Step [3000/5158], Loss: 0.1796\nEpoch [2/5], Step [3010/5158], Loss: 0.1611\nEpoch [2/5], Step [3020/5158], Loss: 0.2453\nEpoch [2/5], Step [3030/5158], Loss: 0.1848\nEpoch [2/5], Step [3040/5158], Loss: 0.1680\nEpoch [2/5], Step [3050/5158], Loss: 0.2733\nEpoch [2/5], Step [3060/5158], Loss: 0.2213\nEpoch [2/5], Step [3070/5158], Loss: 0.1955\nEpoch [2/5], Step [3080/5158], Loss: 0.1890\nEpoch [2/5], Step [3090/5158], Loss: 0.1575\nEpoch [2/5], Step [3100/5158], Loss: 0.1687\nEpoch [2/5], Step [3110/5158], Loss: 0.1784\nEpoch [2/5], Step [3120/5158], Loss: 0.1921\nEpoch [2/5], Step [3130/5158], Loss: 0.2172\nEpoch [2/5], Step [3140/5158], Loss: 0.2202\nEpoch [2/5], Step [3150/5158], Loss: 0.2229\nEpoch [2/5], Step [3160/5158], Loss: 0.1992\nEpoch [2/5], Step [3170/5158], Loss: 0.1872\nEpoch [2/5], Step [3180/5158], Loss: 0.1208\nEpoch [2/5], Step [3190/5158], Loss: 0.1991\nEpoch [2/5], Step [3200/5158], Loss: 0.2001\nEpoch [2/5], Step [3210/5158], Loss: 0.1619\nEpoch [2/5], Step [3220/5158], Loss: 0.2016\nEpoch [2/5], Step [3230/5158], Loss: 0.1978\nEpoch [2/5], Step [3240/5158], Loss: 0.1900\nEpoch [2/5], Step [3250/5158], Loss: 0.2433\nEpoch [2/5], Step [3260/5158], Loss: 0.2366\nEpoch [2/5], Step [3270/5158], Loss: 0.1637\nEpoch [2/5], Step [3280/5158], Loss: 0.2300\nEpoch [2/5], Step [3290/5158], Loss: 0.2327\nEpoch [2/5], Step [3300/5158], Loss: 0.1752\nEpoch [2/5], Step [3310/5158], Loss: 0.2046\nEpoch [2/5], Step [3320/5158], Loss: 0.2698\nEpoch [2/5], Step [3330/5158], Loss: 0.1621\nEpoch [2/5], Step [3340/5158], Loss: 0.1500\nEpoch [2/5], Step [3350/5158], Loss: 0.2347\nEpoch [2/5], Step [3360/5158], Loss: 0.2038\nEpoch [2/5], Step [3370/5158], Loss: 0.1592\nEpoch [2/5], Step [3380/5158], Loss: 0.2183\nEpoch [2/5], Step [3390/5158], Loss: 0.2107\nEpoch [2/5], Step [3400/5158], Loss: 0.2030\nEpoch [2/5], Step [3410/5158], Loss: 0.2119\nEpoch [2/5], Step [3420/5158], Loss: 0.1404\nEpoch [2/5], Step [3430/5158], Loss: 0.2125\nEpoch [2/5], Step [3440/5158], Loss: 0.1695\nEpoch [2/5], Step [3450/5158], Loss: 0.1726\nEpoch [2/5], Step [3460/5158], Loss: 0.2072\nEpoch [2/5], Step [3470/5158], Loss: 0.2421\nEpoch [2/5], Step [3480/5158], Loss: 0.2514\nEpoch [2/5], Step [3490/5158], Loss: 0.1915\nEpoch [2/5], Step [3500/5158], Loss: 0.2149\nEpoch [2/5], Step [3510/5158], Loss: 0.1975\nEpoch [2/5], Step [3520/5158], Loss: 0.2216\nEpoch [2/5], Step [3530/5158], Loss: 0.1659\nEpoch [2/5], Step [3540/5158], Loss: 0.1889\nEpoch [2/5], Step [3550/5158], Loss: 0.2806\nEpoch [2/5], Step [3560/5158], Loss: 0.1578\nEpoch [2/5], Step [3570/5158], Loss: 0.1719\nEpoch [2/5], Step [3580/5158], Loss: 0.1360\nEpoch [2/5], Step [3590/5158], Loss: 0.1952\nEpoch [2/5], Step [3600/5158], Loss: 0.2348\nEpoch [2/5], Step [3610/5158], Loss: 0.1950\nEpoch [2/5], Step [3620/5158], Loss: 0.1242\nEpoch [2/5], Step [3630/5158], Loss: 0.1740\nEpoch [2/5], Step [3640/5158], Loss: 0.2456\nEpoch [2/5], Step [3650/5158], Loss: 0.1746\nEpoch [2/5], Step [3660/5158], Loss: 0.1794\nEpoch [2/5], Step [3670/5158], Loss: 0.2233\nEpoch [2/5], Step [3680/5158], Loss: 0.2099\nEpoch [2/5], Step [3690/5158], Loss: 0.1947\nEpoch [2/5], Step [3700/5158], Loss: 0.1853\nEpoch [2/5], Step [3710/5158], Loss: 0.2457\nEpoch [2/5], Step [3720/5158], Loss: 0.1730\nEpoch [2/5], Step [3730/5158], Loss: 0.1989\nEpoch [2/5], Step [3740/5158], Loss: 0.1564\nEpoch [2/5], Step [3750/5158], Loss: 0.1633\nEpoch [2/5], Step [3760/5158], Loss: 0.1669\nEpoch [2/5], Step [3770/5158], Loss: 0.1696\nEpoch [2/5], Step [3780/5158], Loss: 0.2289\nEpoch [2/5], Step [3790/5158], Loss: 0.2391\nEpoch [2/5], Step [3800/5158], Loss: 0.1542\nEpoch [2/5], Step [3810/5158], Loss: 0.1778\nEpoch [2/5], Step [3820/5158], Loss: 0.1793\nEpoch [2/5], Step [3830/5158], Loss: 0.2203\nEpoch [2/5], Step [3840/5158], Loss: 0.2028\nEpoch [2/5], Step [3850/5158], Loss: 0.2121\nEpoch [2/5], Step [3860/5158], Loss: 0.2436\nEpoch [2/5], Step [3870/5158], Loss: 0.2213\nEpoch [2/5], Step [3880/5158], Loss: 0.2371\nEpoch [2/5], Step [3890/5158], Loss: 0.2109\nEpoch [2/5], Step [3900/5158], Loss: 0.1860\nEpoch [2/5], Step [3910/5158], Loss: 0.2082\nEpoch [2/5], Step [3920/5158], Loss: 0.2022\nEpoch [2/5], Step [3930/5158], Loss: 0.3034\nEpoch [2/5], Step [3940/5158], Loss: 0.1720\nEpoch [2/5], Step [3950/5158], Loss: 0.1999\nEpoch [2/5], Step [3960/5158], Loss: 0.2399\nEpoch [2/5], Step [3970/5158], Loss: 0.2007\nEpoch [2/5], Step [3980/5158], Loss: 0.2172\nEpoch [2/5], Step [3990/5158], Loss: 0.2019\nEpoch [2/5], Step [4000/5158], Loss: 0.2245\nEpoch [2/5], Step [4010/5158], Loss: 0.2571\nEpoch [2/5], Step [4020/5158], Loss: 0.1965\nEpoch [2/5], Step [4030/5158], Loss: 0.1469\nEpoch [2/5], Step [4040/5158], Loss: 0.1915\nEpoch [2/5], Step [4050/5158], Loss: 0.1673\nEpoch [2/5], Step [4060/5158], Loss: 0.1801\nEpoch [2/5], Step [4070/5158], Loss: 0.1914\nEpoch [2/5], Step [4080/5158], Loss: 0.2624\nEpoch [2/5], Step [4090/5158], Loss: 0.2377\nEpoch [2/5], Step [4100/5158], Loss: 0.2079\nEpoch [2/5], Step [4110/5158], Loss: 0.1818\nEpoch [2/5], Step [4120/5158], Loss: 0.2159\nEpoch [2/5], Step [4130/5158], Loss: 0.2113\nEpoch [2/5], Step [4140/5158], Loss: 0.2811\nEpoch [2/5], Step [4150/5158], Loss: 0.1498\nEpoch [2/5], Step [4160/5158], Loss: 0.2387\nEpoch [2/5], Step [4170/5158], Loss: 0.2445\nEpoch [2/5], Step [4180/5158], Loss: 0.2416\nEpoch [2/5], Step [4190/5158], Loss: 0.1663\nEpoch [2/5], Step [4200/5158], Loss: 0.1635\nEpoch [2/5], Step [4210/5158], Loss: 0.2259\nEpoch [2/5], Step [4220/5158], Loss: 0.2117\nEpoch [2/5], Step [4230/5158], Loss: 0.1792\nEpoch [2/5], Step [4240/5158], Loss: 0.1660\nEpoch [2/5], Step [4250/5158], Loss: 0.1637\nEpoch [2/5], Step [4260/5158], Loss: 0.2126\nEpoch [2/5], Step [4270/5158], Loss: 0.1664\nEpoch [2/5], Step [4280/5158], Loss: 0.1737\nEpoch [2/5], Step [4290/5158], Loss: 0.2153\nEpoch [2/5], Step [4300/5158], Loss: 0.2273\nEpoch [2/5], Step [4310/5158], Loss: 0.2513\nEpoch [2/5], Step [4320/5158], Loss: 0.2425\nEpoch [2/5], Step [4330/5158], Loss: 0.2150\nEpoch [2/5], Step [4340/5158], Loss: 0.1361\nEpoch [2/5], Step [4350/5158], Loss: 0.2047\nEpoch [2/5], Step [4360/5158], Loss: 0.1947\nEpoch [2/5], Step [4370/5158], Loss: 0.1960\nEpoch [2/5], Step [4380/5158], Loss: 0.2135\nEpoch [2/5], Step [4390/5158], Loss: 0.1810\nEpoch [2/5], Step [4400/5158], Loss: 0.2364\nEpoch [2/5], Step [4410/5158], Loss: 0.1298\nEpoch [2/5], Step [4420/5158], Loss: 0.1793\nEpoch [2/5], Step [4430/5158], Loss: 0.1738\nEpoch [2/5], Step [4440/5158], Loss: 0.2011\nEpoch [2/5], Step [4450/5158], Loss: 0.2043\nEpoch [2/5], Step [4460/5158], Loss: 0.1591\nEpoch [2/5], Step [4470/5158], Loss: 0.2241\nEpoch [2/5], Step [4480/5158], Loss: 0.2242\nEpoch [2/5], Step [4490/5158], Loss: 0.2252\nEpoch [2/5], Step [4500/5158], Loss: 0.2288\nEpoch [2/5], Step [4510/5158], Loss: 0.1484\nEpoch [2/5], Step [4520/5158], Loss: 0.2662\nEpoch [2/5], Step [4530/5158], Loss: 0.1904\nEpoch [2/5], Step [4540/5158], Loss: 0.1957\nEpoch [2/5], Step [4550/5158], Loss: 0.2077\nEpoch [2/5], Step [4560/5158], Loss: 0.2086\nEpoch [2/5], Step [4570/5158], Loss: 0.2317\nEpoch [2/5], Step [4580/5158], Loss: 0.1326\nEpoch [2/5], Step [4590/5158], Loss: 0.2470\nEpoch [2/5], Step [4600/5158], Loss: 0.1880\nEpoch [2/5], Step [4610/5158], Loss: 0.1844\nEpoch [2/5], Step [4620/5158], Loss: 0.1777\nEpoch [2/5], Step [4630/5158], Loss: 0.2079\nEpoch [2/5], Step [4640/5158], Loss: 0.1690\nEpoch [2/5], Step [4650/5158], Loss: 0.2169\nEpoch [2/5], Step [4660/5158], Loss: 0.1994\nEpoch [2/5], Step [4670/5158], Loss: 0.2194\nEpoch [2/5], Step [4680/5158], Loss: 0.2327\nEpoch [2/5], Step [4690/5158], Loss: 0.2844\nEpoch [2/5], Step [4700/5158], Loss: 0.1764\nEpoch [2/5], Step [4710/5158], Loss: 0.1363\nEpoch [2/5], Step [4720/5158], Loss: 0.1835\nEpoch [2/5], Step [4730/5158], Loss: 0.1705\nEpoch [2/5], Step [4740/5158], Loss: 0.1894\nEpoch [2/5], Step [4750/5158], Loss: 0.2203\nEpoch [2/5], Step [4760/5158], Loss: 0.2489\nEpoch [2/5], Step [4770/5158], Loss: 0.1565\nEpoch [2/5], Step [4780/5158], Loss: 0.1852\nEpoch [2/5], Step [4790/5158], Loss: 0.2389\nEpoch [2/5], Step [4800/5158], Loss: 0.1922\nEpoch [2/5], Step [4810/5158], Loss: 0.2152\nEpoch [2/5], Step [4820/5158], Loss: 0.1512\nEpoch [2/5], Step [4830/5158], Loss: 0.2279\nEpoch [2/5], Step [4840/5158], Loss: 0.2230\nEpoch [2/5], Step [4850/5158], Loss: 0.2125\nEpoch [2/5], Step [4860/5158], Loss: 0.2222\nEpoch [2/5], Step [4870/5158], Loss: 0.1755\nEpoch [2/5], Step [4880/5158], Loss: 0.2610\nEpoch [2/5], Step [4890/5158], Loss: 0.1849\nEpoch [2/5], Step [4900/5158], Loss: 0.1612\nEpoch [2/5], Step [4910/5158], Loss: 0.1653\nEpoch [2/5], Step [4920/5158], Loss: 0.1991\nEpoch [2/5], Step [4930/5158], Loss: 0.1938\nEpoch [2/5], Step [4940/5158], Loss: 0.2596\nEpoch [2/5], Step [4950/5158], Loss: 0.2176\nEpoch [2/5], Step [4960/5158], Loss: 0.2019\nEpoch [2/5], Step [4970/5158], Loss: 0.2063\nEpoch [2/5], Step [4980/5158], Loss: 0.2139\nEpoch [2/5], Step [4990/5158], Loss: 0.2432\nEpoch [2/5], Step [5000/5158], Loss: 0.2175\nEpoch [2/5], Step [5010/5158], Loss: 0.2238\nEpoch [2/5], Step [5020/5158], Loss: 0.2058\nEpoch [2/5], Step [5030/5158], Loss: 0.1831\nEpoch [2/5], Step [5040/5158], Loss: 0.2435\nEpoch [2/5], Step [5050/5158], Loss: 0.2397\nEpoch [2/5], Step [5060/5158], Loss: 0.2262\nEpoch [2/5], Step [5070/5158], Loss: 0.1875\nEpoch [2/5], Step [5080/5158], Loss: 0.1924\nEpoch [2/5], Step [5090/5158], Loss: 0.1558\nEpoch [2/5], Step [5100/5158], Loss: 0.1400\nEpoch [2/5], Step [5110/5158], Loss: 0.2213\nEpoch [2/5], Step [5120/5158], Loss: 0.2008\nEpoch [2/5], Step [5130/5158], Loss: 0.1543\nEpoch [2/5], Step [5140/5158], Loss: 0.2266\nEpoch [2/5], Step [5150/5158], Loss: 0.1929\nEpoch [3/5], Step [10/5158], Loss: 0.1469\nEpoch [3/5], Step [20/5158], Loss: 0.2254\nEpoch [3/5], Step [30/5158], Loss: 0.1759\nEpoch [3/5], Step [40/5158], Loss: 0.1449\nEpoch [3/5], Step [50/5158], Loss: 0.1580\nEpoch [3/5], Step [60/5158], Loss: 0.1912\nEpoch [3/5], Step [70/5158], Loss: 0.1698\nEpoch [3/5], Step [80/5158], Loss: 0.1622\nEpoch [3/5], Step [90/5158], Loss: 0.1307\nEpoch [3/5], Step [100/5158], Loss: 0.1778\nEpoch [3/5], Step [110/5158], Loss: 0.1427\nEpoch [3/5], Step [120/5158], Loss: 0.2053\nEpoch [3/5], Step [130/5158], Loss: 0.1917\nEpoch [3/5], Step [140/5158], Loss: 0.1721\nEpoch [3/5], Step [150/5158], Loss: 0.1609\nEpoch [3/5], Step [160/5158], Loss: 0.1673\nEpoch [3/5], Step [170/5158], Loss: 0.1436\nEpoch [3/5], Step [180/5158], Loss: 0.1876\nEpoch [3/5], Step [190/5158], Loss: 0.1667\nEpoch [3/5], Step [200/5158], Loss: 0.1485\nEpoch [3/5], Step [210/5158], Loss: 0.1475\nEpoch [3/5], Step [220/5158], Loss: 0.1769\nEpoch [3/5], Step [230/5158], Loss: 0.2167\nEpoch [3/5], Step [240/5158], Loss: 0.2409\nEpoch [3/5], Step [250/5158], Loss: 0.1637\nEpoch [3/5], Step [260/5158], Loss: 0.1361\nEpoch [3/5], Step [270/5158], Loss: 0.1655\nEpoch [3/5], Step [280/5158], Loss: 0.1240\nEpoch [3/5], Step [290/5158], Loss: 0.1551\nEpoch [3/5], Step [300/5158], Loss: 0.1876\nEpoch [3/5], Step [310/5158], Loss: 0.1686\nEpoch [3/5], Step [320/5158], Loss: 0.1621\nEpoch [3/5], Step [330/5158], Loss: 0.1852\nEpoch [3/5], Step [340/5158], Loss: 0.2004\nEpoch [3/5], Step [350/5158], Loss: 0.1427\nEpoch [3/5], Step [360/5158], Loss: 0.2011\nEpoch [3/5], Step [370/5158], Loss: 0.1660\nEpoch [3/5], Step [380/5158], Loss: 0.1699\nEpoch [3/5], Step [390/5158], Loss: 0.1680\nEpoch [3/5], Step [400/5158], Loss: 0.1841\nEpoch [3/5], Step [410/5158], Loss: 0.1395\nEpoch [3/5], Step [420/5158], Loss: 0.2227\nEpoch [3/5], Step [430/5158], Loss: 0.1711\nEpoch [3/5], Step [440/5158], Loss: 0.2054\nEpoch [3/5], Step [450/5158], Loss: 0.1664\nEpoch [3/5], Step [460/5158], Loss: 0.1166\nEpoch [3/5], Step [470/5158], Loss: 0.1603\nEpoch [3/5], Step [480/5158], Loss: 0.1941\nEpoch [3/5], Step [490/5158], Loss: 0.2110\nEpoch [3/5], Step [500/5158], Loss: 0.2114\nEpoch [3/5], Step [510/5158], Loss: 0.1850\nEpoch [3/5], Step [520/5158], Loss: 0.2153\nEpoch [3/5], Step [530/5158], Loss: 0.1575\nEpoch [3/5], Step [540/5158], Loss: 0.1734\nEpoch [3/5], Step [550/5158], Loss: 0.1177\nEpoch [3/5], Step [560/5158], Loss: 0.1859\nEpoch [3/5], Step [570/5158], Loss: 0.1619\nEpoch [3/5], Step [580/5158], Loss: 0.1610\nEpoch [3/5], Step [590/5158], Loss: 0.2540\nEpoch [3/5], Step [600/5158], Loss: 0.1255\nEpoch [3/5], Step [610/5158], Loss: 0.1513\nEpoch [3/5], Step [620/5158], Loss: 0.1596\nEpoch [3/5], Step [630/5158], Loss: 0.1250\nEpoch [3/5], Step [640/5158], Loss: 0.1871\nEpoch [3/5], Step [650/5158], Loss: 0.1899\nEpoch [3/5], Step [660/5158], Loss: 0.2239\nEpoch [3/5], Step [670/5158], Loss: 0.1584\nEpoch [3/5], Step [680/5158], Loss: 0.1894\nEpoch [3/5], Step [690/5158], Loss: 0.1821\nEpoch [3/5], Step [700/5158], Loss: 0.1463\nEpoch [3/5], Step [710/5158], Loss: 0.1793\nEpoch [3/5], Step [720/5158], Loss: 0.1872\nEpoch [3/5], Step [730/5158], Loss: 0.1307\nEpoch [3/5], Step [740/5158], Loss: 0.1621\nEpoch [3/5], Step [750/5158], Loss: 0.1447\nEpoch [3/5], Step [760/5158], Loss: 0.1979\nEpoch [3/5], Step [770/5158], Loss: 0.1347\nEpoch [3/5], Step [780/5158], Loss: 0.2078\nEpoch [3/5], Step [790/5158], Loss: 0.1405\nEpoch [3/5], Step [800/5158], Loss: 0.1447\nEpoch [3/5], Step [810/5158], Loss: 0.1298\nEpoch [3/5], Step [820/5158], Loss: 0.1612\nEpoch [3/5], Step [830/5158], Loss: 0.1808\nEpoch [3/5], Step [840/5158], Loss: 0.1653\nEpoch [3/5], Step [850/5158], Loss: 0.1589\nEpoch [3/5], Step [860/5158], Loss: 0.2659\nEpoch [3/5], Step [870/5158], Loss: 0.1929\nEpoch [3/5], Step [880/5158], Loss: 0.1418\nEpoch [3/5], Step [890/5158], Loss: 0.1737\nEpoch [3/5], Step [900/5158], Loss: 0.1937\nEpoch [3/5], Step [910/5158], Loss: 0.1379\nEpoch [3/5], Step [920/5158], Loss: 0.1548\nEpoch [3/5], Step [930/5158], Loss: 0.1582\nEpoch [3/5], Step [940/5158], Loss: 0.1409\nEpoch [3/5], Step [950/5158], Loss: 0.1572\nEpoch [3/5], Step [960/5158], Loss: 0.2661\nEpoch [3/5], Step [970/5158], Loss: 0.2681\nEpoch [3/5], Step [980/5158], Loss: 0.2117\nEpoch [3/5], Step [990/5158], Loss: 0.1631\nEpoch [3/5], Step [1000/5158], Loss: 0.1650\nEpoch [3/5], Step [1010/5158], Loss: 0.1201\nEpoch [3/5], Step [1020/5158], Loss: 0.1626\nEpoch [3/5], Step [1030/5158], Loss: 0.1616\nEpoch [3/5], Step [1040/5158], Loss: 0.1732\nEpoch [3/5], Step [1050/5158], Loss: 0.1520\nEpoch [3/5], Step [1060/5158], Loss: 0.1473\nEpoch [3/5], Step [1070/5158], Loss: 0.1859\nEpoch [3/5], Step [1080/5158], Loss: 0.1586\nEpoch [3/5], Step [1090/5158], Loss: 0.2004\nEpoch [3/5], Step [1100/5158], Loss: 0.1432\nEpoch [3/5], Step [1110/5158], Loss: 0.1802\nEpoch [3/5], Step [1120/5158], Loss: 0.1733\nEpoch [3/5], Step [1130/5158], Loss: 0.2093\nEpoch [3/5], Step [1140/5158], Loss: 0.1340\nEpoch [3/5], Step [1150/5158], Loss: 0.1540\nEpoch [3/5], Step [1160/5158], Loss: 0.2078\nEpoch [3/5], Step [1170/5158], Loss: 0.1332\nEpoch [3/5], Step [1180/5158], Loss: 0.0953\nEpoch [3/5], Step [1190/5158], Loss: 0.1535\nEpoch [3/5], Step [1200/5158], Loss: 0.1888\nEpoch [3/5], Step [1210/5158], Loss: 0.1650\nEpoch [3/5], Step [1220/5158], Loss: 0.1914\nEpoch [3/5], Step [1230/5158], Loss: 0.1767\nEpoch [3/5], Step [1240/5158], Loss: 0.1966\nEpoch [3/5], Step [1250/5158], Loss: 0.1850\nEpoch [3/5], Step [1260/5158], Loss: 0.1931\nEpoch [3/5], Step [1270/5158], Loss: 0.1204\nEpoch [3/5], Step [1280/5158], Loss: 0.1559\nEpoch [3/5], Step [1290/5158], Loss: 0.1779\nEpoch [3/5], Step [1300/5158], Loss: 0.1664\nEpoch [3/5], Step [1310/5158], Loss: 0.1684\nEpoch [3/5], Step [1320/5158], Loss: 0.1657\nEpoch [3/5], Step [1330/5158], Loss: 0.2022\nEpoch [3/5], Step [1340/5158], Loss: 0.1453\nEpoch [3/5], Step [1350/5158], Loss: 0.1819\nEpoch [3/5], Step [1360/5158], Loss: 0.1852\nEpoch [3/5], Step [1370/5158], Loss: 0.1601\nEpoch [3/5], Step [1380/5158], Loss: 0.1833\nEpoch [3/5], Step [1390/5158], Loss: 0.1414\nEpoch [3/5], Step [1400/5158], Loss: 0.1313\nEpoch [3/5], Step [1410/5158], Loss: 0.1843\nEpoch [3/5], Step [1420/5158], Loss: 0.2517\nEpoch [3/5], Step [1430/5158], Loss: 0.1440\nEpoch [3/5], Step [1440/5158], Loss: 0.2250\nEpoch [3/5], Step [1450/5158], Loss: 0.1940\nEpoch [3/5], Step [1460/5158], Loss: 0.1900\nEpoch [3/5], Step [1470/5158], Loss: 0.1220\nEpoch [3/5], Step [1480/5158], Loss: 0.1528\nEpoch [3/5], Step [1490/5158], Loss: 0.1519\nEpoch [3/5], Step [1500/5158], Loss: 0.1512\nEpoch [3/5], Step [1510/5158], Loss: 0.1759\nEpoch [3/5], Step [1520/5158], Loss: 0.1241\nEpoch [3/5], Step [1530/5158], Loss: 0.1792\nEpoch [3/5], Step [1540/5158], Loss: 0.1542\nEpoch [3/5], Step [1550/5158], Loss: 0.1688\nEpoch [3/5], Step [1560/5158], Loss: 0.2017\nEpoch [3/5], Step [1570/5158], Loss: 0.2140\nEpoch [3/5], Step [1580/5158], Loss: 0.1668\nEpoch [3/5], Step [1590/5158], Loss: 0.1789\nEpoch [3/5], Step [1600/5158], Loss: 0.1910\nEpoch [3/5], Step [1610/5158], Loss: 0.1720\nEpoch [3/5], Step [1620/5158], Loss: 0.1774\nEpoch [3/5], Step [1630/5158], Loss: 0.1867\nEpoch [3/5], Step [1640/5158], Loss: 0.2246\nEpoch [3/5], Step [1650/5158], Loss: 0.1201\nEpoch [3/5], Step [1660/5158], Loss: 0.1724\nEpoch [3/5], Step [1670/5158], Loss: 0.1451\nEpoch [3/5], Step [1680/5158], Loss: 0.1463\nEpoch [3/5], Step [1690/5158], Loss: 0.1531\nEpoch [3/5], Step [1700/5158], Loss: 0.1344\nEpoch [3/5], Step [1710/5158], Loss: 0.1773\nEpoch [3/5], Step [1720/5158], Loss: 0.1464\nEpoch [3/5], Step [1730/5158], Loss: 0.1909\nEpoch [3/5], Step [1740/5158], Loss: 0.1357\nEpoch [3/5], Step [1750/5158], Loss: 0.2028\nEpoch [3/5], Step [1760/5158], Loss: 0.1722\nEpoch [3/5], Step [1770/5158], Loss: 0.2114\nEpoch [3/5], Step [1780/5158], Loss: 0.1859\nEpoch [3/5], Step [1790/5158], Loss: 0.1671\nEpoch [3/5], Step [1800/5158], Loss: 0.2012\nEpoch [3/5], Step [1810/5158], Loss: 0.1620\nEpoch [3/5], Step [1820/5158], Loss: 0.1757\nEpoch [3/5], Step [1830/5158], Loss: 0.1505\nEpoch [3/5], Step [1840/5158], Loss: 0.1818\nEpoch [3/5], Step [1850/5158], Loss: 0.1373\nEpoch [3/5], Step [1860/5158], Loss: 0.1542\nEpoch [3/5], Step [1870/5158], Loss: 0.1444\nEpoch [3/5], Step [1880/5158], Loss: 0.1970\nEpoch [3/5], Step [1890/5158], Loss: 0.1365\nEpoch [3/5], Step [1900/5158], Loss: 0.1975\nEpoch [3/5], Step [1910/5158], Loss: 0.2278\nEpoch [3/5], Step [1920/5158], Loss: 0.1465\nEpoch [3/5], Step [1930/5158], Loss: 0.1387\nEpoch [3/5], Step [1940/5158], Loss: 0.2135\nEpoch [3/5], Step [1950/5158], Loss: 0.2145\nEpoch [3/5], Step [1960/5158], Loss: 0.1625\nEpoch [3/5], Step [1970/5158], Loss: 0.1651\nEpoch [3/5], Step [1980/5158], Loss: 0.1297\nEpoch [3/5], Step [1990/5158], Loss: 0.1836\nEpoch [3/5], Step [2000/5158], Loss: 0.1872\nEpoch [3/5], Step [2010/5158], Loss: 0.1723\nEpoch [3/5], Step [2020/5158], Loss: 0.1474\nEpoch [3/5], Step [2030/5158], Loss: 0.1611\nEpoch [3/5], Step [2040/5158], Loss: 0.1715\nEpoch [3/5], Step [2050/5158], Loss: 0.1772\nEpoch [3/5], Step [2060/5158], Loss: 0.1549\nEpoch [3/5], Step [2070/5158], Loss: 0.1320\nEpoch [3/5], Step [2080/5158], Loss: 0.2197\nEpoch [3/5], Step [2090/5158], Loss: 0.2036\nEpoch [3/5], Step [2100/5158], Loss: 0.2093\nEpoch [3/5], Step [2110/5158], Loss: 0.2032\nEpoch [3/5], Step [2120/5158], Loss: 0.1788\nEpoch [3/5], Step [2130/5158], Loss: 0.2087\nEpoch [3/5], Step [2140/5158], Loss: 0.1732\nEpoch [3/5], Step [2150/5158], Loss: 0.1561\nEpoch [3/5], Step [2160/5158], Loss: 0.1665\nEpoch [3/5], Step [2170/5158], Loss: 0.1235\nEpoch [3/5], Step [2180/5158], Loss: 0.1152\nEpoch [3/5], Step [2190/5158], Loss: 0.1901\nEpoch [3/5], Step [2200/5158], Loss: 0.1853\nEpoch [3/5], Step [2210/5158], Loss: 0.1766\nEpoch [3/5], Step [2220/5158], Loss: 0.1996\nEpoch [3/5], Step [2230/5158], Loss: 0.1994\nEpoch [3/5], Step [2240/5158], Loss: 0.1317\nEpoch [3/5], Step [2250/5158], Loss: 0.1049\nEpoch [3/5], Step [2260/5158], Loss: 0.2284\nEpoch [3/5], Step [2270/5158], Loss: 0.1192\nEpoch [3/5], Step [2280/5158], Loss: 0.2385\nEpoch [3/5], Step [2290/5158], Loss: 0.1509\nEpoch [3/5], Step [2300/5158], Loss: 0.1303\nEpoch [3/5], Step [2310/5158], Loss: 0.1921\nEpoch [3/5], Step [2320/5158], Loss: 0.2009\nEpoch [3/5], Step [2330/5158], Loss: 0.1651\nEpoch [3/5], Step [2340/5158], Loss: 0.2032\nEpoch [3/5], Step [2350/5158], Loss: 0.1900\nEpoch [3/5], Step [2360/5158], Loss: 0.1890\nEpoch [3/5], Step [2370/5158], Loss: 0.1988\nEpoch [3/5], Step [2380/5158], Loss: 0.1607\nEpoch [3/5], Step [2390/5158], Loss: 0.1594\nEpoch [3/5], Step [2400/5158], Loss: 0.1169\nEpoch [3/5], Step [2410/5158], Loss: 0.1730\nEpoch [3/5], Step [2420/5158], Loss: 0.2451\nEpoch [3/5], Step [2430/5158], Loss: 0.1818\nEpoch [3/5], Step [2440/5158], Loss: 0.1936\nEpoch [3/5], Step [2450/5158], Loss: 0.2030\nEpoch [3/5], Step [2460/5158], Loss: 0.1507\nEpoch [3/5], Step [2470/5158], Loss: 0.1988\nEpoch [3/5], Step [2480/5158], Loss: 0.1952\nEpoch [3/5], Step [2490/5158], Loss: 0.1484\nEpoch [3/5], Step [2500/5158], Loss: 0.1792\nEpoch [3/5], Step [2510/5158], Loss: 0.1505\nEpoch [3/5], Step [2520/5158], Loss: 0.1777\nEpoch [3/5], Step [2530/5158], Loss: 0.2148\nEpoch [3/5], Step [2540/5158], Loss: 0.1203\nEpoch [3/5], Step [2550/5158], Loss: 0.1771\nEpoch [3/5], Step [2560/5158], Loss: 0.1277\nEpoch [3/5], Step [2570/5158], Loss: 0.1741\nEpoch [3/5], Step [2580/5158], Loss: 0.2203\nEpoch [3/5], Step [2590/5158], Loss: 0.1631\nEpoch [3/5], Step [2600/5158], Loss: 0.2322\nEpoch [3/5], Step [2610/5158], Loss: 0.1403\nEpoch [3/5], Step [2620/5158], Loss: 0.1498\nEpoch [3/5], Step [2630/5158], Loss: 0.1265\nEpoch [3/5], Step [2640/5158], Loss: 0.1567\nEpoch [3/5], Step [2650/5158], Loss: 0.1084\nEpoch [3/5], Step [2660/5158], Loss: 0.1157\nEpoch [3/5], Step [2670/5158], Loss: 0.1761\nEpoch [3/5], Step [2680/5158], Loss: 0.1487\nEpoch [3/5], Step [2690/5158], Loss: 0.1177\nEpoch [3/5], Step [2700/5158], Loss: 0.2234\nEpoch [3/5], Step [2710/5158], Loss: 0.1855\nEpoch [3/5], Step [2720/5158], Loss: 0.2135\nEpoch [3/5], Step [2730/5158], Loss: 0.1458\nEpoch [3/5], Step [2740/5158], Loss: 0.1524\nEpoch [3/5], Step [2750/5158], Loss: 0.2042\nEpoch [3/5], Step [2760/5158], Loss: 0.1102\nEpoch [3/5], Step [2770/5158], Loss: 0.1781\nEpoch [3/5], Step [2780/5158], Loss: 0.2375\nEpoch [3/5], Step [2790/5158], Loss: 0.1424\nEpoch [3/5], Step [2800/5158], Loss: 0.1765\nEpoch [3/5], Step [2810/5158], Loss: 0.1666\nEpoch [3/5], Step [2820/5158], Loss: 0.1817\nEpoch [3/5], Step [2830/5158], Loss: 0.1765\nEpoch [3/5], Step [2840/5158], Loss: 0.1481\nEpoch [3/5], Step [2850/5158], Loss: 0.1295\nEpoch [3/5], Step [2860/5158], Loss: 0.1875\nEpoch [3/5], Step [2870/5158], Loss: 0.1512\nEpoch [3/5], Step [2880/5158], Loss: 0.1250\nEpoch [3/5], Step [2890/5158], Loss: 0.1512\nEpoch [3/5], Step [2900/5158], Loss: 0.2021\nEpoch [3/5], Step [2910/5158], Loss: 0.1905\nEpoch [3/5], Step [2920/5158], Loss: 0.1819\nEpoch [3/5], Step [2930/5158], Loss: 0.1607\nEpoch [3/5], Step [2940/5158], Loss: 0.1900\nEpoch [3/5], Step [2950/5158], Loss: 0.1622\nEpoch [3/5], Step [2960/5158], Loss: 0.1892\nEpoch [3/5], Step [2970/5158], Loss: 0.1660\nEpoch [3/5], Step [2980/5158], Loss: 0.1805\nEpoch [3/5], Step [2990/5158], Loss: 0.2027\nEpoch [3/5], Step [3000/5158], Loss: 0.1288\nEpoch [3/5], Step [3010/5158], Loss: 0.1949\nEpoch [3/5], Step [3020/5158], Loss: 0.2089\nEpoch [3/5], Step [3030/5158], Loss: 0.1302\nEpoch [3/5], Step [3040/5158], Loss: 0.1342\nEpoch [3/5], Step [3050/5158], Loss: 0.1737\nEpoch [3/5], Step [3060/5158], Loss: 0.1155\nEpoch [3/5], Step [3070/5158], Loss: 0.1543\nEpoch [3/5], Step [3080/5158], Loss: 0.1467\nEpoch [3/5], Step [3090/5158], Loss: 0.1621\nEpoch [3/5], Step [3100/5158], Loss: 0.1768\nEpoch [3/5], Step [3110/5158], Loss: 0.1553\nEpoch [3/5], Step [3120/5158], Loss: 0.1477\nEpoch [3/5], Step [3130/5158], Loss: 0.1411\nEpoch [3/5], Step [3140/5158], Loss: 0.1775\nEpoch [3/5], Step [3150/5158], Loss: 0.1128\nEpoch [3/5], Step [3160/5158], Loss: 0.2042\nEpoch [3/5], Step [3170/5158], Loss: 0.1604\nEpoch [3/5], Step [3180/5158], Loss: 0.1538\nEpoch [3/5], Step [3190/5158], Loss: 0.1359\nEpoch [3/5], Step [3200/5158], Loss: 0.1740\nEpoch [3/5], Step [3210/5158], Loss: 0.2007\nEpoch [3/5], Step [3220/5158], Loss: 0.1920\nEpoch [3/5], Step [3230/5158], Loss: 0.1377\nEpoch [3/5], Step [3240/5158], Loss: 0.2177\nEpoch [3/5], Step [3250/5158], Loss: 0.1111\nEpoch [3/5], Step [3260/5158], Loss: 0.1794\nEpoch [3/5], Step [3270/5158], Loss: 0.1893\nEpoch [3/5], Step [3280/5158], Loss: 0.1662\nEpoch [3/5], Step [3290/5158], Loss: 0.1623\nEpoch [3/5], Step [3300/5158], Loss: 0.1437\nEpoch [3/5], Step [3310/5158], Loss: 0.1557\nEpoch [3/5], Step [3320/5158], Loss: 0.2230\nEpoch [3/5], Step [3330/5158], Loss: 0.2048\nEpoch [3/5], Step [3340/5158], Loss: 0.0926\nEpoch [3/5], Step [3350/5158], Loss: 0.2122\nEpoch [3/5], Step [3360/5158], Loss: 0.2880\nEpoch [3/5], Step [3370/5158], Loss: 0.1621\nEpoch [3/5], Step [3380/5158], Loss: 0.1360\nEpoch [3/5], Step [3390/5158], Loss: 0.1741\nEpoch [3/5], Step [3400/5158], Loss: 0.2099\nEpoch [3/5], Step [3410/5158], Loss: 0.1949\nEpoch [3/5], Step [3420/5158], Loss: 0.1708\nEpoch [3/5], Step [3430/5158], Loss: 0.1534\nEpoch [3/5], Step [3440/5158], Loss: 0.1542\nEpoch [3/5], Step [3450/5158], Loss: 0.1360\nEpoch [3/5], Step [3460/5158], Loss: 0.1764\nEpoch [3/5], Step [3470/5158], Loss: 0.1888\nEpoch [3/5], Step [3480/5158], Loss: 0.1309\nEpoch [3/5], Step [3490/5158], Loss: 0.1795\nEpoch [3/5], Step [3500/5158], Loss: 0.1254\nEpoch [3/5], Step [3510/5158], Loss: 0.1789\nEpoch [3/5], Step [3520/5158], Loss: 0.1340\nEpoch [3/5], Step [3530/5158], Loss: 0.1842\nEpoch [3/5], Step [3540/5158], Loss: 0.2040\nEpoch [3/5], Step [3550/5158], Loss: 0.1938\nEpoch [3/5], Step [3560/5158], Loss: 0.1211\nEpoch [3/5], Step [3570/5158], Loss: 0.1652\nEpoch [3/5], Step [3580/5158], Loss: 0.1986\nEpoch [3/5], Step [3590/5158], Loss: 0.1679\nEpoch [3/5], Step [3600/5158], Loss: 0.1327\nEpoch [3/5], Step [3610/5158], Loss: 0.1830\nEpoch [3/5], Step [3620/5158], Loss: 0.1750\nEpoch [3/5], Step [3630/5158], Loss: 0.1949\nEpoch [3/5], Step [3640/5158], Loss: 0.2044\nEpoch [3/5], Step [3650/5158], Loss: 0.1936\nEpoch [3/5], Step [3660/5158], Loss: 0.1586\nEpoch [3/5], Step [3670/5158], Loss: 0.2063\nEpoch [3/5], Step [3680/5158], Loss: 0.1699\nEpoch [3/5], Step [3690/5158], Loss: 0.1219\nEpoch [3/5], Step [3700/5158], Loss: 0.1731\nEpoch [3/5], Step [3710/5158], Loss: 0.1583\nEpoch [3/5], Step [3720/5158], Loss: 0.1820\nEpoch [3/5], Step [3730/5158], Loss: 0.1824\nEpoch [3/5], Step [3740/5158], Loss: 0.1749\nEpoch [3/5], Step [3750/5158], Loss: 0.1666\nEpoch [3/5], Step [3760/5158], Loss: 0.2128\nEpoch [3/5], Step [3770/5158], Loss: 0.1691\nEpoch [3/5], Step [3780/5158], Loss: 0.1875\nEpoch [3/5], Step [3790/5158], Loss: 0.2021\nEpoch [3/5], Step [3800/5158], Loss: 0.1308\nEpoch [3/5], Step [3810/5158], Loss: 0.1665\nEpoch [3/5], Step [3820/5158], Loss: 0.1446\nEpoch [3/5], Step [3830/5158], Loss: 0.2285\nEpoch [3/5], Step [3840/5158], Loss: 0.1473\nEpoch [3/5], Step [3850/5158], Loss: 0.1755\nEpoch [3/5], Step [3860/5158], Loss: 0.1842\nEpoch [3/5], Step [3870/5158], Loss: 0.1750\nEpoch [3/5], Step [3880/5158], Loss: 0.1421\nEpoch [3/5], Step [3890/5158], Loss: 0.1296\nEpoch [3/5], Step [3900/5158], Loss: 0.1945\nEpoch [3/5], Step [3910/5158], Loss: 0.1433\nEpoch [3/5], Step [3920/5158], Loss: 0.2004\nEpoch [3/5], Step [3930/5158], Loss: 0.1771\nEpoch [3/5], Step [3940/5158], Loss: 0.1461\nEpoch [3/5], Step [3950/5158], Loss: 0.1388\nEpoch [3/5], Step [3960/5158], Loss: 0.1623\nEpoch [3/5], Step [3970/5158], Loss: 0.1695\nEpoch [3/5], Step [3980/5158], Loss: 0.1884\nEpoch [3/5], Step [3990/5158], Loss: 0.1655\nEpoch [3/5], Step [4000/5158], Loss: 0.2271\nEpoch [3/5], Step [4010/5158], Loss: 0.1404\nEpoch [3/5], Step [4020/5158], Loss: 0.1655\nEpoch [3/5], Step [4030/5158], Loss: 0.1879\nEpoch [3/5], Step [4040/5158], Loss: 0.1555\nEpoch [3/5], Step [4050/5158], Loss: 0.1983\nEpoch [3/5], Step [4060/5158], Loss: 0.1816\nEpoch [3/5], Step [4070/5158], Loss: 0.1229\nEpoch [3/5], Step [4080/5158], Loss: 0.1389\nEpoch [3/5], Step [4090/5158], Loss: 0.1752\nEpoch [3/5], Step [4100/5158], Loss: 0.2007\nEpoch [3/5], Step [4110/5158], Loss: 0.2066\nEpoch [3/5], Step [4120/5158], Loss: 0.1537\nEpoch [3/5], Step [4130/5158], Loss: 0.1637\nEpoch [3/5], Step [4140/5158], Loss: 0.2251\nEpoch [3/5], Step [4150/5158], Loss: 0.2025\nEpoch [3/5], Step [4160/5158], Loss: 0.1722\nEpoch [3/5], Step [4170/5158], Loss: 0.1527\nEpoch [3/5], Step [4180/5158], Loss: 0.1501\nEpoch [3/5], Step [4190/5158], Loss: 0.1524\nEpoch [3/5], Step [4200/5158], Loss: 0.1218\nEpoch [3/5], Step [4210/5158], Loss: 0.2010\nEpoch [3/5], Step [4220/5158], Loss: 0.1689\nEpoch [3/5], Step [4230/5158], Loss: 0.2036\nEpoch [3/5], Step [4240/5158], Loss: 0.2096\nEpoch [3/5], Step [4250/5158], Loss: 0.2153\nEpoch [3/5], Step [4260/5158], Loss: 0.2035\nEpoch [3/5], Step [4270/5158], Loss: 0.1493\nEpoch [3/5], Step [4280/5158], Loss: 0.1839\nEpoch [3/5], Step [4290/5158], Loss: 0.1436\nEpoch [3/5], Step [4300/5158], Loss: 0.1982\nEpoch [3/5], Step [4310/5158], Loss: 0.1763\nEpoch [3/5], Step [4320/5158], Loss: 0.2221\nEpoch [3/5], Step [4330/5158], Loss: 0.2296\nEpoch [3/5], Step [4340/5158], Loss: 0.1836\nEpoch [3/5], Step [4350/5158], Loss: 0.1472\nEpoch [3/5], Step [4360/5158], Loss: 0.1587\nEpoch [3/5], Step [4370/5158], Loss: 0.1652\nEpoch [3/5], Step [4380/5158], Loss: 0.2156\nEpoch [3/5], Step [4390/5158], Loss: 0.1673\nEpoch [3/5], Step [4400/5158], Loss: 0.1657\nEpoch [3/5], Step [4410/5158], Loss: 0.1836\nEpoch [3/5], Step [4420/5158], Loss: 0.2021\nEpoch [3/5], Step [4430/5158], Loss: 0.1997\nEpoch [3/5], Step [4440/5158], Loss: 0.1944\nEpoch [3/5], Step [4450/5158], Loss: 0.1412\nEpoch [3/5], Step [4460/5158], Loss: 0.1258\nEpoch [3/5], Step [4470/5158], Loss: 0.1694\nEpoch [3/5], Step [4480/5158], Loss: 0.2308\nEpoch [3/5], Step [4490/5158], Loss: 0.1824\nEpoch [3/5], Step [4500/5158], Loss: 0.1819\nEpoch [3/5], Step [4510/5158], Loss: 0.1675\nEpoch [3/5], Step [4520/5158], Loss: 0.1975\nEpoch [3/5], Step [4530/5158], Loss: 0.1939\nEpoch [3/5], Step [4540/5158], Loss: 0.1665\nEpoch [3/5], Step [4550/5158], Loss: 0.1485\nEpoch [3/5], Step [4560/5158], Loss: 0.1217\nEpoch [3/5], Step [4570/5158], Loss: 0.1992\nEpoch [3/5], Step [4580/5158], Loss: 0.1464\nEpoch [3/5], Step [4590/5158], Loss: 0.2548\nEpoch [3/5], Step [4600/5158], Loss: 0.1778\nEpoch [3/5], Step [4610/5158], Loss: 0.1734\nEpoch [3/5], Step [4620/5158], Loss: 0.1518\nEpoch [3/5], Step [4630/5158], Loss: 0.1796\nEpoch [3/5], Step [4640/5158], Loss: 0.2120\nEpoch [3/5], Step [4650/5158], Loss: 0.1900\nEpoch [3/5], Step [4660/5158], Loss: 0.1616\nEpoch [3/5], Step [4670/5158], Loss: 0.1994\nEpoch [3/5], Step [4680/5158], Loss: 0.1830\nEpoch [3/5], Step [4690/5158], Loss: 0.2288\nEpoch [3/5], Step [4700/5158], Loss: 0.1309\nEpoch [3/5], Step [4710/5158], Loss: 0.1743\nEpoch [3/5], Step [4720/5158], Loss: 0.1818\nEpoch [3/5], Step [4730/5158], Loss: 0.2098\nEpoch [3/5], Step [4740/5158], Loss: 0.1499\nEpoch [3/5], Step [4750/5158], Loss: 0.1795\nEpoch [3/5], Step [4760/5158], Loss: 0.1689\nEpoch [3/5], Step [4770/5158], Loss: 0.1760\nEpoch [3/5], Step [4780/5158], Loss: 0.1417\nEpoch [3/5], Step [4790/5158], Loss: 0.2456\nEpoch [3/5], Step [4800/5158], Loss: 0.1705\nEpoch [3/5], Step [4810/5158], Loss: 0.2082\nEpoch [3/5], Step [4820/5158], Loss: 0.1618\nEpoch [3/5], Step [4830/5158], Loss: 0.2246\nEpoch [3/5], Step [4840/5158], Loss: 0.2062\nEpoch [3/5], Step [4850/5158], Loss: 0.1647\nEpoch [3/5], Step [4860/5158], Loss: 0.1760\nEpoch [3/5], Step [4870/5158], Loss: 0.1770\nEpoch [3/5], Step [4880/5158], Loss: 0.1320\nEpoch [3/5], Step [4890/5158], Loss: 0.1429\nEpoch [3/5], Step [4900/5158], Loss: 0.1962\nEpoch [3/5], Step [4910/5158], Loss: 0.2055\nEpoch [3/5], Step [4920/5158], Loss: 0.1773\nEpoch [3/5], Step [4930/5158], Loss: 0.1652\nEpoch [3/5], Step [4940/5158], Loss: 0.1898\nEpoch [3/5], Step [4950/5158], Loss: 0.2023\nEpoch [3/5], Step [4960/5158], Loss: 0.1708\nEpoch [3/5], Step [4970/5158], Loss: 0.1268\nEpoch [3/5], Step [4980/5158], Loss: 0.1474\nEpoch [3/5], Step [4990/5158], Loss: 0.1217\nEpoch [3/5], Step [5000/5158], Loss: 0.1633\nEpoch [3/5], Step [5010/5158], Loss: 0.1996\nEpoch [3/5], Step [5020/5158], Loss: 0.1622\nEpoch [3/5], Step [5030/5158], Loss: 0.2081\nEpoch [3/5], Step [5040/5158], Loss: 0.1628\nEpoch [3/5], Step [5050/5158], Loss: 0.1478\nEpoch [3/5], Step [5060/5158], Loss: 0.1919\nEpoch [3/5], Step [5070/5158], Loss: 0.1650\nEpoch [3/5], Step [5080/5158], Loss: 0.1210\nEpoch [3/5], Step [5090/5158], Loss: 0.2084\nEpoch [3/5], Step [5100/5158], Loss: 0.1653\nEpoch [3/5], Step [5110/5158], Loss: 0.2115\nEpoch [3/5], Step [5120/5158], Loss: 0.2118\nEpoch [3/5], Step [5130/5158], Loss: 0.2072\nEpoch [3/5], Step [5140/5158], Loss: 0.1328\nEpoch [3/5], Step [5150/5158], Loss: 0.1568\nEpoch [4/5], Step [10/5158], Loss: 0.1712\nEpoch [4/5], Step [20/5158], Loss: 0.1107\nEpoch [4/5], Step [30/5158], Loss: 0.1486\nEpoch [4/5], Step [40/5158], Loss: 0.1972\nEpoch [4/5], Step [50/5158], Loss: 0.1900\nEpoch [4/5], Step [60/5158], Loss: 0.1665\nEpoch [4/5], Step [70/5158], Loss: 0.1340\nEpoch [4/5], Step [80/5158], Loss: 0.1424\nEpoch [4/5], Step [90/5158], Loss: 0.1959\nEpoch [4/5], Step [100/5158], Loss: 0.1496\nEpoch [4/5], Step [110/5158], Loss: 0.1411\nEpoch [4/5], Step [120/5158], Loss: 0.1053\nEpoch [4/5], Step [130/5158], Loss: 0.1193\nEpoch [4/5], Step [140/5158], Loss: 0.1281\nEpoch [4/5], Step [150/5158], Loss: 0.1801\nEpoch [4/5], Step [160/5158], Loss: 0.1541\nEpoch [4/5], Step [170/5158], Loss: 0.1550\nEpoch [4/5], Step [180/5158], Loss: 0.1171\nEpoch [4/5], Step [190/5158], Loss: 0.1348\nEpoch [4/5], Step [200/5158], Loss: 0.1766\nEpoch [4/5], Step [210/5158], Loss: 0.1512\nEpoch [4/5], Step [220/5158], Loss: 0.1088\nEpoch [4/5], Step [230/5158], Loss: 0.1501\nEpoch [4/5], Step [240/5158], Loss: 0.1191\nEpoch [4/5], Step [250/5158], Loss: 0.1415\nEpoch [4/5], Step [260/5158], Loss: 0.0930\nEpoch [4/5], Step [270/5158], Loss: 0.1132\nEpoch [4/5], Step [280/5158], Loss: 0.1370\nEpoch [4/5], Step [290/5158], Loss: 0.1433\nEpoch [4/5], Step [300/5158], Loss: 0.1779\nEpoch [4/5], Step [310/5158], Loss: 0.1439\nEpoch [4/5], Step [320/5158], Loss: 0.1299\nEpoch [4/5], Step [330/5158], Loss: 0.1788\nEpoch [4/5], Step [340/5158], Loss: 0.1682\nEpoch [4/5], Step [350/5158], Loss: 0.1589\nEpoch [4/5], Step [360/5158], Loss: 0.1114\nEpoch [4/5], Step [370/5158], Loss: 0.1039\nEpoch [4/5], Step [380/5158], Loss: 0.1325\nEpoch [4/5], Step [390/5158], Loss: 0.1636\nEpoch [4/5], Step [400/5158], Loss: 0.1254\nEpoch [4/5], Step [410/5158], Loss: 0.1219\nEpoch [4/5], Step [420/5158], Loss: 0.1493\nEpoch [4/5], Step [430/5158], Loss: 0.1877\nEpoch [4/5], Step [440/5158], Loss: 0.1882\nEpoch [4/5], Step [450/5158], Loss: 0.1938\nEpoch [4/5], Step [460/5158], Loss: 0.1275\nEpoch [4/5], Step [470/5158], Loss: 0.1739\nEpoch [4/5], Step [480/5158], Loss: 0.1480\nEpoch [4/5], Step [490/5158], Loss: 0.1265\nEpoch [4/5], Step [500/5158], Loss: 0.1534\nEpoch [4/5], Step [510/5158], Loss: 0.1533\nEpoch [4/5], Step [520/5158], Loss: 0.1106\nEpoch [4/5], Step [530/5158], Loss: 0.1363\nEpoch [4/5], Step [540/5158], Loss: 0.1998\nEpoch [4/5], Step [550/5158], Loss: 0.1178\nEpoch [4/5], Step [560/5158], Loss: 0.1058\nEpoch [4/5], Step [570/5158], Loss: 0.1243\nEpoch [4/5], Step [580/5158], Loss: 0.1832\nEpoch [4/5], Step [590/5158], Loss: 0.1831\nEpoch [4/5], Step [600/5158], Loss: 0.1080\nEpoch [4/5], Step [610/5158], Loss: 0.1105\nEpoch [4/5], Step [620/5158], Loss: 0.1502\nEpoch [4/5], Step [630/5158], Loss: 0.1096\nEpoch [4/5], Step [640/5158], Loss: 0.1791\nEpoch [4/5], Step [650/5158], Loss: 0.2069\nEpoch [4/5], Step [660/5158], Loss: 0.1211\nEpoch [4/5], Step [670/5158], Loss: 0.2063\nEpoch [4/5], Step [680/5158], Loss: 0.1525\nEpoch [4/5], Step [690/5158], Loss: 0.1351\nEpoch [4/5], Step [700/5158], Loss: 0.1434\nEpoch [4/5], Step [710/5158], Loss: 0.1693\nEpoch [4/5], Step [720/5158], Loss: 0.1527\nEpoch [4/5], Step [730/5158], Loss: 0.2256\nEpoch [4/5], Step [740/5158], Loss: 0.1356\nEpoch [4/5], Step [750/5158], Loss: 0.1483\nEpoch [4/5], Step [760/5158], Loss: 0.1815\nEpoch [4/5], Step [770/5158], Loss: 0.1808\nEpoch [4/5], Step [780/5158], Loss: 0.1933\nEpoch [4/5], Step [790/5158], Loss: 0.1508\nEpoch [4/5], Step [800/5158], Loss: 0.1775\nEpoch [4/5], Step [810/5158], Loss: 0.1579\nEpoch [4/5], Step [820/5158], Loss: 0.1228\nEpoch [4/5], Step [830/5158], Loss: 0.1467\nEpoch [4/5], Step [840/5158], Loss: 0.1444\nEpoch [4/5], Step [850/5158], Loss: 0.1370\nEpoch [4/5], Step [860/5158], Loss: 0.1770\nEpoch [4/5], Step [870/5158], Loss: 0.1569\nEpoch [4/5], Step [880/5158], Loss: 0.1572\nEpoch [4/5], Step [890/5158], Loss: 0.1756\nEpoch [4/5], Step [900/5158], Loss: 0.1880\nEpoch [4/5], Step [910/5158], Loss: 0.1760\nEpoch [4/5], Step [920/5158], Loss: 0.1725\nEpoch [4/5], Step [930/5158], Loss: 0.1229\nEpoch [4/5], Step [940/5158], Loss: 0.1661\nEpoch [4/5], Step [950/5158], Loss: 0.1629\nEpoch [4/5], Step [960/5158], Loss: 0.0887\nEpoch [4/5], Step [970/5158], Loss: 0.1112\nEpoch [4/5], Step [980/5158], Loss: 0.1424\nEpoch [4/5], Step [990/5158], Loss: 0.1799\nEpoch [4/5], Step [1000/5158], Loss: 0.2156\nEpoch [4/5], Step [1010/5158], Loss: 0.1309\nEpoch [4/5], Step [1020/5158], Loss: 0.1649\nEpoch [4/5], Step [1030/5158], Loss: 0.1001\nEpoch [4/5], Step [1040/5158], Loss: 0.1056\nEpoch [4/5], Step [1050/5158], Loss: 0.1649\nEpoch [4/5], Step [1060/5158], Loss: 0.1435\nEpoch [4/5], Step [1070/5158], Loss: 0.1645\nEpoch [4/5], Step [1080/5158], Loss: 0.1792\nEpoch [4/5], Step [1090/5158], Loss: 0.2273\nEpoch [4/5], Step [1100/5158], Loss: 0.1450\nEpoch [4/5], Step [1110/5158], Loss: 0.1804\nEpoch [4/5], Step [1120/5158], Loss: 0.1531\nEpoch [4/5], Step [1130/5158], Loss: 0.1600\nEpoch [4/5], Step [1140/5158], Loss: 0.2198\nEpoch [4/5], Step [1150/5158], Loss: 0.1533\nEpoch [4/5], Step [1160/5158], Loss: 0.1821\nEpoch [4/5], Step [1170/5158], Loss: 0.1646\nEpoch [4/5], Step [1180/5158], Loss: 0.1737\nEpoch [4/5], Step [1190/5158], Loss: 0.1031\nEpoch [4/5], Step [1200/5158], Loss: 0.1194\nEpoch [4/5], Step [1210/5158], Loss: 0.1708\nEpoch [4/5], Step [1220/5158], Loss: 0.1899\nEpoch [4/5], Step [1230/5158], Loss: 0.1375\nEpoch [4/5], Step [1240/5158], Loss: 0.1578\nEpoch [4/5], Step [1250/5158], Loss: 0.1185\nEpoch [4/5], Step [1260/5158], Loss: 0.2005\nEpoch [4/5], Step [1270/5158], Loss: 0.2085\nEpoch [4/5], Step [1280/5158], Loss: 0.1233\nEpoch [4/5], Step [1290/5158], Loss: 0.1872\nEpoch [4/5], Step [1300/5158], Loss: 0.1911\nEpoch [4/5], Step [1310/5158], Loss: 0.1421\nEpoch [4/5], Step [1320/5158], Loss: 0.1818\nEpoch [4/5], Step [1330/5158], Loss: 0.1381\nEpoch [4/5], Step [1340/5158], Loss: 0.1332\nEpoch [4/5], Step [1350/5158], Loss: 0.1538\nEpoch [4/5], Step [1360/5158], Loss: 0.1380\nEpoch [4/5], Step [1370/5158], Loss: 0.1775\nEpoch [4/5], Step [1380/5158], Loss: 0.1877\nEpoch [4/5], Step [1390/5158], Loss: 0.2021\nEpoch [4/5], Step [1400/5158], Loss: 0.1359\nEpoch [4/5], Step [1410/5158], Loss: 0.1408\nEpoch [4/5], Step [1420/5158], Loss: 0.1681\nEpoch [4/5], Step [1430/5158], Loss: 0.1538\nEpoch [4/5], Step [1440/5158], Loss: 0.1387\nEpoch [4/5], Step [1450/5158], Loss: 0.1403\nEpoch [4/5], Step [1460/5158], Loss: 0.1566\nEpoch [4/5], Step [1470/5158], Loss: 0.1243\nEpoch [4/5], Step [1480/5158], Loss: 0.1320\nEpoch [4/5], Step [1490/5158], Loss: 0.1990\nEpoch [4/5], Step [1500/5158], Loss: 0.1308\nEpoch [4/5], Step [1510/5158], Loss: 0.1139\nEpoch [4/5], Step [1520/5158], Loss: 0.1735\nEpoch [4/5], Step [1530/5158], Loss: 0.1322\nEpoch [4/5], Step [1540/5158], Loss: 0.0936\nEpoch [4/5], Step [1550/5158], Loss: 0.1284\nEpoch [4/5], Step [1560/5158], Loss: 0.1978\nEpoch [4/5], Step [1570/5158], Loss: 0.1500\nEpoch [4/5], Step [1580/5158], Loss: 0.1565\nEpoch [4/5], Step [1590/5158], Loss: 0.1662\nEpoch [4/5], Step [1600/5158], Loss: 0.1064\nEpoch [4/5], Step [1610/5158], Loss: 0.1208\nEpoch [4/5], Step [1620/5158], Loss: 0.1613\nEpoch [4/5], Step [1630/5158], Loss: 0.1653\nEpoch [4/5], Step [1640/5158], Loss: 0.1060\nEpoch [4/5], Step [1650/5158], Loss: 0.1678\nEpoch [4/5], Step [1660/5158], Loss: 0.1934\nEpoch [4/5], Step [1670/5158], Loss: 0.1128\nEpoch [4/5], Step [1680/5158], Loss: 0.1746\nEpoch [4/5], Step [1690/5158], Loss: 0.1427\nEpoch [4/5], Step [1700/5158], Loss: 0.1684\nEpoch [4/5], Step [1710/5158], Loss: 0.1371\nEpoch [4/5], Step [1720/5158], Loss: 0.1605\nEpoch [4/5], Step [1730/5158], Loss: 0.1066\nEpoch [4/5], Step [1740/5158], Loss: 0.1105\nEpoch [4/5], Step [1750/5158], Loss: 0.1244\nEpoch [4/5], Step [1760/5158], Loss: 0.1620\nEpoch [4/5], Step [1770/5158], Loss: 0.1259\nEpoch [4/5], Step [1780/5158], Loss: 0.1009\nEpoch [4/5], Step [1790/5158], Loss: 0.1733\nEpoch [4/5], Step [1800/5158], Loss: 0.1195\nEpoch [4/5], Step [1810/5158], Loss: 0.1443\nEpoch [4/5], Step [1820/5158], Loss: 0.1281\nEpoch [4/5], Step [1830/5158], Loss: 0.1584\nEpoch [4/5], Step [1840/5158], Loss: 0.1150\nEpoch [4/5], Step [1850/5158], Loss: 0.1283\nEpoch [4/5], Step [1860/5158], Loss: 0.1364\nEpoch [4/5], Step [1870/5158], Loss: 0.1586\nEpoch [4/5], Step [1880/5158], Loss: 0.1985\nEpoch [4/5], Step [1890/5158], Loss: 0.1592\nEpoch [4/5], Step [1900/5158], Loss: 0.2009\nEpoch [4/5], Step [1910/5158], Loss: 0.1999\nEpoch [4/5], Step [1920/5158], Loss: 0.1646\nEpoch [4/5], Step [1930/5158], Loss: 0.1803\nEpoch [4/5], Step [1940/5158], Loss: 0.1778\nEpoch [4/5], Step [1950/5158], Loss: 0.1369\nEpoch [4/5], Step [1960/5158], Loss: 0.1496\nEpoch [4/5], Step [1970/5158], Loss: 0.1254\nEpoch [4/5], Step [1980/5158], Loss: 0.1619\nEpoch [4/5], Step [1990/5158], Loss: 0.1675\nEpoch [4/5], Step [2000/5158], Loss: 0.1471\nEpoch [4/5], Step [2010/5158], Loss: 0.1016\nEpoch [4/5], Step [2020/5158], Loss: 0.1547\nEpoch [4/5], Step [2030/5158], Loss: 0.1591\nEpoch [4/5], Step [2040/5158], Loss: 0.1431\nEpoch [4/5], Step [2050/5158], Loss: 0.1230\nEpoch [4/5], Step [2060/5158], Loss: 0.1716\nEpoch [4/5], Step [2070/5158], Loss: 0.1334\nEpoch [4/5], Step [2080/5158], Loss: 0.1161\nEpoch [4/5], Step [2090/5158], Loss: 0.1291\nEpoch [4/5], Step [2100/5158], Loss: 0.1393\nEpoch [4/5], Step [2110/5158], Loss: 0.2091\nEpoch [4/5], Step [2120/5158], Loss: 0.1613\nEpoch [4/5], Step [2130/5158], Loss: 0.1294\nEpoch [4/5], Step [2140/5158], Loss: 0.1570\nEpoch [4/5], Step [2150/5158], Loss: 0.1748\nEpoch [4/5], Step [2160/5158], Loss: 0.1055\nEpoch [4/5], Step [2170/5158], Loss: 0.1221\nEpoch [4/5], Step [2180/5158], Loss: 0.1386\nEpoch [4/5], Step [2190/5158], Loss: 0.1608\nEpoch [4/5], Step [2200/5158], Loss: 0.1448\nEpoch [4/5], Step [2210/5158], Loss: 0.2142\nEpoch [4/5], Step [2220/5158], Loss: 0.1425\nEpoch [4/5], Step [2230/5158], Loss: 0.1677\nEpoch [4/5], Step [2240/5158], Loss: 0.1716\nEpoch [4/5], Step [2250/5158], Loss: 0.1704\nEpoch [4/5], Step [2260/5158], Loss: 0.1735\nEpoch [4/5], Step [2270/5158], Loss: 0.1165\nEpoch [4/5], Step [2280/5158], Loss: 0.1470\nEpoch [4/5], Step [2290/5158], Loss: 0.1707\nEpoch [4/5], Step [2300/5158], Loss: 0.2106\nEpoch [4/5], Step [2310/5158], Loss: 0.1297\nEpoch [4/5], Step [2320/5158], Loss: 0.1388\nEpoch [4/5], Step [2330/5158], Loss: 0.1168\nEpoch [4/5], Step [2340/5158], Loss: 0.1337\nEpoch [4/5], Step [2350/5158], Loss: 0.1151\nEpoch [4/5], Step [2360/5158], Loss: 0.1680\nEpoch [4/5], Step [2370/5158], Loss: 0.0977\nEpoch [4/5], Step [2380/5158], Loss: 0.2058\nEpoch [4/5], Step [2390/5158], Loss: 0.1121\nEpoch [4/5], Step [2400/5158], Loss: 0.1234\nEpoch [4/5], Step [2410/5158], Loss: 0.1568\nEpoch [4/5], Step [2420/5158], Loss: 0.1305\nEpoch [4/5], Step [2430/5158], Loss: 0.1478\nEpoch [4/5], Step [2440/5158], Loss: 0.1548\nEpoch [4/5], Step [2450/5158], Loss: 0.1284\nEpoch [4/5], Step [2460/5158], Loss: 0.1588\nEpoch [4/5], Step [2470/5158], Loss: 0.0984\nEpoch [4/5], Step [2480/5158], Loss: 0.1024\nEpoch [4/5], Step [2490/5158], Loss: 0.1471\nEpoch [4/5], Step [2500/5158], Loss: 0.1978\nEpoch [4/5], Step [2510/5158], Loss: 0.1295\nEpoch [4/5], Step [2520/5158], Loss: 0.1815\nEpoch [4/5], Step [2530/5158], Loss: 0.1634\nEpoch [4/5], Step [2540/5158], Loss: 0.1324\nEpoch [4/5], Step [2550/5158], Loss: 0.1866\nEpoch [4/5], Step [2560/5158], Loss: 0.1257\nEpoch [4/5], Step [2570/5158], Loss: 0.1538\nEpoch [4/5], Step [2580/5158], Loss: 0.1607\nEpoch [4/5], Step [2590/5158], Loss: 0.1292\nEpoch [4/5], Step [2600/5158], Loss: 0.1406\nEpoch [4/5], Step [2610/5158], Loss: 0.1638\nEpoch [4/5], Step [2620/5158], Loss: 0.1576\nEpoch [4/5], Step [2630/5158], Loss: 0.1704\nEpoch [4/5], Step [2640/5158], Loss: 0.1017\nEpoch [4/5], Step [2650/5158], Loss: 0.0898\nEpoch [4/5], Step [2660/5158], Loss: 0.0918\nEpoch [4/5], Step [2670/5158], Loss: 0.1796\nEpoch [4/5], Step [2680/5158], Loss: 0.0922\nEpoch [4/5], Step [2690/5158], Loss: 0.1567\nEpoch [4/5], Step [2700/5158], Loss: 0.1495\nEpoch [4/5], Step [2710/5158], Loss: 0.1771\nEpoch [4/5], Step [2720/5158], Loss: 0.1836\nEpoch [4/5], Step [2730/5158], Loss: 0.1364\nEpoch [4/5], Step [2740/5158], Loss: 0.2006\nEpoch [4/5], Step [2750/5158], Loss: 0.1792\nEpoch [4/5], Step [2760/5158], Loss: 0.1469\nEpoch [4/5], Step [2770/5158], Loss: 0.1612\nEpoch [4/5], Step [2780/5158], Loss: 0.1557\nEpoch [4/5], Step [2790/5158], Loss: 0.1525\nEpoch [4/5], Step [2800/5158], Loss: 0.1501\nEpoch [4/5], Step [2810/5158], Loss: 0.1288\nEpoch [4/5], Step [2820/5158], Loss: 0.1292\nEpoch [4/5], Step [2830/5158], Loss: 0.1394\nEpoch [4/5], Step [2840/5158], Loss: 0.1668\nEpoch [4/5], Step [2850/5158], Loss: 0.1952\nEpoch [4/5], Step [2860/5158], Loss: 0.1677\nEpoch [4/5], Step [2870/5158], Loss: 0.1688\nEpoch [4/5], Step [2880/5158], Loss: 0.2460\nEpoch [4/5], Step [2890/5158], Loss: 0.1163\nEpoch [4/5], Step [2900/5158], Loss: 0.1538\nEpoch [4/5], Step [2910/5158], Loss: 0.1454\nEpoch [4/5], Step [2920/5158], Loss: 0.1538\nEpoch [4/5], Step [2930/5158], Loss: 0.0987\nEpoch [4/5], Step [2940/5158], Loss: 0.1732\nEpoch [4/5], Step [2950/5158], Loss: 0.1559\nEpoch [4/5], Step [2960/5158], Loss: 0.1450\nEpoch [4/5], Step [2970/5158], Loss: 0.1406\nEpoch [4/5], Step [2980/5158], Loss: 0.1352\nEpoch [4/5], Step [2990/5158], Loss: 0.1881\nEpoch [4/5], Step [3000/5158], Loss: 0.1201\nEpoch [4/5], Step [3010/5158], Loss: 0.0941\nEpoch [4/5], Step [3020/5158], Loss: 0.1313\nEpoch [4/5], Step [3030/5158], Loss: 0.1537\nEpoch [4/5], Step [3040/5158], Loss: 0.1205\nEpoch [4/5], Step [3050/5158], Loss: 0.1430\nEpoch [4/5], Step [3060/5158], Loss: 0.1722\nEpoch [4/5], Step [3070/5158], Loss: 0.1265\nEpoch [4/5], Step [3080/5158], Loss: 0.1887\nEpoch [4/5], Step [3090/5158], Loss: 0.2138\nEpoch [4/5], Step [3100/5158], Loss: 0.1717\nEpoch [4/5], Step [3110/5158], Loss: 0.1666\nEpoch [4/5], Step [3120/5158], Loss: 0.1575\nEpoch [4/5], Step [3130/5158], Loss: 0.1307\nEpoch [4/5], Step [3140/5158], Loss: 0.1267\nEpoch [4/5], Step [3150/5158], Loss: 0.1893\nEpoch [4/5], Step [3160/5158], Loss: 0.1267\nEpoch [4/5], Step [3170/5158], Loss: 0.1326\nEpoch [4/5], Step [3180/5158], Loss: 0.1832\nEpoch [4/5], Step [3190/5158], Loss: 0.1301\nEpoch [4/5], Step [3200/5158], Loss: 0.1480\nEpoch [4/5], Step [3210/5158], Loss: 0.1464\nEpoch [4/5], Step [3220/5158], Loss: 0.1333\nEpoch [4/5], Step [3230/5158], Loss: 0.1935\nEpoch [4/5], Step [3240/5158], Loss: 0.1609\nEpoch [4/5], Step [3250/5158], Loss: 0.1804\nEpoch [4/5], Step [3260/5158], Loss: 0.1686\nEpoch [4/5], Step [3270/5158], Loss: 0.2112\nEpoch [4/5], Step [3280/5158], Loss: 0.1886\nEpoch [4/5], Step [3290/5158], Loss: 0.0808\nEpoch [4/5], Step [3300/5158], Loss: 0.1417\nEpoch [4/5], Step [3310/5158], Loss: 0.1778\nEpoch [4/5], Step [3320/5158], Loss: 0.1248\nEpoch [4/5], Step [3330/5158], Loss: 0.1521\nEpoch [4/5], Step [3340/5158], Loss: 0.1244\nEpoch [4/5], Step [3350/5158], Loss: 0.2308\nEpoch [4/5], Step [3360/5158], Loss: 0.1271\nEpoch [4/5], Step [3370/5158], Loss: 0.0977\nEpoch [4/5], Step [3380/5158], Loss: 0.1842\nEpoch [4/5], Step [3390/5158], Loss: 0.1125\nEpoch [4/5], Step [3400/5158], Loss: 0.2208\nEpoch [4/5], Step [3410/5158], Loss: 0.1492\nEpoch [4/5], Step [3420/5158], Loss: 0.1728\nEpoch [4/5], Step [3430/5158], Loss: 0.1326\nEpoch [4/5], Step [3440/5158], Loss: 0.1700\nEpoch [4/5], Step [3450/5158], Loss: 0.1740\nEpoch [4/5], Step [3460/5158], Loss: 0.1521\nEpoch [4/5], Step [3470/5158], Loss: 0.1511\nEpoch [4/5], Step [3480/5158], Loss: 0.1352\nEpoch [4/5], Step [3490/5158], Loss: 0.1175\nEpoch [4/5], Step [3500/5158], Loss: 0.1231\nEpoch [4/5], Step [3510/5158], Loss: 0.1709\nEpoch [4/5], Step [3520/5158], Loss: 0.1015\nEpoch [4/5], Step [3530/5158], Loss: 0.1457\nEpoch [4/5], Step [3540/5158], Loss: 0.1682\nEpoch [4/5], Step [3550/5158], Loss: 0.1482\nEpoch [4/5], Step [3560/5158], Loss: 0.1558\nEpoch [4/5], Step [3570/5158], Loss: 0.1488\nEpoch [4/5], Step [3580/5158], Loss: 0.1572\nEpoch [4/5], Step [3590/5158], Loss: 0.1463\nEpoch [4/5], Step [3600/5158], Loss: 0.1158\nEpoch [4/5], Step [3610/5158], Loss: 0.1514\nEpoch [4/5], Step [3620/5158], Loss: 0.1898\nEpoch [4/5], Step [3630/5158], Loss: 0.1299\nEpoch [4/5], Step [3640/5158], Loss: 0.1349\nEpoch [4/5], Step [3650/5158], Loss: 0.1264\nEpoch [4/5], Step [3660/5158], Loss: 0.0852\nEpoch [4/5], Step [3670/5158], Loss: 0.1732\nEpoch [4/5], Step [3680/5158], Loss: 0.1032\nEpoch [4/5], Step [3690/5158], Loss: 0.1217\nEpoch [4/5], Step [3700/5158], Loss: 0.1768\nEpoch [4/5], Step [3710/5158], Loss: 0.1526\nEpoch [4/5], Step [3720/5158], Loss: 0.1426\nEpoch [4/5], Step [3730/5158], Loss: 0.1438\nEpoch [4/5], Step [3740/5158], Loss: 0.1485\nEpoch [4/5], Step [3750/5158], Loss: 0.2153\nEpoch [4/5], Step [3760/5158], Loss: 0.1269\nEpoch [4/5], Step [3770/5158], Loss: 0.2166\nEpoch [4/5], Step [3780/5158], Loss: 0.1879\nEpoch [4/5], Step [3790/5158], Loss: 0.1468\nEpoch [4/5], Step [3800/5158], Loss: 0.0985\nEpoch [4/5], Step [3810/5158], Loss: 0.1508\nEpoch [4/5], Step [3820/5158], Loss: 0.1479\nEpoch [4/5], Step [3830/5158], Loss: 0.1757\nEpoch [4/5], Step [3840/5158], Loss: 0.1769\nEpoch [4/5], Step [3850/5158], Loss: 0.2135\nEpoch [4/5], Step [3860/5158], Loss: 0.1713\nEpoch [4/5], Step [3870/5158], Loss: 0.1490\nEpoch [4/5], Step [3880/5158], Loss: 0.0918\nEpoch [4/5], Step [3890/5158], Loss: 0.1714\nEpoch [4/5], Step [3900/5158], Loss: 0.1287\nEpoch [4/5], Step [3910/5158], Loss: 0.1152\nEpoch [4/5], Step [3920/5158], Loss: 0.1000\nEpoch [4/5], Step [3930/5158], Loss: 0.1853\nEpoch [4/5], Step [3940/5158], Loss: 0.1365\nEpoch [4/5], Step [3950/5158], Loss: 0.1416\nEpoch [4/5], Step [3960/5158], Loss: 0.1691\nEpoch [4/5], Step [3970/5158], Loss: 0.1826\nEpoch [4/5], Step [3980/5158], Loss: 0.1535\nEpoch [4/5], Step [3990/5158], Loss: 0.1173\nEpoch [4/5], Step [4000/5158], Loss: 0.1377\nEpoch [4/5], Step [4010/5158], Loss: 0.1461\nEpoch [4/5], Step [4020/5158], Loss: 0.1296\nEpoch [4/5], Step [4030/5158], Loss: 0.1007\nEpoch [4/5], Step [4040/5158], Loss: 0.1852\nEpoch [4/5], Step [4050/5158], Loss: 0.1776\nEpoch [4/5], Step [4060/5158], Loss: 0.1715\nEpoch [4/5], Step [4070/5158], Loss: 0.1526\nEpoch [4/5], Step [4080/5158], Loss: 0.2184\nEpoch [4/5], Step [4090/5158], Loss: 0.1746\nEpoch [4/5], Step [4100/5158], Loss: 0.1749\nEpoch [4/5], Step [4110/5158], Loss: 0.1103\nEpoch [4/5], Step [4120/5158], Loss: 0.1513\nEpoch [4/5], Step [4130/5158], Loss: 0.2054\nEpoch [4/5], Step [4140/5158], Loss: 0.1645\nEpoch [4/5], Step [4150/5158], Loss: 0.1344\nEpoch [4/5], Step [4160/5158], Loss: 0.1681\nEpoch [4/5], Step [4170/5158], Loss: 0.1865\nEpoch [4/5], Step [4180/5158], Loss: 0.2203\nEpoch [4/5], Step [4190/5158], Loss: 0.1503\nEpoch [4/5], Step [4200/5158], Loss: 0.1403\nEpoch [4/5], Step [4210/5158], Loss: 0.1447\nEpoch [4/5], Step [4220/5158], Loss: 0.1957\nEpoch [4/5], Step [4230/5158], Loss: 0.1474\nEpoch [4/5], Step [4240/5158], Loss: 0.1581\nEpoch [4/5], Step [4250/5158], Loss: 0.1417\nEpoch [4/5], Step [4260/5158], Loss: 0.1265\nEpoch [4/5], Step [4270/5158], Loss: 0.1622\nEpoch [4/5], Step [4280/5158], Loss: 0.2002\nEpoch [4/5], Step [4290/5158], Loss: 0.1641\nEpoch [4/5], Step [4300/5158], Loss: 0.1595\nEpoch [4/5], Step [4310/5158], Loss: 0.1588\nEpoch [4/5], Step [4320/5158], Loss: 0.1896\nEpoch [4/5], Step [4330/5158], Loss: 0.1318\nEpoch [4/5], Step [4340/5158], Loss: 0.1476\nEpoch [4/5], Step [4350/5158], Loss: 0.0983\nEpoch [4/5], Step [4360/5158], Loss: 0.2142\nEpoch [4/5], Step [4370/5158], Loss: 0.0937\nEpoch [4/5], Step [4380/5158], Loss: 0.1289\nEpoch [4/5], Step [4390/5158], Loss: 0.1603\nEpoch [4/5], Step [4400/5158], Loss: 0.1652\nEpoch [4/5], Step [4410/5158], Loss: 0.1238\nEpoch [4/5], Step [4420/5158], Loss: 0.1603\nEpoch [4/5], Step [4430/5158], Loss: 0.1308\nEpoch [4/5], Step [4440/5158], Loss: 0.1448\nEpoch [4/5], Step [4450/5158], Loss: 0.1492\nEpoch [4/5], Step [4460/5158], Loss: 0.1715\nEpoch [4/5], Step [4470/5158], Loss: 0.1860\nEpoch [4/5], Step [4480/5158], Loss: 0.1434\nEpoch [4/5], Step [4490/5158], Loss: 0.1209\nEpoch [4/5], Step [4500/5158], Loss: 0.1642\nEpoch [4/5], Step [4510/5158], Loss: 0.1676\nEpoch [4/5], Step [4520/5158], Loss: 0.1581\nEpoch [4/5], Step [4530/5158], Loss: 0.1553\nEpoch [4/5], Step [4540/5158], Loss: 0.1300\nEpoch [4/5], Step [4550/5158], Loss: 0.1367\nEpoch [4/5], Step [4560/5158], Loss: 0.1807\nEpoch [4/5], Step [4570/5158], Loss: 0.2048\nEpoch [4/5], Step [4580/5158], Loss: 0.1663\nEpoch [4/5], Step [4590/5158], Loss: 0.1144\nEpoch [4/5], Step [4600/5158], Loss: 0.1197\nEpoch [4/5], Step [4610/5158], Loss: 0.1459\nEpoch [4/5], Step [4620/5158], Loss: 0.1545\nEpoch [4/5], Step [4630/5158], Loss: 0.0864\nEpoch [4/5], Step [4640/5158], Loss: 0.1804\nEpoch [4/5], Step [4650/5158], Loss: 0.1919\nEpoch [4/5], Step [4660/5158], Loss: 0.2233\nEpoch [4/5], Step [4670/5158], Loss: 0.1738\nEpoch [4/5], Step [4680/5158], Loss: 0.1650\nEpoch [4/5], Step [4690/5158], Loss: 0.1620\nEpoch [4/5], Step [4700/5158], Loss: 0.0985\nEpoch [4/5], Step [4710/5158], Loss: 0.1242\nEpoch [4/5], Step [4720/5158], Loss: 0.1244\nEpoch [4/5], Step [4730/5158], Loss: 0.2175\nEpoch [4/5], Step [4740/5158], Loss: 0.1014\nEpoch [4/5], Step [4750/5158], Loss: 0.1464\nEpoch [4/5], Step [4760/5158], Loss: 0.1174\nEpoch [4/5], Step [4770/5158], Loss: 0.1442\nEpoch [4/5], Step [4780/5158], Loss: 0.1589\nEpoch [4/5], Step [4790/5158], Loss: 0.1279\nEpoch [4/5], Step [4800/5158], Loss: 0.1638\nEpoch [4/5], Step [4810/5158], Loss: 0.2131\nEpoch [4/5], Step [4820/5158], Loss: 0.1711\nEpoch [4/5], Step [4830/5158], Loss: 0.1499\nEpoch [4/5], Step [4840/5158], Loss: 0.1826\nEpoch [4/5], Step [4850/5158], Loss: 0.1164\nEpoch [4/5], Step [4860/5158], Loss: 0.1855\nEpoch [4/5], Step [4870/5158], Loss: 0.0905\nEpoch [4/5], Step [4880/5158], Loss: 0.1719\nEpoch [4/5], Step [4890/5158], Loss: 0.0945\nEpoch [4/5], Step [4900/5158], Loss: 0.1161\nEpoch [4/5], Step [4910/5158], Loss: 0.1318\nEpoch [4/5], Step [4920/5158], Loss: 0.2150\nEpoch [4/5], Step [4930/5158], Loss: 0.1208\nEpoch [4/5], Step [4940/5158], Loss: 0.1553\nEpoch [4/5], Step [4950/5158], Loss: 0.1776\nEpoch [4/5], Step [4960/5158], Loss: 0.1834\nEpoch [4/5], Step [4970/5158], Loss: 0.1793\nEpoch [4/5], Step [4980/5158], Loss: 0.1927\nEpoch [4/5], Step [4990/5158], Loss: 0.1245\nEpoch [4/5], Step [5000/5158], Loss: 0.1775\nEpoch [4/5], Step [5010/5158], Loss: 0.1387\nEpoch [4/5], Step [5020/5158], Loss: 0.1424\nEpoch [4/5], Step [5030/5158], Loss: 0.1873\nEpoch [4/5], Step [5040/5158], Loss: 0.1339\nEpoch [4/5], Step [5050/5158], Loss: 0.1394\nEpoch [4/5], Step [5060/5158], Loss: 0.1090\nEpoch [4/5], Step [5070/5158], Loss: 0.1108\nEpoch [4/5], Step [5080/5158], Loss: 0.1325\nEpoch [4/5], Step [5090/5158], Loss: 0.0996\nEpoch [4/5], Step [5100/5158], Loss: 0.1211\nEpoch [4/5], Step [5110/5158], Loss: 0.1450\nEpoch [4/5], Step [5120/5158], Loss: 0.1604\nEpoch [4/5], Step [5130/5158], Loss: 0.1252\nEpoch [4/5], Step [5140/5158], Loss: 0.1831\nEpoch [4/5], Step [5150/5158], Loss: 0.1349\nEpoch [5/5], Step [10/5158], Loss: 0.1383\nEpoch [5/5], Step [20/5158], Loss: 0.1044\nEpoch [5/5], Step [30/5158], Loss: 0.1009\nEpoch [5/5], Step [40/5158], Loss: 0.0607\nEpoch [5/5], Step [50/5158], Loss: 0.1680\nEpoch [5/5], Step [60/5158], Loss: 0.1216\nEpoch [5/5], Step [70/5158], Loss: 0.1590\nEpoch [5/5], Step [80/5158], Loss: 0.0732\nEpoch [5/5], Step [90/5158], Loss: 0.1233\nEpoch [5/5], Step [100/5158], Loss: 0.1216\nEpoch [5/5], Step [110/5158], Loss: 0.0995\nEpoch [5/5], Step [120/5158], Loss: 0.1030\nEpoch [5/5], Step [130/5158], Loss: 0.1215\nEpoch [5/5], Step [140/5158], Loss: 0.1208\nEpoch [5/5], Step [150/5158], Loss: 0.2057\nEpoch [5/5], Step [160/5158], Loss: 0.1172\nEpoch [5/5], Step [170/5158], Loss: 0.1438\nEpoch [5/5], Step [180/5158], Loss: 0.1288\nEpoch [5/5], Step [190/5158], Loss: 0.1482\nEpoch [5/5], Step [200/5158], Loss: 0.1080\nEpoch [5/5], Step [210/5158], Loss: 0.1095\nEpoch [5/5], Step [220/5158], Loss: 0.0727\nEpoch [5/5], Step [230/5158], Loss: 0.0968\nEpoch [5/5], Step [240/5158], Loss: 0.1642\nEpoch [5/5], Step [250/5158], Loss: 0.1698\nEpoch [5/5], Step [260/5158], Loss: 0.1870\nEpoch [5/5], Step [270/5158], Loss: 0.1473\nEpoch [5/5], Step [280/5158], Loss: 0.1139\nEpoch [5/5], Step [290/5158], Loss: 0.1894\nEpoch [5/5], Step [300/5158], Loss: 0.1212\nEpoch [5/5], Step [310/5158], Loss: 0.1086\nEpoch [5/5], Step [320/5158], Loss: 0.1503\nEpoch [5/5], Step [330/5158], Loss: 0.1523\nEpoch [5/5], Step [340/5158], Loss: 0.1403\nEpoch [5/5], Step [350/5158], Loss: 0.1393\nEpoch [5/5], Step [360/5158], Loss: 0.1463\nEpoch [5/5], Step [370/5158], Loss: 0.1557\nEpoch [5/5], Step [380/5158], Loss: 0.1223\nEpoch [5/5], Step [390/5158], Loss: 0.1477\nEpoch [5/5], Step [400/5158], Loss: 0.1226\nEpoch [5/5], Step [410/5158], Loss: 0.1335\nEpoch [5/5], Step [420/5158], Loss: 0.1075\nEpoch [5/5], Step [430/5158], Loss: 0.1512\nEpoch [5/5], Step [440/5158], Loss: 0.1287\nEpoch [5/5], Step [450/5158], Loss: 0.1224\nEpoch [5/5], Step [460/5158], Loss: 0.1372\nEpoch [5/5], Step [470/5158], Loss: 0.0738\nEpoch [5/5], Step [480/5158], Loss: 0.1228\nEpoch [5/5], Step [490/5158], Loss: 0.1183\nEpoch [5/5], Step [500/5158], Loss: 0.0956\nEpoch [5/5], Step [510/5158], Loss: 0.1020\nEpoch [5/5], Step [520/5158], Loss: 0.1579\nEpoch [5/5], Step [530/5158], Loss: 0.1397\nEpoch [5/5], Step [540/5158], Loss: 0.1185\nEpoch [5/5], Step [550/5158], Loss: 0.1644\nEpoch [5/5], Step [560/5158], Loss: 0.1664\nEpoch [5/5], Step [570/5158], Loss: 0.1444\nEpoch [5/5], Step [580/5158], Loss: 0.1286\nEpoch [5/5], Step [590/5158], Loss: 0.1921\nEpoch [5/5], Step [600/5158], Loss: 0.0932\nEpoch [5/5], Step [610/5158], Loss: 0.1283\nEpoch [5/5], Step [620/5158], Loss: 0.1360\nEpoch [5/5], Step [630/5158], Loss: 0.1112\nEpoch [5/5], Step [640/5158], Loss: 0.0994\nEpoch [5/5], Step [650/5158], Loss: 0.1358\nEpoch [5/5], Step [660/5158], Loss: 0.2048\nEpoch [5/5], Step [670/5158], Loss: 0.1233\nEpoch [5/5], Step [680/5158], Loss: 0.1436\nEpoch [5/5], Step [690/5158], Loss: 0.1003\nEpoch [5/5], Step [700/5158], Loss: 0.0957\nEpoch [5/5], Step [710/5158], Loss: 0.1327\nEpoch [5/5], Step [720/5158], Loss: 0.1378\nEpoch [5/5], Step [730/5158], Loss: 0.1516\nEpoch [5/5], Step [740/5158], Loss: 0.1123\nEpoch [5/5], Step [750/5158], Loss: 0.1114\nEpoch [5/5], Step [760/5158], Loss: 0.1762\nEpoch [5/5], Step [770/5158], Loss: 0.1515\nEpoch [5/5], Step [780/5158], Loss: 0.0742\nEpoch [5/5], Step [790/5158], Loss: 0.1002\nEpoch [5/5], Step [800/5158], Loss: 0.0747\nEpoch [5/5], Step [810/5158], Loss: 0.1703\nEpoch [5/5], Step [820/5158], Loss: 0.0924\nEpoch [5/5], Step [830/5158], Loss: 0.1211\nEpoch [5/5], Step [840/5158], Loss: 0.1013\nEpoch [5/5], Step [850/5158], Loss: 0.1188\nEpoch [5/5], Step [860/5158], Loss: 0.1180\nEpoch [5/5], Step [870/5158], Loss: 0.0987\nEpoch [5/5], Step [880/5158], Loss: 0.1552\nEpoch [5/5], Step [890/5158], Loss: 0.1532\nEpoch [5/5], Step [900/5158], Loss: 0.0981\nEpoch [5/5], Step [910/5158], Loss: 0.1426\nEpoch [5/5], Step [920/5158], Loss: 0.1379\nEpoch [5/5], Step [930/5158], Loss: 0.1133\nEpoch [5/5], Step [940/5158], Loss: 0.1288\nEpoch [5/5], Step [950/5158], Loss: 0.1378\nEpoch [5/5], Step [960/5158], Loss: 0.0760\nEpoch [5/5], Step [970/5158], Loss: 0.1407\nEpoch [5/5], Step [980/5158], Loss: 0.1624\nEpoch [5/5], Step [990/5158], Loss: 0.1256\nEpoch [5/5], Step [1000/5158], Loss: 0.1507\nEpoch [5/5], Step [1010/5158], Loss: 0.1616\nEpoch [5/5], Step [1020/5158], Loss: 0.1149\nEpoch [5/5], Step [1030/5158], Loss: 0.1582\nEpoch [5/5], Step [1040/5158], Loss: 0.1371\nEpoch [5/5], Step [1050/5158], Loss: 0.1471\nEpoch [5/5], Step [1060/5158], Loss: 0.1726\nEpoch [5/5], Step [1070/5158], Loss: 0.0891\nEpoch [5/5], Step [1080/5158], Loss: 0.1908\nEpoch [5/5], Step [1090/5158], Loss: 0.1437\nEpoch [5/5], Step [1100/5158], Loss: 0.1018\nEpoch [5/5], Step [1110/5158], Loss: 0.1099\nEpoch [5/5], Step [1120/5158], Loss: 0.1145\nEpoch [5/5], Step [1130/5158], Loss: 0.1765\nEpoch [5/5], Step [1140/5158], Loss: 0.1219\nEpoch [5/5], Step [1150/5158], Loss: 0.1485\nEpoch [5/5], Step [1160/5158], Loss: 0.0893\nEpoch [5/5], Step [1170/5158], Loss: 0.1509\nEpoch [5/5], Step [1180/5158], Loss: 0.1482\nEpoch [5/5], Step [1190/5158], Loss: 0.1085\nEpoch [5/5], Step [1200/5158], Loss: 0.1346\nEpoch [5/5], Step [1210/5158], Loss: 0.1573\nEpoch [5/5], Step [1220/5158], Loss: 0.1311\nEpoch [5/5], Step [1230/5158], Loss: 0.1319\nEpoch [5/5], Step [1240/5158], Loss: 0.1293\nEpoch [5/5], Step [1250/5158], Loss: 0.1629\nEpoch [5/5], Step [1260/5158], Loss: 0.1271\nEpoch [5/5], Step [1270/5158], Loss: 0.1147\nEpoch [5/5], Step [1280/5158], Loss: 0.0922\nEpoch [5/5], Step [1290/5158], Loss: 0.1377\nEpoch [5/5], Step [1300/5158], Loss: 0.1161\nEpoch [5/5], Step [1310/5158], Loss: 0.1581\nEpoch [5/5], Step [1320/5158], Loss: 0.1344\nEpoch [5/5], Step [1330/5158], Loss: 0.1961\nEpoch [5/5], Step [1340/5158], Loss: 0.0869\nEpoch [5/5], Step [1350/5158], Loss: 0.1429\nEpoch [5/5], Step [1360/5158], Loss: 0.1161\nEpoch [5/5], Step [1370/5158], Loss: 0.1127\nEpoch [5/5], Step [1380/5158], Loss: 0.1452\nEpoch [5/5], Step [1390/5158], Loss: 0.0779\nEpoch [5/5], Step [1400/5158], Loss: 0.1493\nEpoch [5/5], Step [1410/5158], Loss: 0.1010\nEpoch [5/5], Step [1420/5158], Loss: 0.1138\nEpoch [5/5], Step [1430/5158], Loss: 0.0947\nEpoch [5/5], Step [1440/5158], Loss: 0.1543\nEpoch [5/5], Step [1450/5158], Loss: 0.1605\nEpoch [5/5], Step [1460/5158], Loss: 0.1297\nEpoch [5/5], Step [1470/5158], Loss: 0.1977\nEpoch [5/5], Step [1480/5158], Loss: 0.1018\nEpoch [5/5], Step [1490/5158], Loss: 0.1047\nEpoch [5/5], Step [1500/5158], Loss: 0.1617\nEpoch [5/5], Step [1510/5158], Loss: 0.1058\nEpoch [5/5], Step [1520/5158], Loss: 0.1184\nEpoch [5/5], Step [1530/5158], Loss: 0.1676\nEpoch [5/5], Step [1540/5158], Loss: 0.1058\nEpoch [5/5], Step [1550/5158], Loss: 0.1463\nEpoch [5/5], Step [1560/5158], Loss: 0.1047\nEpoch [5/5], Step [1570/5158], Loss: 0.1344\nEpoch [5/5], Step [1580/5158], Loss: 0.0982\nEpoch [5/5], Step [1590/5158], Loss: 0.1128\nEpoch [5/5], Step [1600/5158], Loss: 0.1075\nEpoch [5/5], Step [1610/5158], Loss: 0.1340\nEpoch [5/5], Step [1620/5158], Loss: 0.1193\nEpoch [5/5], Step [1630/5158], Loss: 0.1430\nEpoch [5/5], Step [1640/5158], Loss: 0.1191\nEpoch [5/5], Step [1650/5158], Loss: 0.1244\nEpoch [5/5], Step [1660/5158], Loss: 0.1818\nEpoch [5/5], Step [1670/5158], Loss: 0.1584\nEpoch [5/5], Step [1680/5158], Loss: 0.1127\nEpoch [5/5], Step [1690/5158], Loss: 0.1511\nEpoch [5/5], Step [1700/5158], Loss: 0.0889\nEpoch [5/5], Step [1710/5158], Loss: 0.1348\nEpoch [5/5], Step [1720/5158], Loss: 0.1111\nEpoch [5/5], Step [1730/5158], Loss: 0.0945\nEpoch [5/5], Step [1740/5158], Loss: 0.1646\nEpoch [5/5], Step [1750/5158], Loss: 0.1027\nEpoch [5/5], Step [1760/5158], Loss: 0.1390\nEpoch [5/5], Step [1770/5158], Loss: 0.1929\nEpoch [5/5], Step [1780/5158], Loss: 0.1099\nEpoch [5/5], Step [1790/5158], Loss: 0.1382\nEpoch [5/5], Step [1800/5158], Loss: 0.1430\nEpoch [5/5], Step [1810/5158], Loss: 0.1284\nEpoch [5/5], Step [1820/5158], Loss: 0.1265\nEpoch [5/5], Step [1830/5158], Loss: 0.1344\nEpoch [5/5], Step [1840/5158], Loss: 0.1261\nEpoch [5/5], Step [1850/5158], Loss: 0.1377\nEpoch [5/5], Step [1860/5158], Loss: 0.1331\nEpoch [5/5], Step [1870/5158], Loss: 0.2233\nEpoch [5/5], Step [1880/5158], Loss: 0.1220\nEpoch [5/5], Step [1890/5158], Loss: 0.1342\nEpoch [5/5], Step [1900/5158], Loss: 0.0993\nEpoch [5/5], Step [1910/5158], Loss: 0.1402\nEpoch [5/5], Step [1920/5158], Loss: 0.1173\nEpoch [5/5], Step [1930/5158], Loss: 0.1168\nEpoch [5/5], Step [1940/5158], Loss: 0.1259\nEpoch [5/5], Step [1950/5158], Loss: 0.1541\nEpoch [5/5], Step [1960/5158], Loss: 0.1341\nEpoch [5/5], Step [1970/5158], Loss: 0.0971\nEpoch [5/5], Step [1980/5158], Loss: 0.1220\nEpoch [5/5], Step [1990/5158], Loss: 0.0811\nEpoch [5/5], Step [2000/5158], Loss: 0.0884\nEpoch [5/5], Step [2010/5158], Loss: 0.1575\nEpoch [5/5], Step [2020/5158], Loss: 0.1539\nEpoch [5/5], Step [2030/5158], Loss: 0.2146\nEpoch [5/5], Step [2040/5158], Loss: 0.1553\nEpoch [5/5], Step [2050/5158], Loss: 0.1735\nEpoch [5/5], Step [2060/5158], Loss: 0.1662\nEpoch [5/5], Step [2070/5158], Loss: 0.0991\nEpoch [5/5], Step [2080/5158], Loss: 0.1335\nEpoch [5/5], Step [2090/5158], Loss: 0.1471\nEpoch [5/5], Step [2100/5158], Loss: 0.1512\nEpoch [5/5], Step [2110/5158], Loss: 0.1658\nEpoch [5/5], Step [2120/5158], Loss: 0.0890\nEpoch [5/5], Step [2130/5158], Loss: 0.1874\nEpoch [5/5], Step [2140/5158], Loss: 0.1439\nEpoch [5/5], Step [2150/5158], Loss: 0.1401\nEpoch [5/5], Step [2160/5158], Loss: 0.1140\nEpoch [5/5], Step [2170/5158], Loss: 0.1433\nEpoch [5/5], Step [2180/5158], Loss: 0.0868\nEpoch [5/5], Step [2190/5158], Loss: 0.1201\nEpoch [5/5], Step [2200/5158], Loss: 0.1416\nEpoch [5/5], Step [2210/5158], Loss: 0.1123\nEpoch [5/5], Step [2220/5158], Loss: 0.1200\nEpoch [5/5], Step [2230/5158], Loss: 0.1005\nEpoch [5/5], Step [2240/5158], Loss: 0.1710\nEpoch [5/5], Step [2250/5158], Loss: 0.1400\nEpoch [5/5], Step [2260/5158], Loss: 0.0770\nEpoch [5/5], Step [2270/5158], Loss: 0.1061\nEpoch [5/5], Step [2280/5158], Loss: 0.1790\nEpoch [5/5], Step [2290/5158], Loss: 0.1435\nEpoch [5/5], Step [2300/5158], Loss: 0.1173\nEpoch [5/5], Step [2310/5158], Loss: 0.1447\nEpoch [5/5], Step [2320/5158], Loss: 0.1566\nEpoch [5/5], Step [2330/5158], Loss: 0.1265\nEpoch [5/5], Step [2340/5158], Loss: 0.1215\nEpoch [5/5], Step [2350/5158], Loss: 0.2413\nEpoch [5/5], Step [2360/5158], Loss: 0.1553\nEpoch [5/5], Step [2370/5158], Loss: 0.1272\nEpoch [5/5], Step [2380/5158], Loss: 0.1531\nEpoch [5/5], Step [2390/5158], Loss: 0.1318\nEpoch [5/5], Step [2400/5158], Loss: 0.1039\nEpoch [5/5], Step [2410/5158], Loss: 0.1010\nEpoch [5/5], Step [2420/5158], Loss: 0.1230\nEpoch [5/5], Step [2430/5158], Loss: 0.1178\nEpoch [5/5], Step [2440/5158], Loss: 0.1468\nEpoch [5/5], Step [2450/5158], Loss: 0.1492\nEpoch [5/5], Step [2460/5158], Loss: 0.1370\nEpoch [5/5], Step [2470/5158], Loss: 0.1287\nEpoch [5/5], Step [2480/5158], Loss: 0.1194\nEpoch [5/5], Step [2490/5158], Loss: 0.1339\nEpoch [5/5], Step [2500/5158], Loss: 0.1227\nEpoch [5/5], Step [2510/5158], Loss: 0.1192\nEpoch [5/5], Step [2520/5158], Loss: 0.1442\nEpoch [5/5], Step [2530/5158], Loss: 0.1221\nEpoch [5/5], Step [2540/5158], Loss: 0.1101\nEpoch [5/5], Step [2550/5158], Loss: 0.1091\nEpoch [5/5], Step [2560/5158], Loss: 0.1447\nEpoch [5/5], Step [2570/5158], Loss: 0.1637\nEpoch [5/5], Step [2580/5158], Loss: 0.1345\nEpoch [5/5], Step [2590/5158], Loss: 0.1720\nEpoch [5/5], Step [2600/5158], Loss: 0.0870\nEpoch [5/5], Step [2610/5158], Loss: 0.1273\nEpoch [5/5], Step [2620/5158], Loss: 0.1306\nEpoch [5/5], Step [2630/5158], Loss: 0.1261\nEpoch [5/5], Step [2640/5158], Loss: 0.1353\nEpoch [5/5], Step [2650/5158], Loss: 0.1395\nEpoch [5/5], Step [2660/5158], Loss: 0.1622\nEpoch [5/5], Step [2670/5158], Loss: 0.1091\nEpoch [5/5], Step [2680/5158], Loss: 0.1719\nEpoch [5/5], Step [2690/5158], Loss: 0.1490\nEpoch [5/5], Step [2700/5158], Loss: 0.1223\nEpoch [5/5], Step [2710/5158], Loss: 0.1059\nEpoch [5/5], Step [2720/5158], Loss: 0.1120\nEpoch [5/5], Step [2730/5158], Loss: 0.1510\nEpoch [5/5], Step [2740/5158], Loss: 0.1106\nEpoch [5/5], Step [2750/5158], Loss: 0.1216\nEpoch [5/5], Step [2760/5158], Loss: 0.1835\nEpoch [5/5], Step [2770/5158], Loss: 0.0826\nEpoch [5/5], Step [2780/5158], Loss: 0.1512\nEpoch [5/5], Step [2790/5158], Loss: 0.1710\nEpoch [5/5], Step [2800/5158], Loss: 0.1452\nEpoch [5/5], Step [2810/5158], Loss: 0.1278\nEpoch [5/5], Step [2820/5158], Loss: 0.1317\nEpoch [5/5], Step [2830/5158], Loss: 0.1098\nEpoch [5/5], Step [2840/5158], Loss: 0.1066\nEpoch [5/5], Step [2850/5158], Loss: 0.1411\nEpoch [5/5], Step [2860/5158], Loss: 0.1543\nEpoch [5/5], Step [2870/5158], Loss: 0.1856\nEpoch [5/5], Step [2880/5158], Loss: 0.1250\nEpoch [5/5], Step [2890/5158], Loss: 0.0911\nEpoch [5/5], Step [2900/5158], Loss: 0.1463\nEpoch [5/5], Step [2910/5158], Loss: 0.1620\nEpoch [5/5], Step [2920/5158], Loss: 0.1434\nEpoch [5/5], Step [2930/5158], Loss: 0.1731\nEpoch [5/5], Step [2940/5158], Loss: 0.0793\nEpoch [5/5], Step [2950/5158], Loss: 0.1157\nEpoch [5/5], Step [2960/5158], Loss: 0.1321\nEpoch [5/5], Step [2970/5158], Loss: 0.1366\nEpoch [5/5], Step [2980/5158], Loss: 0.1508\nEpoch [5/5], Step [2990/5158], Loss: 0.1806\nEpoch [5/5], Step [3000/5158], Loss: 0.1361\nEpoch [5/5], Step [3010/5158], Loss: 0.1806\nEpoch [5/5], Step [3020/5158], Loss: 0.1305\nEpoch [5/5], Step [3030/5158], Loss: 0.1576\nEpoch [5/5], Step [3040/5158], Loss: 0.1200\nEpoch [5/5], Step [3050/5158], Loss: 0.1233\nEpoch [5/5], Step [3060/5158], Loss: 0.1332\nEpoch [5/5], Step [3070/5158], Loss: 0.1387\nEpoch [5/5], Step [3080/5158], Loss: 0.1533\nEpoch [5/5], Step [3090/5158], Loss: 0.1140\nEpoch [5/5], Step [3100/5158], Loss: 0.1305\nEpoch [5/5], Step [3110/5158], Loss: 0.1040\nEpoch [5/5], Step [3120/5158], Loss: 0.1403\nEpoch [5/5], Step [3130/5158], Loss: 0.1246\nEpoch [5/5], Step [3140/5158], Loss: 0.1054\nEpoch [5/5], Step [3150/5158], Loss: 0.1128\nEpoch [5/5], Step [3160/5158], Loss: 0.1713\nEpoch [5/5], Step [3170/5158], Loss: 0.0714\nEpoch [5/5], Step [3180/5158], Loss: 0.1515\nEpoch [5/5], Step [3190/5158], Loss: 0.1503\nEpoch [5/5], Step [3200/5158], Loss: 0.1473\nEpoch [5/5], Step [3210/5158], Loss: 0.0767\nEpoch [5/5], Step [3220/5158], Loss: 0.0675\nEpoch [5/5], Step [3230/5158], Loss: 0.1319\nEpoch [5/5], Step [3240/5158], Loss: 0.1556\nEpoch [5/5], Step [3250/5158], Loss: 0.1422\nEpoch [5/5], Step [3260/5158], Loss: 0.1224\nEpoch [5/5], Step [3270/5158], Loss: 0.1439\nEpoch [5/5], Step [3280/5158], Loss: 0.1293\nEpoch [5/5], Step [3290/5158], Loss: 0.0897\nEpoch [5/5], Step [3300/5158], Loss: 0.1590\nEpoch [5/5], Step [3310/5158], Loss: 0.1736\nEpoch [5/5], Step [3320/5158], Loss: 0.1054\nEpoch [5/5], Step [3330/5158], Loss: 0.1389\nEpoch [5/5], Step [3340/5158], Loss: 0.2202\nEpoch [5/5], Step [3350/5158], Loss: 0.1371\nEpoch [5/5], Step [3360/5158], Loss: 0.1481\nEpoch [5/5], Step [3370/5158], Loss: 0.1511\nEpoch [5/5], Step [3380/5158], Loss: 0.0978\nEpoch [5/5], Step [3390/5158], Loss: 0.1078\nEpoch [5/5], Step [3400/5158], Loss: 0.1184\nEpoch [5/5], Step [3410/5158], Loss: 0.1846\nEpoch [5/5], Step [3420/5158], Loss: 0.1615\nEpoch [5/5], Step [3430/5158], Loss: 0.1246\nEpoch [5/5], Step [3440/5158], Loss: 0.1126\nEpoch [5/5], Step [3450/5158], Loss: 0.1407\nEpoch [5/5], Step [3460/5158], Loss: 0.0754\nEpoch [5/5], Step [3470/5158], Loss: 0.1357\nEpoch [5/5], Step [3480/5158], Loss: 0.1524\nEpoch [5/5], Step [3490/5158], Loss: 0.1408\nEpoch [5/5], Step [3500/5158], Loss: 0.1572\nEpoch [5/5], Step [3510/5158], Loss: 0.1817\nEpoch [5/5], Step [3520/5158], Loss: 0.1849\nEpoch [5/5], Step [3530/5158], Loss: 0.1338\nEpoch [5/5], Step [3540/5158], Loss: 0.1387\nEpoch [5/5], Step [3550/5158], Loss: 0.1341\nEpoch [5/5], Step [3560/5158], Loss: 0.1524\nEpoch [5/5], Step [3570/5158], Loss: 0.1094\nEpoch [5/5], Step [3580/5158], Loss: 0.1566\nEpoch [5/5], Step [3590/5158], Loss: 0.0722\nEpoch [5/5], Step [3600/5158], Loss: 0.1275\nEpoch [5/5], Step [3610/5158], Loss: 0.1809\nEpoch [5/5], Step [3620/5158], Loss: 0.1340\nEpoch [5/5], Step [3630/5158], Loss: 0.1745\nEpoch [5/5], Step [3640/5158], Loss: 0.1136\nEpoch [5/5], Step [3650/5158], Loss: 0.1540\nEpoch [5/5], Step [3660/5158], Loss: 0.1368\nEpoch [5/5], Step [3670/5158], Loss: 0.1255\nEpoch [5/5], Step [3680/5158], Loss: 0.1072\nEpoch [5/5], Step [3690/5158], Loss: 0.1023\nEpoch [5/5], Step [3700/5158], Loss: 0.1227\nEpoch [5/5], Step [3710/5158], Loss: 0.1642\nEpoch [5/5], Step [3720/5158], Loss: 0.1563\nEpoch [5/5], Step [3730/5158], Loss: 0.1295\nEpoch [5/5], Step [3740/5158], Loss: 0.1574\nEpoch [5/5], Step [3750/5158], Loss: 0.1356\nEpoch [5/5], Step [3760/5158], Loss: 0.1780\nEpoch [5/5], Step [3770/5158], Loss: 0.2070\nEpoch [5/5], Step [3780/5158], Loss: 0.1413\nEpoch [5/5], Step [3790/5158], Loss: 0.1142\nEpoch [5/5], Step [3800/5158], Loss: 0.1347\nEpoch [5/5], Step [3810/5158], Loss: 0.1239\nEpoch [5/5], Step [3820/5158], Loss: 0.1202\nEpoch [5/5], Step [3830/5158], Loss: 0.1469\nEpoch [5/5], Step [3840/5158], Loss: 0.1415\nEpoch [5/5], Step [3850/5158], Loss: 0.1264\nEpoch [5/5], Step [3860/5158], Loss: 0.0813\nEpoch [5/5], Step [3870/5158], Loss: 0.1945\nEpoch [5/5], Step [3880/5158], Loss: 0.1257\nEpoch [5/5], Step [3890/5158], Loss: 0.0650\nEpoch [5/5], Step [3900/5158], Loss: 0.1790\nEpoch [5/5], Step [3910/5158], Loss: 0.1476\nEpoch [5/5], Step [3920/5158], Loss: 0.1427\nEpoch [5/5], Step [3930/5158], Loss: 0.1320\nEpoch [5/5], Step [3940/5158], Loss: 0.1189\nEpoch [5/5], Step [3950/5158], Loss: 0.1174\nEpoch [5/5], Step [3960/5158], Loss: 0.1144\nEpoch [5/5], Step [3970/5158], Loss: 0.1753\nEpoch [5/5], Step [3980/5158], Loss: 0.1248\nEpoch [5/5], Step [3990/5158], Loss: 0.1136\nEpoch [5/5], Step [4000/5158], Loss: 0.1500\nEpoch [5/5], Step [4010/5158], Loss: 0.1406\nEpoch [5/5], Step [4020/5158], Loss: 0.1327\nEpoch [5/5], Step [4030/5158], Loss: 0.1429\nEpoch [5/5], Step [4040/5158], Loss: 0.1561\nEpoch [5/5], Step [4050/5158], Loss: 0.1538\nEpoch [5/5], Step [4060/5158], Loss: 0.1236\nEpoch [5/5], Step [4070/5158], Loss: 0.1186\nEpoch [5/5], Step [4080/5158], Loss: 0.1433\nEpoch [5/5], Step [4090/5158], Loss: 0.1330\nEpoch [5/5], Step [4100/5158], Loss: 0.1636\nEpoch [5/5], Step [4110/5158], Loss: 0.0882\nEpoch [5/5], Step [4120/5158], Loss: 0.1283\nEpoch [5/5], Step [4130/5158], Loss: 0.1901\nEpoch [5/5], Step [4140/5158], Loss: 0.1284\nEpoch [5/5], Step [4150/5158], Loss: 0.1760\nEpoch [5/5], Step [4160/5158], Loss: 0.1089\nEpoch [5/5], Step [4170/5158], Loss: 0.1041\nEpoch [5/5], Step [4180/5158], Loss: 0.1249\nEpoch [5/5], Step [4190/5158], Loss: 0.1311\nEpoch [5/5], Step [4200/5158], Loss: 0.1461\nEpoch [5/5], Step [4210/5158], Loss: 0.1050\nEpoch [5/5], Step [4220/5158], Loss: 0.1641\nEpoch [5/5], Step [4230/5158], Loss: 0.1268\nEpoch [5/5], Step [4240/5158], Loss: 0.1022\nEpoch [5/5], Step [4250/5158], Loss: 0.1206\nEpoch [5/5], Step [4260/5158], Loss: 0.1248\nEpoch [5/5], Step [4270/5158], Loss: 0.1214\nEpoch [5/5], Step [4280/5158], Loss: 0.1226\nEpoch [5/5], Step [4290/5158], Loss: 0.1570\nEpoch [5/5], Step [4300/5158], Loss: 0.1423\nEpoch [5/5], Step [4310/5158], Loss: 0.1014\nEpoch [5/5], Step [4320/5158], Loss: 0.1375\nEpoch [5/5], Step [4330/5158], Loss: 0.1195\nEpoch [5/5], Step [4340/5158], Loss: 0.0939\nEpoch [5/5], Step [4350/5158], Loss: 0.1032\nEpoch [5/5], Step [4360/5158], Loss: 0.1444\nEpoch [5/5], Step [4370/5158], Loss: 0.0966\nEpoch [5/5], Step [4380/5158], Loss: 0.1222\nEpoch [5/5], Step [4390/5158], Loss: 0.1968\nEpoch [5/5], Step [4400/5158], Loss: 0.1033\nEpoch [5/5], Step [4410/5158], Loss: 0.1168\nEpoch [5/5], Step [4420/5158], Loss: 0.1405\nEpoch [5/5], Step [4430/5158], Loss: 0.1042\nEpoch [5/5], Step [4440/5158], Loss: 0.1201\nEpoch [5/5], Step [4450/5158], Loss: 0.1699\nEpoch [5/5], Step [4460/5158], Loss: 0.1429\nEpoch [5/5], Step [4470/5158], Loss: 0.1524\nEpoch [5/5], Step [4480/5158], Loss: 0.0989\nEpoch [5/5], Step [4490/5158], Loss: 0.1440\nEpoch [5/5], Step [4500/5158], Loss: 0.1587\nEpoch [5/5], Step [4510/5158], Loss: 0.1037\nEpoch [5/5], Step [4520/5158], Loss: 0.0805\nEpoch [5/5], Step [4530/5158], Loss: 0.1302\nEpoch [5/5], Step [4540/5158], Loss: 0.1467\nEpoch [5/5], Step [4550/5158], Loss: 0.1498\nEpoch [5/5], Step [4560/5158], Loss: 0.1471\nEpoch [5/5], Step [4570/5158], Loss: 0.1136\nEpoch [5/5], Step [4580/5158], Loss: 0.1513\nEpoch [5/5], Step [4590/5158], Loss: 0.1084\nEpoch [5/5], Step [4600/5158], Loss: 0.1799\nEpoch [5/5], Step [4610/5158], Loss: 0.1125\nEpoch [5/5], Step [4620/5158], Loss: 0.1529\nEpoch [5/5], Step [4630/5158], Loss: 0.1219\nEpoch [5/5], Step [4640/5158], Loss: 0.1100\nEpoch [5/5], Step [4650/5158], Loss: 0.0950\nEpoch [5/5], Step [4660/5158], Loss: 0.1487\nEpoch [5/5], Step [4670/5158], Loss: 0.1568\nEpoch [5/5], Step [4680/5158], Loss: 0.1204\nEpoch [5/5], Step [4690/5158], Loss: 0.1340\nEpoch [5/5], Step [4700/5158], Loss: 0.1330\nEpoch [5/5], Step [4710/5158], Loss: 0.1215\nEpoch [5/5], Step [4720/5158], Loss: 0.0647\nEpoch [5/5], Step [4730/5158], Loss: 0.1319\nEpoch [5/5], Step [4740/5158], Loss: 0.1810\nEpoch [5/5], Step [4750/5158], Loss: 0.0961\nEpoch [5/5], Step [4760/5158], Loss: 0.1563\nEpoch [5/5], Step [4770/5158], Loss: 0.1575\nEpoch [5/5], Step [4780/5158], Loss: 0.1925\nEpoch [5/5], Step [4790/5158], Loss: 0.1512\nEpoch [5/5], Step [4800/5158], Loss: 0.0935\nEpoch [5/5], Step [4810/5158], Loss: 0.0821\nEpoch [5/5], Step [4820/5158], Loss: 0.1415\nEpoch [5/5], Step [4830/5158], Loss: 0.1239\nEpoch [5/5], Step [4840/5158], Loss: 0.1954\nEpoch [5/5], Step [4850/5158], Loss: 0.0980\nEpoch [5/5], Step [4860/5158], Loss: 0.1571\nEpoch [5/5], Step [4870/5158], Loss: 0.1398\nEpoch [5/5], Step [4880/5158], Loss: 0.1520\nEpoch [5/5], Step [4890/5158], Loss: 0.1218\nEpoch [5/5], Step [4900/5158], Loss: 0.1159\nEpoch [5/5], Step [4910/5158], Loss: 0.1173\nEpoch [5/5], Step [4920/5158], Loss: 0.1894\nEpoch [5/5], Step [4930/5158], Loss: 0.1531\nEpoch [5/5], Step [4940/5158], Loss: 0.1048\nEpoch [5/5], Step [4950/5158], Loss: 0.1117\nEpoch [5/5], Step [4960/5158], Loss: 0.1202\nEpoch [5/5], Step [4970/5158], Loss: 0.1201\nEpoch [5/5], Step [4980/5158], Loss: 0.1230\nEpoch [5/5], Step [4990/5158], Loss: 0.1013\nEpoch [5/5], Step [5000/5158], Loss: 0.1375\nEpoch [5/5], Step [5010/5158], Loss: 0.1017\nEpoch [5/5], Step [5020/5158], Loss: 0.1417\nEpoch [5/5], Step [5030/5158], Loss: 0.1005\nEpoch [5/5], Step [5040/5158], Loss: 0.1174\nEpoch [5/5], Step [5050/5158], Loss: 0.1298\nEpoch [5/5], Step [5060/5158], Loss: 0.1073\nEpoch [5/5], Step [5070/5158], Loss: 0.1653\nEpoch [5/5], Step [5080/5158], Loss: 0.1435\nEpoch [5/5], Step [5090/5158], Loss: 0.1574\nEpoch [5/5], Step [5100/5158], Loss: 0.1053\nEpoch [5/5], Step [5110/5158], Loss: 0.1001\nEpoch [5/5], Step [5120/5158], Loss: 0.1560\nEpoch [5/5], Step [5130/5158], Loss: 0.1192\nEpoch [5/5], Step [5140/5158], Loss: 0.1321\nEpoch [5/5], Step [5150/5158], Loss: 0.1349\n","output_type":"stream"},{"name":"stderr","text":"../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [117,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [118,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1858337514.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'测试集准确率: {100 * correct / total:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":41},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel.eval()\nmodel = model.to(device)\ntotal, correct = 0, 0\nwith torch.no_grad():\n    for cmt, lbl in test_dataloader:\n        cmt, lbl = cmt.to(device), lbl.to(device)  # 恢复这行代码\n        outputs = model(cmt)\n        _, predicted = torch.max(outputs.data, 1)\n        total += lbl.size(0)\n        correct += (predicted == lbl).sum().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T14:10:34.306976Z","iopub.execute_input":"2025-04-17T14:10:34.307257Z","iopub.status.idle":"2025-04-17T14:10:34.374452Z","shell.execute_reply.started":"2025-04-17T14:10:34.307239Z","shell.execute_reply":"2025-04-17T14:10:34.373448Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3947670287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 恢复这行代码\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":48},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}