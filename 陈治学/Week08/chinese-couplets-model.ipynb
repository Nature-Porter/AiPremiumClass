{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2726695,"sourceType":"datasetVersion","datasetId":1661983},{"sourceId":374178,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":309384,"modelId":329766}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# import packages","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport json\nimport sys\n\n# 添加自定义模块路径（根据实际存放位置修改路径）\nsys.path.append('/kaggle/input/encoderdecoderattenmodel/pytorch/default/1')\nfrom EncoderDecoderAttenModel import Seq2Seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:16:35.345449Z","iopub.execute_input":"2025-05-05T10:16:35.346090Z","iopub.status.idle":"2025-05-05T10:16:35.350021Z","shell.execute_reply.started":"2025-05-05T10:16:35.346069Z","shell.execute_reply":"2025-05-05T10:16:35.349293Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# 数据预处理","metadata":{}},{"cell_type":"code","source":"from itertools import islice\nimport torch\nfrom torch.utils.data import Dataset\n\nclass CoupletDataset(Dataset):\n    def __init__(self, enc_file, dec_file, max_len=50, max_samples=50000):\"\n        # 高效读取前max_samples行数据\n        self.enc_data = self._read_lines(enc_file, max_samples)\n        self.dec_data = self._read_lines(dec_file, max_samples)\n        \n        # 数据一致性校验\n        self._validate_data()\n        \n        # 构建词汇表\n        self.char2idx, self.idx2char = self._build_vocab()\n        self.max_len = max_len\n\n    def _read_lines(self, file_path, max_lines):\n        \"\"\" 内存优化的数据读取方法 \"\"\"\n        lines = []\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in islice(f, max_lines):  # 逐行读取避免内存压力\n                line = line.strip()\n                if line:  # 过滤空行\n                    lines.append(line)\n        return lines\n\n    def _validate_data(self):\n        \"\"\" 数据验证 \"\"\"\n        if len(self.enc_data) != len(self.dec_data):\n            raise ValueError(\n                f\"数据不匹配！上联数：{len(self.enc_data)}, 下联数：{len(self.dec_data)}\")\n        \n        min_length = 3  # 最小有效对联长度（如：上联为2字可能有错误）\n        error_samples = []\n        for i, (enc, dec) in enumerate(zip(self.enc_data, self.dec_data)):\n            if len(enc) < min_length or len(dec) < min_length:\n                error_samples.append(i)\n        if error_samples:\n            print(f\"警告：发现{len(error_samples)}条异常数据（行号：{error_samples[:5]}...）\")\n\n    def _build_vocab(self):\n        \"\"\" 优化后的词汇表构建 \"\"\"\n        char_counter = set()\n        for enc, dec in zip(self.enc_data, self.dec_data):\n            char_counter.update(enc)\n            char_counter.update(dec)\n        \n        # 创建词典（添加特殊标记）\n        char2idx = {\n            '<PAD>': 0, \n            '<SOS>': 1,   # Start of Sequence\n            '<EOS>': 2,   # End of Sequence\n            '<UNK>': 3    # Unknown token（添加）\n        }\n        # 按字符频率排序（可选优化）\n        for idx, char in enumerate(char_counter, start=4):\n            char2idx[char] = idx\n            \n        idx2char = {v:k for k, v in char2idx.items()}\n        return char2idx, idx2char\n\n    def __len__(self):\n        return len(self.enc_data)\n\n    def __getitem__(self, idx):\n        \"\"\" 添加处理优化和异常防御 \"\"\"\n        # 获取原始文本\n        enc_text = self.enc_data[idx][:self.max_len]  # 预截断\n        dec_text = self.dec_data[idx][:self.max_len-1]  # 保留EOS位置\n        \n        # 编码器输入处理\n        enc_indices = [\n            self.char2idx.get(c, self.char2idx['<UNK>']) \n            for c in enc_text\n        ]\n        # 填充处理\n        enc_padded = enc_indices + [self.char2idx['<PAD>']] * (self.max_len - len(enc_indices))\n        \n        # 解码器输入处理（含EOS标记）\n        dec_indices = (\n            [self.char2idx['<SOS>']] +\n            [self.char2idx.get(c, self.char2idx['<UNK>']) for c in dec_text] +\n            [self.char2idx['<EOS>']]\n        )\n        # 截断或填充\n        dec_padded = dec_indices[:self.max_len] + [self.char2idx['<PAD>']] * (self.max_len - len(dec_indices))\n        \n        return (\n            torch.LongTensor(enc_padded),\n            torch.LongTensor(dec_padded)\n        )\n\n    def analyze(self):\n        \"\"\" 数据集分析工具 \"\"\"\n        stats = {\n            'total_pairs': len(self),\n            'vocab_size': len(self.char2idx),\n            'max_length': self.max_len,\n            'enc_lengths': [len(s) for s in self.enc_data],\n            'dec_lengths': [len(s) for s in self.dec_data]\n        }\n        print(f\"数据集分析：\")\n        print(f\"- 样本总数：{stats['total_pairs']}\")\n        print(f\"- 词汇表大小：{stats['vocab_size']}\")\n        print(f\"- 上联平均长度：{sum(stats['enc_lengths'])/len(self):.1f}\")\n        print(f\"- 下联平均长度：{sum(stats['dec_lengths'])/len(self):.1f}\")\n        print(f\"- 最大允许长度：{self.max_len}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:16:39.138471Z","iopub.execute_input":"2025-05-05T10:16:39.138986Z","iopub.status.idle":"2025-05-05T10:16:39.183885Z","shell.execute_reply.started":"2025-05-05T10:16:39.138965Z","shell.execute_reply":"2025-05-05T10:16:39.183151Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 检查并设置设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# 数据集路径（Kaggle输入目录）\ndataset = CoupletDataset(\n    enc_file='/kaggle/input/chinese-couplets/couplet/train/in.txt',\n    dec_file='/kaggle/input/chinese-couplets/couplet/train/out.txt'\n)\n\ndataloader = DataLoader(dataset, batch_size=256, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:20:23.382094Z","iopub.execute_input":"2025-05-05T10:20:23.382395Z","iopub.status.idle":"2025-05-05T10:20:23.501515Z","shell.execute_reply.started":"2025-05-05T10:20:23.382376Z","shell.execute_reply":"2025-05-05T10:20:23.500913Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n警告：发现147条异常数据（行号：[167, 634, 688, 1248, 1603]...）\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# 模型训练","metadata":{}},{"cell_type":"code","source":"# 初始化模型\nmodel = Seq2Seq(\n    enc_emb_size=len(dataset.char2idx),\n    dec_emb_size=len(dataset.char2idx),\n    emb_dim=256,\n    hidden_size=512,\n    dropout=0.3\n).to(device)\n\n# 训练参数设置\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\nwriter = SummaryWriter('/kaggle/working/logs')  # tensorboard日志目录\n\nfor epoch in range(20):\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    \n    for batch_idx, (enc_inputs, dec_inputs) in enumerate(dataloader):\n        enc_inputs = enc_inputs.to(device)\n        dec_inputs = dec_inputs.to(device)\n        \n        # 前向传播\n        outputs, _ = model(enc_inputs, dec_inputs[:, :-1])\n        \n        # 计算损失\n        loss = criterion(\n            outputs.view(-1, outputs.size(-1)),\n            dec_inputs[:, 1:].contiguous().view(-1)\n        )\n        \n        # 计算准确率\n        preds = outputs.argmax(dim=-1)\n        targets = dec_inputs[:, 1:].contiguous().view(-1)\n        mask = targets != 0  # 忽略padding部分\n        correct = (preds.view(-1)[mask] == targets[mask]).sum().item()\n        \n        total_correct += correct\n        total_tokens += mask.sum().item()\n        \n        # 记录训练指标\n        writer.add_scalar('Loss/train_batch', loss.item(), epoch*len(dataloader)+batch_idx)\n        writer.add_scalar('Accuracy/train_batch', correct/mask.sum().item(), epoch*len(dataloader)+batch_idx)\n        \n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        if batch_idx % 100 == 0:\n            print(f'Epoch:{epoch+1} | Batch:{batch_idx} | Loss:{loss.item():.4f}')\n    \n    # 记录epoch指标\n    epoch_loss = total_loss / len(dataloader)\n    epoch_acc = total_correct / total_tokens if total_tokens >0 else 0\n    writer.add_scalar('Loss/train_epoch', epoch_loss, epoch)\n    writer.add_scalar('Accuracy/train_epoch', epoch_acc, epoch)\n    \n    print(f'Epoch {epoch+1} Complete | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}')\n\n# 保存模型和词汇表\ntorch.save(model.state_dict(), '/kaggle/working/couplet_model.pth')\nwith open('/kaggle/working/vocab.json', 'w') as f:\n    json.dump(dataset.char2idx, f)\n\nwriter.close()\nprint(\"Training complete and models saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:20:29.767026Z","iopub.execute_input":"2025-05-05T10:20:29.767310Z","iopub.status.idle":"2025-05-05T10:42:43.827028Z","shell.execute_reply.started":"2025-05-05T10:20:29.767290Z","shell.execute_reply":"2025-05-05T10:42:43.826266Z"}},"outputs":[{"name":"stdout","text":"Epoch:1 | Batch:0 | Loss:8.6732\nEpoch:1 | Batch:100 | Loss:3.5198\nEpoch 1 Complete | Loss: 3.7209 | Acc: 0.4910\nEpoch:2 | Batch:0 | Loss:3.4952\nEpoch:2 | Batch:100 | Loss:3.3289\nEpoch 2 Complete | Loss: 3.3310 | Acc: 0.5297\nEpoch:3 | Batch:0 | Loss:3.2104\nEpoch:3 | Batch:100 | Loss:3.0578\nEpoch 3 Complete | Loss: 3.0800 | Acc: 0.5471\nEpoch:4 | Batch:0 | Loss:2.8649\nEpoch:4 | Batch:100 | Loss:2.7508\nEpoch 4 Complete | Loss: 2.7592 | Acc: 0.5726\nEpoch:5 | Batch:0 | Loss:2.4187\nEpoch:5 | Batch:100 | Loss:2.4248\nEpoch 5 Complete | Loss: 2.4270 | Acc: 0.6007\nEpoch:6 | Batch:0 | Loss:2.1532\nEpoch:6 | Batch:100 | Loss:2.2045\nEpoch 6 Complete | Loss: 2.1974 | Acc: 0.6193\nEpoch:7 | Batch:0 | Loss:1.9283\nEpoch:7 | Batch:100 | Loss:1.9666\nEpoch 7 Complete | Loss: 1.9994 | Acc: 0.6370\nEpoch:8 | Batch:0 | Loss:1.7416\nEpoch:8 | Batch:100 | Loss:1.8291\nEpoch 8 Complete | Loss: 1.7946 | Acc: 0.6588\nEpoch:9 | Batch:0 | Loss:1.4696\nEpoch:9 | Batch:100 | Loss:1.5674\nEpoch 9 Complete | Loss: 1.5757 | Acc: 0.6873\nEpoch:10 | Batch:0 | Loss:1.2600\nEpoch:10 | Batch:100 | Loss:1.3772\nEpoch 10 Complete | Loss: 1.3400 | Acc: 0.7237\nEpoch:11 | Batch:0 | Loss:0.9984\nEpoch:11 | Batch:100 | Loss:1.1362\nEpoch 11 Complete | Loss: 1.0998 | Acc: 0.7664\nEpoch:12 | Batch:0 | Loss:0.7697\nEpoch:12 | Batch:100 | Loss:0.8946\nEpoch 12 Complete | Loss: 0.8494 | Acc: 0.8173\nEpoch:13 | Batch:0 | Loss:0.5440\nEpoch:13 | Batch:100 | Loss:0.5963\nEpoch 13 Complete | Loss: 0.6159 | Acc: 0.8700\nEpoch:14 | Batch:0 | Loss:0.3670\nEpoch:14 | Batch:100 | Loss:0.4235\nEpoch 14 Complete | Loss: 0.4241 | Acc: 0.9159\nEpoch:15 | Batch:0 | Loss:0.2389\nEpoch:15 | Batch:100 | Loss:0.2650\nEpoch 15 Complete | Loss: 0.2774 | Acc: 0.9521\nEpoch:16 | Batch:0 | Loss:0.1356\nEpoch:16 | Batch:100 | Loss:0.1821\nEpoch 16 Complete | Loss: 0.1736 | Acc: 0.9764\nEpoch:17 | Batch:0 | Loss:0.0868\nEpoch:17 | Batch:100 | Loss:0.1272\nEpoch 17 Complete | Loss: 0.1258 | Acc: 0.9859\nEpoch:18 | Batch:0 | Loss:0.0698\nEpoch:18 | Batch:100 | Loss:0.0897\nEpoch 18 Complete | Loss: 0.0929 | Acc: 0.9911\nEpoch:19 | Batch:0 | Loss:0.0575\nEpoch:19 | Batch:100 | Loss:0.0761\nEpoch 19 Complete | Loss: 0.0778 | Acc: 0.9928\nEpoch:20 | Batch:0 | Loss:0.0478\nEpoch:20 | Batch:100 | Loss:0.0546\nEpoch 20 Complete | Loss: 0.0585 | Acc: 0.9947\nTraining complete and models saved!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# 推理实现","metadata":{}},{"cell_type":"code","source":"class CoupletInfer:\n    def __init__(self, model_path, vocab_path, max_len=50):\n        # 加载词汇表\n        with open(vocab_path) as f:\n            self.char2idx = json.load(f)\n        self.idx2char = {v:k for k,v in self.char2idx.items()}\n        \n        # 加载模型\n        self.model = torch.load(model_path)\n        self.model.eval()\n        self.max_len = max_len\n\n    def decode(self, text):\n        # 文本 -> 编码\n        enc_ids = [self.char2idx.get(c, 3) for c in text[:self.max_len]]\n        enc_tensor = torch.LongTensor(enc_ids).unsqueeze(0)\n        \n        # 模型推理\n        enc_out, hidden = self.model.encoder(enc_tensor)\n        dec_ids = [1]  # SOS=1\n        \n        for _ in range(self.max_len):\n            dec_tensor = torch.LongTensor([dec_ids[-1]])\n            dec_out, hidden = self.model.decoder(dec_tensor, hidden, enc_out)\n            next_id = dec_out.argmax().item()\n            if next_id == 2: break  # EOS=2\n            dec_ids.append(next_id)\n        \n        # 结果转换\n        return ''.join([self.idx2char[i] for i in dec_ids[1:]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:17:37.522884Z","iopub.execute_input":"2025-05-05T11:17:37.523432Z","iopub.status.idle":"2025-05-05T11:17:37.529943Z","shell.execute_reply.started":"2025-05-05T11:17:37.523408Z","shell.execute_reply":"2025-05-05T11:17:37.529325Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"infer = CoupletInfer(\n    model_path=\"/kaggle/working/couplet_model.pth\",\n    vocab_path=\"/kaggle/working/vocab.json\"\n)\n\nprint(infer.decode(\"春风送暖\"))  # 输出：大地回春","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:17:42.086331Z","iopub.execute_input":"2025-05-05T11:17:42.086597Z","iopub.status.idle":"2025-05-05T11:17:42.154184Z","shell.execute_reply.started":"2025-05-05T11:17:42.086579Z","shell.execute_reply":"2025-05-05T11:17:42.153254Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/642187359.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model = torch.load(model_path)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2390626522.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m infer = CoupletInfer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/kaggle/working/couplet_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvocab_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/kaggle/working/vocab.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/642187359.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, vocab_path, max_len)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# 加载模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"],"ename":"AttributeError","evalue":"'collections.OrderedDict' object has no attribute 'eval'","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}