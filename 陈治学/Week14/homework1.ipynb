{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb1722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\czx\\AppData\\Local\\Temp\\ipykernel_17352\\61900178.py:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\czx\\AppData\\Local\\Temp\\ipykernel_17352\\61900178.py:32: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 当然，我来为你详细解释一下 **RAG（Retrieval-Augmented Generation）** 和 **LangChain** 之间的关系。\n",
      "\n",
      "---\n",
      "\n",
      "### 一、什么是 RAG？\n",
      "\n",
      "**RAG（Retrieval-Augmented Generation）** 是一种结合信息检索（Retrieval）和文本生成（Generation）的 AI 技术架构。它的核心思想是：\n",
      "\n",
      "> 在生成回答之前，先从外部知识库中检索相关信息，然后将这些信息作为上下文输入给语言模型，从而让模型能够基于最新的、准确的数据进行回答。\n",
      "\n",
      "#### RAG 的典型流程：\n",
      "1. **用户提问**：例如，“特斯拉2024年的营收是多少？”\n",
      "2. **检索阶段（Retrieval）**：系统会从知识库或数据库中查找与“特斯拉 2024 营收”相关的信息。\n",
      "3. **生成阶段（Generation）**：把检索到的信息和原始问题一起输入给大语言模型（如 LLaMA、ChatGPT 等），由模型整合后生成一个结构化、准确的回答。\n",
      "\n",
      "这种方式可以弥补大语言模型无法实时更新知识的问题，非常适合构建企业级问答系统、智能客服等应用。\n",
      "\n",
      "---\n",
      "\n",
      "### 二、什么是 LangChain？\n",
      "\n",
      "**LangChain** 是一个用于开发基于大语言模型（LLM）应用程序的开源框架。它提供了一套工具和模块，帮助开发者更高效地构建复杂的应用逻辑，比如：\n",
      "\n",
      "- 链式调用多个 LLM 模块\n",
      "- 集成外部数据源\n",
      "- 构建记忆机制（Memory）\n",
      "- 支持代理（Agents）、工具调用等功能\n",
      "\n",
      "LangChain 的一大优势在于其**模块化设计**，你可以像搭积木一样组合不同的组件来构建你的 AI 应用。\n",
      "\n",
      "---\n",
      "\n",
      "### 三、RAG 与 LangChain 的关系\n",
      "\n",
      "现在我们来看两者的关系：\n",
      "\n",
      "| 对比维度 | RAG | LangChain |\n",
      "|----------|-----|-----------|\n",
      "| 性质 | 一种技术架构 | 一个开发框架 |\n",
      "| 功能 | 增强语言模型的知识来源 | 提供构建 LLM 应用的工具链 |\n",
      "| 使用场景 | 构建问答系统、知识增强型对话系统 | 构建各种基于 LLM 的应用，包括但不限于 RAG 系统 |\n",
      "\n",
      "#### 更具体地说：\n",
      "\n",
      "- **LangChain 可以用来实现 RAG 架构**。\n",
      "- LangChain 提供了以下关键组件来支持 RAG 实现：\n",
      "  - **文档加载器（Document Loaders）**：从各种来源加载数据。\n",
      "  - **向量化存储（Vector Stores）**：将文档转换为向量并存储，便于检索。\n",
      "  - **检索器（Retrievers）**：根据查询语句快速从知识库中找到相关文档。\n",
      "  - **提示模板（Prompt Templates）**：构造输入给 LLM 的提示内容。\n",
      "  - **LLM 接口**：调用本地或云端的语言模型进行生成。\n",
      "\n",
      "---\n",
      "\n",
      "### 四、举个例子说明它们如何协作\n",
      "\n",
      "假设你要做一个企业内部的知识问答机器人：\n",
      "\n",
      "1. 使用 **LangChain 的 Document Loader** 加载公司内部的 PDF、网页、数据库等资料；\n",
      "2. 通过 **Embedding 模型** 将这些文档向量化，并使用 **FAISS 或 Chroma 向量数据库** 存储；\n",
      "3. 用户提问时，LangChain 的 **Retriever** 会在向量库中检索出最相关的几段内容；\n",
      "4. 将检索结果和用户问题一起组成 prompt，传给 GPT 或本地 LLM；\n",
      "5. 最终输出一个基于企业最新数据的回答。\n",
      "\n",
      "这个过程就是一个典型的 **基于 LangChain 实现的 RAG 系统**。\n",
      "\n",
      "---\n",
      "\n",
      "### 五、总结\n",
      "\n",
      "| 角度 | RAG | LangChain |\n",
      "|------|-----|-----------|\n",
      "| 定位 | 技术架构 | 开发框架 |\n",
      "| 目标 | 增强模型知识能力 | 构建 LLM 应用 |\n",
      "| 关系 | 是一种应用场景 | 是实现该场景的工具平台 |\n",
      "| 典型用途 | 构建知识增强型问答系统 | 构建包括 RAG、Agent、聊天机器人等在内的 LLM 应用 |\n",
      "\n",
      "---\n",
      "\n",
      "如果你正在开发一个需要结合外部知识的大模型应用，那么 **LangChain 是实现 RAG 架构的理想选择之一**。\n",
      "\n",
      "如需进一步了解具体代码实现或案例分析，我也可以继续为你介绍。\n",
      "AI: 当然可以！下面是一个使用 **LangChain** 实现 **RAG（Retrieval-Augmented Generation）** 的简单示例代码，控制在 20 行以内，使用的是 Hugging Face 的嵌入模型和 FAISS 向量数据库：\n",
      "\n",
      "```python\n",
      "from langchain.document_loaders import TextLoader\n",
      "from langchain.embeddings import HuggingFaceEmbeddings\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.llms import HuggingFacePipeline\n",
      "\n",
      "# 加载文档\n",
      "loader = TextLoader(\"your_document.txt\")\n",
      "docs = loader.load()\n",
      "\n",
      "# 创建嵌入并构建向量库\n",
      "embeddings = HuggingFaceEmbeddings()\n",
      "db = FAISS.from_documents(docs, embeddings)\n",
      "\n",
      "# 初始化一个本地语言模型（如 LLaMA、Falcon 等）\n",
      "llm = HuggingFacePipeline.from_model_id(model_id=\"google/flan-t5-small\", task=\"text2text-generation\")\n",
      "\n",
      "# 构建 RAG 链\n",
      "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())\n",
      "\n",
      "# 使用 RAG 回答问题\n",
      "query = \"特斯拉2024年的营收是多少？\"\n",
      "response = qa_chain.run(query)\n",
      "print(response)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 说明：\n",
      "- 使用 `TextLoader` 加载本地文本文件；\n",
      "- 利用 `HuggingFaceEmbeddings` 生成向量；\n",
      "- 使用 `FAISS` 构建本地向量数据库；\n",
      "- 通过 `RetrievalQA` 快速构建 RAG 链；\n",
      "- 最后运行查询时，系统会先检索再生成回答。\n",
      "\n",
      "> 注：你需要安装 `langchain`, `transformers`, `faiss-cpu`, `torch` 等依赖包。如果使用 GPU 支持的版本，可将 `faiss-cpu` 替换为 `faiss-gpu`。\n",
      "\n",
      "如果你希望使用更轻量或不同组件（比如 OpenAI API + Pinecone），我也可以提供对应版本。\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.llms import Tongyi\n",
    "import dashscope  \n",
    "\n",
    "\n",
    "dashscope.api_key = \"\"\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"你是一个专业的科技顾问，请用专业但易懂的语言回答问题。\n",
    "    \n",
    "    对话历史：\n",
    "    {history}\n",
    "    \n",
    "    当前问题：{input}\n",
    "    回答：\"\"\"\n",
    ")\n",
    "\n",
    "llm = Tongyi(\n",
    "    dashscope_api_key=dashscope.api_key,\n",
    "    model_name=\"qwen-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=custom_prompt,  \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"你: \")  \n",
    "    \n",
    "    if user_input.lower() == 'exit':  \n",
    "        break\n",
    "    \n",
    "\n",
    "    response = conversation.predict(input=user_input)\n",
    "    \n",
    "    print(f\"AI: {response}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6989966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对话历史已保存！\n"
     ]
    }
   ],
   "source": [
    "if user_input.lower() == 'exit':\n",
    "\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    \n",
    "\n",
    "    with open(\"conversation_history.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(history)\n",
    "    \n",
    "    print(\"对话历史已保存！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9737ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
